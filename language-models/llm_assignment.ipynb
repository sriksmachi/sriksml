{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriksmachi/sriksml/blob/main/language-models/llm_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning FLAN T5\n",
        "\n",
        "This notebook contains code artifacts for fine tuning Flan-T5-small model. The notebook does the following tasks.\n",
        "\n",
        "1. Use a pre-trained google/flan-t5-small as the model.\n",
        "2. Verify if the summarization task works.\n",
        "3. Verify if the Q&A task works.\n",
        "4. Verify if English to French translation task works.\n",
        "5. Programmatically print the names of all the model layers and their dimensions.\n",
        "6. Programmatically print the total number of parameters/weights in this model.\n",
        "7. Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros.\n",
        "8. Verify if the Q&A task works aWer reseXng the weights of the above layer.\n",
        "9. Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions and adjust all the dependent layers to match the dimension\n",
        "10. Reload the original google/flan-t5-small model.\n",
        "11. Train the model for a Q&A task that takes a context as additional input along with the queson. You can use SQuAD dataset. Choose an\n",
        "appropriate task prefix/trigger word and justify the choice.\n",
        "12. Evaluate the quality of the model\n",
        "\n",
        "Paper: https://arxiv.org/abs/2210.11416 </br>\n",
        "Official repo: https://github.com/google-research/t5x"
      ],
      "metadata": {
        "id": "2yMznrnobb3S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q2vmIk-Mwfob"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -q transformers[torch]\n",
        "pip install -q sentencepiece\n",
        "pip install -q datasets\n",
        "pip install -q tokenizers\n",
        "pip install -q evaluate\n",
        "pip install -q rouge_score\n",
        "pip install -q nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sDtFNLzJPNS"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9ILlvvoyw2dA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import torch\n",
        "from torch import nn\n",
        "import nltk\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
        "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ScMqzLk2xit1"
      },
      "outputs": [],
      "source": [
        "from transformers.models.blip_2.modeling_blip_2 import AutoModelForSeq2SeqLM\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izzscG9kiQcN"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xXrGklMChiWY"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import evaluate\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# loading Rouge\n",
        "rogue_metric = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsek27LtfCfu"
      },
      "source": [
        "## Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DEA_hh5jxu-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b92a00-79e0-4fb4-c282-4c9cc04ec86e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nk9oB39Nxehc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50895e9-0085-495e-f259-756fc8177194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 571,  405, 4505, 1635, 1707,  161,    3,   58,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "How does summarization work?</s>\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "sentence = \"How does summarization work ?\"\n",
        "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
        "print(sentence_encoded)\n",
        "sentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0])\n",
        "print(sentence_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2NC1LRcez4aP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40ad12e-75cc-44c6-c411-ca51f708b383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue\n",
            "#Person1#: So, what do you think? How do you like New York?\n",
            "#Person2#: I'm having a great time. I love it. I'm glad we came.\n",
            "#Person1#: Yeah. I really like the stores and the shopping.\n",
            "#Person2#: I love the museums, too.\n",
            "#Person1#: But the traffic is pretty bad.\n",
            "#Person2#: Yeah. I hate all this traffic. It's really noisy.\n",
            "#Person1#: Listen, it's almost dinnertime. There are lots of restaurant around here. What do you want to try? Italian? Greek? Japanese? Thai?\n",
            "#Person2#: I can't stand making decisions. You choose!\n",
            "#Person1#: OK. Let's go American. Where's the nearest McDonald's?\n",
            "==================================================\n",
            "Baseline Summary\n",
            "#Person1# and #Person2# discuss how they feel about New York. Then they decide to go to McDonald's.\n",
            "==================================================\n",
            "Generated Summary without prompt\n",
            "#Person1#: No, it's not.\n",
            "==================================================\n",
            "{'rouge1': 0.1739130434782609, 'rouge2': 0.0, 'rougeL': 0.1739130434782609, 'rougeLsum': 0.1739130434782609}\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "random_idx = random.randint(0, len(dataset[\"train\"]))\n",
        "print(\"Dialogue\")\n",
        "sentence = dataset[\"train\"][random_idx][\"dialogue\"]\n",
        "print(sentence)\n",
        "print(\"=====\"*10)\n",
        "print(\"Baseline Summary\")\n",
        "baseline_summary = dataset[\"train\"][random_idx][\"summary\"]\n",
        "print(baseline_summary)\n",
        "print(\"=====\"*10)\n",
        "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
        "pred_summary = model.generate(sentence_encoded['input_ids'], max_new_tokens=50)\n",
        "sentence_decoded = tokenizer.decode(pred_summary[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary without prompt\")\n",
        "print(sentence_decoded)\n",
        "print(\"=====\"*10)\n",
        "rouge_score = rogue_metric.compute(predictions=[sentence_decoded], references=[baseline_summary])\n",
        "print(rouge_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3TuHqtE51tYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a548d01c-1738-4ea8-a76d-f8b72d27b6e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a sentence summarization bot. Please summarize the conversation.Dialogue:\n",
            "#Person1#: So, what do you think? How do you like New York?\n",
            "#Person2#: I'm having a great time. I love it. I'm glad we came.\n",
            "#Person1#: Yeah. I really like the stores and the shopping.\n",
            "#Person2#: I love the museums, too.\n",
            "#Person1#: But the traffic is pretty bad.\n",
            "#Person2#: Yeah. I hate all this traffic. It's really noisy.\n",
            "#Person1#: Listen, it's almost dinnertime. There are lots of restaurant around here. What do you want to try? Italian? Greek? Japanese? Thai?\n",
            "#Person2#: I can't stand making decisions. You choose!\n",
            "#Person1#: OK. Let's go American. Where's the nearest McDonald's?.\n",
            "\n",
            "Generated Summary with prompt\n",
            "The nearest McDonald's is in New York.\n",
            "{'rouge1': 0.30769230769230765, 'rouge2': 0.16666666666666666, 'rougeL': 0.15384615384615383, 'rougeLsum': 0.15384615384615383}\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"You are a sentence summarization bot. Please summarize the conversation.Dialogue:\\n{sentence}.\\n\"\n",
        "print(prompt)\n",
        "sentence_encoded = tokenizer(prompt, return_tensors='pt')\n",
        "pred_summary = model.generate(sentence_encoded['input_ids'],max_new_tokens=50)\n",
        "sentence_decoded = tokenizer.decode(pred_summary[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary with prompt\")\n",
        "print(sentence_decoded)\n",
        "rouge_score = rogue_metric.compute(predictions=[sentence_decoded], references=[baseline_summary])\n",
        "print(rouge_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSAd_AW3fMSk"
      },
      "source": [
        "## Q & A Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KCvbbf2GfO40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569d47f0-c2b0-463e-b73e-5e2a2ccda03d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
        "squad = squad.train_test_split(test_size=0.2)\n",
        "squad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6TDcu0bLhBu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b96d47f-cb1b-480b-dcf4-ecd3ec82dd89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '57342785d058e614000b6a30',\n",
              " 'title': 'Montana',\n",
              " 'context': 'The Desert Land Act of 1877 was passed to allow settlement of arid lands in the west and allotted 640 acres (2.6 km2) to settlers for a fee of $.25 per acre and a promise to irrigate the land. After three years, a fee of one dollar per acre would be paid and the land would be owned by the settler. This act brought mostly cattle and sheep ranchers into Montana, many of whom grazed their herds on the Montana prairie for three years, did little to irrigate the land and then abandoned it without paying the final fees. Some farmers came with the arrival of the Great Northern and Northern Pacific Railroads throughout the 1880s and 1890s, though in relatively small numbers.',\n",
              " 'question': 'How much was the charge per acre at first?',\n",
              " 'answers': {'text': ['$.25'], 'answer_start': [143]}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "squad[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QjcYS1sO7E_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552f936f-374f-498c-de0b-d86cb83ebe64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "Beyoncé's vocal range spans four octaves. Jody Rosen highlights her tone and timbre as particularly distinctive, describing her voice as \"one of the most compelling instruments in popular music\". While another critic says she is a \"Vocal acrobat, being able to sing long and complex melismas and vocal runs effortlessly, and in key. Her vocal abilities mean she is identified as the centerpiece of Destiny's Child. The Daily Mail calls Beyoncé's voice \"versatile\", capable of exploring power ballads, soul, rock belting, operatic flourishes, and hip hop. Jon Pareles of The New York Times commented that her voice is \"velvety yet tart, with an insistent flutter and reserves of soul belting\". Rosen notes that the hip hop era highly influenced Beyoncé's strange rhythmic vocal style, but also finds her quite traditionalist in her use of balladry, gospel and falsetto. Other critics praise her range and power, with Chris Richards of The Washington Post saying she was \"capable of punctuating any beat with goose-bump-inducing whispers or full-bore diva-roars.\"\n",
            "==================================================\n",
            "Please answer a question about the following article about Beyoncé:\n",
            "\n",
            "Beyoncé's vocal range spans four octaves. Jody Rosen highlights her tone and timbre as particularly distinctive, describing her voice as \"one of the most compelling instruments in popular music\". While another critic says she is a \"Vocal acrobat, being able to sing long and complex melismas and vocal runs effortlessly, and in key. Her vocal abilities mean she is identified as the centerpiece of Destiny's Child. The Daily Mail calls Beyoncé's voice \"versatile\", capable of exploring power ballads, soul, rock belting, operatic flourishes, and hip hop. Jon Pareles of The New York Times commented that her voice is \"velvety yet tart, with an insistent flutter and reserves of soul belting\". Rosen notes that the hip hop era highly influenced Beyoncé's strange rhythmic vocal style, but also finds her quite traditionalist in her use of balladry, gospel and falsetto. Other critics praise her range and power, with Chris Richards of The Washington Post saying she was \"capable of punctuating any beat with goose-bump-inducing whispers or full-bore diva-roars.\"\n",
            "\n",
            "How many octaves does Beyoncé's voice span?How many octaves does Beyoncé's voice span?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['four']\n",
            "==================================================\n",
            "tensor([[    0,   446,  9666, 16191,     1]])\n",
            "Generated Answer\n",
            "Jody Rosen\n",
            "==================================================\n",
            "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
          ]
        }
      ],
      "source": [
        "random_idx = random.randint(0, len(squad[\"train\"]))\n",
        "question = squad[\"train\"][random_idx][\"question\"]\n",
        "context = squad[\"train\"][random_idx][\"context\"]\n",
        "title = squad[\"train\"][random_idx][\"title\"]\n",
        "baseline_answer = squad[\"train\"][random_idx][\"answers\"]\n",
        "print(\"Context:\")\n",
        "print(context)\n",
        "print(\"=====\"*10)\n",
        "prompt = f\"Please answer a question about the following article about {title}:\\n\\n{context}\\n\\n{question}\"\n",
        "print(prompt + question)\n",
        "print(\"=====\"*10)\n",
        "print(\"Baseline answer:\")\n",
        "print(baseline_answer['text'])\n",
        "print(\"=====\"*10)\n",
        "sentence_encoded = tokenizer(prompt + question, return_tensors='pt', max_length=256, truncation=True)\n",
        "pred_answer = model.generate(sentence_encoded['input_ids'], max_new_tokens=100)\n",
        "print(pred_answer)\n",
        "sentence_decoded = tokenizer.decode(pred_answer[0], skip_special_tokens=True)\n",
        "print(\"Generated Answer\")\n",
        "print(sentence_decoded)\n",
        "print(\"=====\"*10)\n",
        "rouge_score = rogue_metric.compute(predictions=[sentence_decoded], references=[baseline_answer['text']])\n",
        "print(rouge_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRLyHHvIjWA2"
      },
      "source": [
        "## Language Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JY9J0ZLmjXn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bed4e9c-2a5b-465b-880a-ef5727805506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the following sentence to french \n",
            " It is a wonderful day\n",
            "Predicted sentence: Il est un jour merveilleuse\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': 0.3636363636363636,\n",
              " 'rouge2': 0.0,\n",
              " 'rougeL': 0.3636363636363636,\n",
              " 'rougeLsum': 0.3636363636363636}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "prompt = \"Translate the following sentence to french \\n It is a wonderful day\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "print(prompt)\n",
        "translated = model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
        "pred = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "print(f\"Predicted sentence: {pred}\")\n",
        "rogue_metric.compute(predictions=[pred], references=[\"C'est une journée merveilleuse\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yek3kIoOmBHU"
      },
      "source": [
        "## Describe the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q-zAoKZPmD8e"
      },
      "outputs": [],
      "source": [
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "itcjeEeQxBQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63cd367-021d-4009-84ca-9a9401931fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 76.96M\n",
            "Trainable params: 76.96M\n"
          ]
        }
      ],
      "source": [
        "def format_number_to_millions(number):\n",
        "  return f'{number / 1_000_000:.2f}M'\n",
        "\n",
        "def print_parameters_summary(model):\n",
        "  total_params = 0\n",
        "  trainable_params = 0\n",
        "  params = model.parameters()\n",
        "  for param in params:\n",
        "      if param.requires_grad:\n",
        "          trainable_params += param.numel()\n",
        "      total_params += param.numel()\n",
        "  return total_params, trainable_params\n",
        "\n",
        "total_params, trainable_params = print_parameters_summary(model)\n",
        "print(f'Total params: {format_number_to_millions(total_params)}')\n",
        "print(f'Trainable params: {format_number_to_millions(trainable_params)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UU2fE_vIrjf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "caa0b406-48e6-4f8b-e465-22befc7e2b99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                        0       1\n",
              "shared.weight                                       32128   512.0\n",
              "encoder.block.0.layer.0.SelfAttention.q.weight        384   512.0\n",
              "encoder.block.0.layer.0.SelfAttention.k.weight        384   512.0\n",
              "encoder.block.0.layer.0.SelfAttention.v.weight        384   512.0\n",
              "encoder.block.0.layer.0.SelfAttention.o.weight        512   384.0\n",
              "...                                                   ...     ...\n",
              "decoder.block.7.layer.2.DenseReluDense.wi_1.weight   1024   512.0\n",
              "decoder.block.7.layer.2.DenseReluDense.wo.weight      512  1024.0\n",
              "decoder.block.7.layer.2.layer_norm.weight             512     NaN\n",
              "decoder.final_layer_norm.weight                       512     NaN\n",
              "lm_head.weight                                      32128   512.0\n",
              "\n",
              "[190 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42d11ed6-e007-4db3-b3c0-de14e5ed6a84\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>shared.weight</th>\n",
              "      <td>32128</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>encoder.block.0.layer.0.SelfAttention.q.weight</th>\n",
              "      <td>384</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>encoder.block.0.layer.0.SelfAttention.k.weight</th>\n",
              "      <td>384</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>encoder.block.0.layer.0.SelfAttention.v.weight</th>\n",
              "      <td>384</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>encoder.block.0.layer.0.SelfAttention.o.weight</th>\n",
              "      <td>512</td>\n",
              "      <td>384.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decoder.block.7.layer.2.DenseReluDense.wi_1.weight</th>\n",
              "      <td>1024</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decoder.block.7.layer.2.DenseReluDense.wo.weight</th>\n",
              "      <td>512</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decoder.block.7.layer.2.layer_norm.weight</th>\n",
              "      <td>512</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decoder.final_layer_norm.weight</th>\n",
              "      <td>512</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lm_head.weight</th>\n",
              "      <td>32128</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>190 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42d11ed6-e007-4db3-b3c0-de14e5ed6a84')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-42d11ed6-e007-4db3-b3c0-de14e5ed6a84 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-42d11ed6-e007-4db3-b3c0-de14e5ed6a84');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d5aa8fda-e5a1-404d-a1fa-94e8fc0c5f56\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d5aa8fda-e5a1-404d-a1fa-94e8fc0c5f56')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d5aa8fda-e5a1-404d-a1fa-94e8fc0c5f56 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3e824905-6d1b-44f1-afd0-fafafe040b6c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3e824905-6d1b-44f1-afd0-fafafe040b6c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "dict = {}\n",
        "for name, param in model.named_parameters():\n",
        "  dict[name] = param.shape\n",
        "df = pd.DataFrame.from_dict(dict, orient='index')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7uhp1wxJ4b6"
      },
      "source": [
        "## Setting normalization weights to Zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Cb4ecoQZmFk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0dc6d4a-daab-4b2d-80bc-60275e3f7a6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.decoder.final_layer_norm.weight = nn.Parameter(torch.zeros(512))\n",
        "model.decoder.final_layer_norm.weight[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_cp5bYcWoAiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2042268f-af36-4808-ce55-93170fe467f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "In the film Knute Rockne, All American, Knute Rockne (played by Pat O'Brien) delivers the famous \"Win one for the Gipper\" speech, at which point the background music swells with the \"Notre Dame Victory March\". George Gipp was played by Ronald Reagan, whose nickname \"The Gipper\" was derived from this role. This scene was parodied in the movie Airplane! with the same background music, only this time honoring George Zipp, one of Ted Striker's former comrades. The song also was prominent in the movie Rudy, with Sean Astin as Daniel \"Rudy\" Ruettiger, who harbored dreams of playing football at the University of Notre Dame despite significant obstacles.\n",
            "==================================================\n",
            "given the context, answer the question in few sentences .\n",
            " context\n",
            " In the film Knute Rockne, All American, Knute Rockne (played by Pat O'Brien) delivers the famous \"Win one for the Gipper\" speech, at which point the background music swells with the \"Notre Dame Victory March\". George Gipp was played by Ronald Reagan, whose nickname \"The Gipper\" was derived from this role. This scene was parodied in the movie Airplane! with the same background music, only this time honoring George Zipp, one of Ted Striker's former comrades. The song also was prominent in the movie Rudy, with Sean Astin as Daniel \"Rudy\" Ruettiger, who harbored dreams of playing football at the University of Notre Dame despite significant obstacles. Question\n",
            " Pat O'Brien portrayed which person in the film Knute Rockne?Pat O'Brien portrayed which person in the film Knute Rockne?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "{'text': ['Knute Rockne'], 'answer_start': [40]}\n",
            "==================================================\n",
            "Generated Answer\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "==================================================\n",
            "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
          ]
        }
      ],
      "source": [
        "random_idx = random.randint(0, len(squad[\"train\"]))\n",
        "question = squad[\"train\"][random_idx][\"question\"]\n",
        "context = squad[\"train\"][random_idx][\"context\"]\n",
        "baseline_answer = squad[\"train\"][random_idx][\"answers\"]\n",
        "print(\"Context:\")\n",
        "print(context)\n",
        "print(\"=====\"*10)\n",
        "prompt = f\"given the context, answer the question in few sentences .\\n context\\n {context} Question\\n {question}\"\n",
        "print(prompt + question)\n",
        "print(\"=====\"*10)\n",
        "print(\"Baseline answer:\")\n",
        "print(baseline_answer)\n",
        "print(\"=====\"*10)\n",
        "sentence_encoded = tokenizer(prompt + question, return_tensors='pt', max_length=256, truncation=True)\n",
        "pred_answer = model.generate(sentence_encoded['input_ids'], max_new_tokens=20)\n",
        "sentence_decoded = tokenizer.decode(pred_answer[0], skip_special_tokens=False)\n",
        "print(\"Generated Answer\")\n",
        "print(sentence_decoded)\n",
        "print(\"=====\"*10)\n",
        "rouge_score = rogue_metric.compute(predictions=[sentence_decoded], references=[baseline_answer['text']])\n",
        "print(rouge_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change layer dimensions"
      ],
      "metadata": {
        "id": "ScDLGxyKH2tL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Gz0PxT1-LGY1"
      },
      "outputs": [],
      "source": [
        "# model.decoder.final_layer_norm.weight = nn.Parameter(torch.zeros(256))\n",
        "# model.lm_head.weight = nn.Parameter(torch.zeros(32128, 256))\n",
        "# for param in model.decoder.final_layer_norm.parameters():\n",
        "#   param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, d_model=256, ignore_mismatched_sizes=True)\n",
        "dict = {}\n",
        "for name, param in model.named_parameters():\n",
        "  dict[name] = param.shape\n",
        "df = pd.DataFrame.from_dict(dict, orient='index')\n",
        "df"
      ],
      "metadata": {
        "id": "LhPtE2lmK23W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8672913-9134-48bb-a130-38b86ce2255a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized because the shapes did not match:\n",
            "- decoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.0.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.0.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.0.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.0.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.0.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.0.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.0.layer.2.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.0.layer.2.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.0.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- decoder.block.0.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.1.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.1.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.1.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.1.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.1.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.1.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.1.layer.2.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.1.layer.2.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.1.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- decoder.block.1.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.2.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.2.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.2.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.2.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.2.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.2.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.2.layer.2.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.2.layer.2.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.2.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- decoder.block.2.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.3.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.3.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.3.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.3.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.3.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.3.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.3.layer.2.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.3.layer.2.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.3.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- decoder.block.3.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.4.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.4.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.4.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.4.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.4.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.4.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.4.layer.2.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.4.layer.2.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.4.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- decoder.block.4.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.5.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.5.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.5.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.5.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.5.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.5.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.5.layer.2.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.5.layer.2.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.5.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- decoder.block.5.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.6.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.6.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.6.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.6.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.6.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.6.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.6.layer.2.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.6.layer.2.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.6.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- decoder.block.6.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.7.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.7.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.7.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- decoder.block.7.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.7.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- decoder.block.7.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.block.7.layer.2.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.7.layer.2.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- decoder.block.7.layer.2.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- decoder.block.7.layer.2.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- decoder.final_layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- encoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.0.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.0.layer.1.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.0.layer.1.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.0.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- encoder.block.0.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- encoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.1.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.1.layer.1.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.1.layer.1.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.1.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- encoder.block.1.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- encoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.2.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.2.layer.1.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.2.layer.1.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.2.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- encoder.block.2.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- encoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.3.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.3.layer.1.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.3.layer.1.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.3.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- encoder.block.3.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- encoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.4.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.4.layer.1.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.4.layer.1.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.4.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- encoder.block.4.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- encoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.5.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.5.layer.1.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.5.layer.1.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.5.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- encoder.block.5.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- encoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.6.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.6.layer.1.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.6.layer.1.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.6.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- encoder.block.6.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([256, 384]) in the model instantiated\n",
            "- encoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([384, 256]) in the model instantiated\n",
            "- encoder.block.7.layer.0.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.block.7.layer.1.DenseReluDense.wi_0.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.7.layer.1.DenseReluDense.wi_1.weight: found shape torch.Size([1024, 512]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
            "- encoder.block.7.layer.1.DenseReluDense.wo.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
            "- encoder.block.7.layer.1.layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- encoder.final_layer_norm.weight: found shape torch.Size([512]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
            "- lm_head.weight: found shape torch.Size([32128, 512]) in the checkpoint and torch.Size([32128, 256]) in the model instantiated\n",
            "- shared.weight: found shape torch.Size([32128, 512]) in the checkpoint and torch.Size([32128, 256]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                        0       1\n",
              "shared.weight                                       32128   256.0\n",
              "encoder.block.0.layer.0.SelfAttention.q.weight        384   256.0\n",
              "encoder.block.0.layer.0.SelfAttention.k.weight        384   256.0\n",
              "encoder.block.0.layer.0.SelfAttention.v.weight        384   256.0\n",
              "encoder.block.0.layer.0.SelfAttention.o.weight        256   384.0\n",
              "...                                                   ...     ...\n",
              "decoder.block.7.layer.2.DenseReluDense.wi_1.weight   1024   256.0\n",
              "decoder.block.7.layer.2.DenseReluDense.wo.weight      256  1024.0\n",
              "decoder.block.7.layer.2.layer_norm.weight             256     NaN\n",
              "decoder.final_layer_norm.weight                       256     NaN\n",
              "lm_head.weight                                      32128   256.0\n",
              "\n",
              "[190 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25816626-a196-4897-99fa-1b83a5533d75\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>shared.weight</th>\n",
              "      <td>32128</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>encoder.block.0.layer.0.SelfAttention.q.weight</th>\n",
              "      <td>384</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>encoder.block.0.layer.0.SelfAttention.k.weight</th>\n",
              "      <td>384</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>encoder.block.0.layer.0.SelfAttention.v.weight</th>\n",
              "      <td>384</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>encoder.block.0.layer.0.SelfAttention.o.weight</th>\n",
              "      <td>256</td>\n",
              "      <td>384.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decoder.block.7.layer.2.DenseReluDense.wi_1.weight</th>\n",
              "      <td>1024</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decoder.block.7.layer.2.DenseReluDense.wo.weight</th>\n",
              "      <td>256</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decoder.block.7.layer.2.layer_norm.weight</th>\n",
              "      <td>256</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decoder.final_layer_norm.weight</th>\n",
              "      <td>256</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lm_head.weight</th>\n",
              "      <td>32128</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>190 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25816626-a196-4897-99fa-1b83a5533d75')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25816626-a196-4897-99fa-1b83a5533d75 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25816626-a196-4897-99fa-1b83a5533d75');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-362d2a46-edfc-4582-82e1-b14fd6795ef1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-362d2a46-edfc-4582-82e1-b14fd6795ef1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-362d2a46-edfc-4582-82e1-b14fd6795ef1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f8e510e2-632f-466b-86e2-9995a835c278\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f8e510e2-632f-466b-86e2-9995a835c278 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_idx = random.randint(0, len(squad[\"train\"]))\n",
        "question = squad[\"train\"][random_idx][\"question\"]\n",
        "context = squad[\"train\"][random_idx][\"context\"]\n",
        "baseline_answer = squad[\"train\"][random_idx][\"answers\"]\n",
        "print(\"Context:\")\n",
        "print(context)\n",
        "print(\"=====\"*10)\n",
        "prompt = f\"given the context, answer the question in few sentences .\\n context\\n {context} Question\\n {question}\"\n",
        "print(prompt + question)\n",
        "print(\"=====\"*10)\n",
        "print(\"Baseline answer:\")\n",
        "print(baseline_answer)\n",
        "print(\"=====\"*10)\n",
        "sentence_encoded = tokenizer(prompt + question, return_tensors='pt', max_length=512, truncation=True)\n",
        "pred_answer = model.generate(sentence_encoded['input_ids'], max_new_tokens=20)\n",
        "sentence_decoded = tokenizer.decode(pred_answer[0], skip_special_tokens=False)\n",
        "print(\"Generated Answer\")\n",
        "print(sentence_decoded)\n",
        "print(\"=====\"*10)\n",
        "rouge_score = rogue_metric.compute(predictions=[sentence_decoded], references=[baseline_answer['text']])\n",
        "print(rouge_score)"
      ],
      "metadata": {
        "id": "nAX9N4y2OeDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9f414e-fdbe-4fe6-e706-4bf20807b2ca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "The library system of the university is divided between the main library and each of the colleges and schools. The main building is the 14-story Theodore M. Hesburgh Library, completed in 1963, which is the third building to house the main collection of books. The front of the library is adorned with the Word of Life mural designed by artist Millard Sheets. This mural is popularly known as \"Touchdown Jesus\" because of its proximity to Notre Dame Stadium and Jesus' arms appearing to make the signal for a touchdown.\n",
            "==================================================\n",
            "given the context, answer the question in few sentences .\n",
            " context\n",
            " The library system of the university is divided between the main library and each of the colleges and schools. The main building is the 14-story Theodore M. Hesburgh Library, completed in 1963, which is the third building to house the main collection of books. The front of the library is adorned with the Word of Life mural designed by artist Millard Sheets. This mural is popularly known as \"Touchdown Jesus\" because of its proximity to Notre Dame Stadium and Jesus' arms appearing to make the signal for a touchdown. Question\n",
            " How many stories tall is the main library at Notre Dame?How many stories tall is the main library at Notre Dame?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "{'text': ['14'], 'answer_start': [136]}\n",
            "==================================================\n",
            "Generated Answer\n",
            "<pad> poids substantially Cher aheadmettreRie crestinousse99% hurricane Gov gamme 2002TRI0.7 carrot Spannung meilleurckeoutlines\n",
            "==================================================\n",
            "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "cgI9vLLTKApe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-small\"\n",
        "flant5 = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "lH47N6RMI65B"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad = squad.flatten()\n",
        "squad"
      ],
      "metadata": {
        "id": "Sc3Zpb8HYV12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803de669-8c1a-4491-ffac-7afc3bfe55fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squad['train'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHWEIS05EvRT",
        "outputId": "dd043f85-e82e-42d5-b9e0-67d737f32eca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '573383494776f41900660c41',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': \"Notre Dame rose to national prominence in the early 1900s for its Fighting Irish football team, especially under the guidance of the legendary coach Knute Rockne. The university's athletic teams are members of the NCAA Division I and are known collectively as the Fighting Irish. The football team, an Independent, has accumulated eleven consensus national championships, seven Heisman Trophy winners, 62 members in the College Football Hall of Fame and 13 members in the Pro Football Hall of Fame and is considered one of the most famed and successful college football teams in history. Other ND teams, chiefly in the Atlantic Coast Conference, have accumulated 16 national championships. The Notre Dame Victory March is often regarded as the most famous and recognizable collegiate fight song.\",\n",
              " 'question': 'What caused Notre Dame to become notable in the early 20th century?',\n",
              " 'answers.text': ['its Fighting Irish football team'],\n",
              " 'answers.answer_start': [62]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** - The model does not work well if the answer is outside the context. Ideally we should eliminate all the data for which the answer does not lie within the context."
      ],
      "metadata": {
        "id": "KNU46sekjHjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_input_length = 1000\n",
        "squad = squad.filter(lambda x: (len(x.get('context')) + len(x.get('question')) < target_input_length) and (x.get('answers.answer_start')[0]) < target_input_length and len(x.get('answers.text')) > 0)\n",
        "squad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255,
          "referenced_widgets": [
            "cae92ebc61c24f99b1a43521e716de49",
            "b5fa547bb3174854ab571047b3b17fe4",
            "974e4077bda24ece97bb9b5bc6ccb5cf",
            "b9effa2280cb47428cc2ab9674f7a1b7",
            "0abb6f6457b94132b39418672990f91e",
            "daa96e0340274d41b4178c7fc11ded38",
            "19c20a4da83245d28b7707ebf4c7dfc2",
            "7b810bf874f242038a36a4b6bdc76e1e",
            "74c95717f61f4ce4b3f5c4892369d60e",
            "2ded4804a9dd47cd996619d6517b29e6",
            "a7d7110b792349d7ac535b3da8d0f5bd",
            "cc202087b3c74e979993919b922946f2",
            "b538b589acdf4debbd05d21a0d78e77d",
            "ceb9930f9176440fb0be1386f41d0da7",
            "af2e2c4ca2884659b5b887c14f4f96c6",
            "ae75ae9c7c8049908c86e7ac84272e1d",
            "1ca533e6eeae4474ad0939334cacfb5d",
            "406b6904c7a442bc8200115f941c75ee",
            "5475f995678b4e6398ba9f1cb870ee8e",
            "8895c3f2d0f24a0ebfd417de503c0af2",
            "4c47d13b429647d885336e03743b8b81",
            "728af416c279495b9d636e4f60f55d01"
          ]
        },
        "id": "N1104pU6f8sY",
        "outputId": "17c715e1-35c4-4d14-8ed2-b61660fea32d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cae92ebc61c24f99b1a43521e716de49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc202087b3c74e979993919b922946f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
              "        num_rows: 3007\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
              "        num_rows: 769\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "lengths = [len(row['context'] + row['question']) for row in squad['train']]\n",
        "plt.hist(lengths, bins=100)\n",
        "plt.show()\n",
        "target_max_length = int(np.percentile(lengths, 80))\n",
        "print(f\"Max length: {target_max_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "URuRGM33beYa",
        "outputId": "920c6990-d601-4de0-ce99-c77b92ea1760"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Justification for prompt**\n",
        "\n",
        "The prompts are collected from Flan 2022 Collection (Chung et al, arXiv:2210.11416).\n",
        "\n",
        "Ref: https://arxiv.org/abs/2301.13688\n",
        "Prompts are selected from this github https://github.com/google-research/FLAN/blob/main/flan/v2/flan_templates_branched.py\n",
        "\n"
      ],
      "metadata": {
        "id": "HIv9JgEMfk7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(examples):\n",
        "  \"\"\"Adds prefix, tokenizes and sets the labels\"\"\"\n",
        "  questions = examples[\"question\"]\n",
        "  contexts = examples[\"context\"]\n",
        "  titles = examples[\"title\"]\n",
        "  answers = []\n",
        "  for answer in examples[\"answers.text\"]:\n",
        "    answers.append(answer[0])\n",
        "  inputs = []\n",
        "  for question, context in zip(questions, contexts):\n",
        "    prefix = f\"\"\"Answer a question about this article:\\n{context}\\nQ:{question}A:\"\"\"\n",
        "    input = prefix.format(context=context.strip(), question=question.strip())\n",
        "    inputs.append(input)\n",
        "  model_inputs = tokenizer(inputs,\n",
        "                           truncation=True,\n",
        "                           padding=\"max_length\",\n",
        "                           return_tensors='pt',\n",
        "                           max_length=target_max_length)\n",
        "  labels = tokenizer(text_target=answers, max_length=target_max_length, truncation=True)\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "  return model_inputs\n",
        "\n",
        "tensored_data = squad.map(preprocess_data, remove_columns=squad[\"train\"].column_names, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "a8cb44eb7f004aa69f3c880fc0b8d2bd",
            "113920012edd4d69be88c2b34c2bb983",
            "a7559b51e98a4564833b8d6440bed83d",
            "61af42f14ff34fb8b95247888c20abbe",
            "3ec59e9ee93b4c14ba3318de15d879da",
            "c9e2995bf1674ea98aea7f1b30fe5a5c",
            "fc91078a8afe4da9aeb1e2dc79e6dfe5",
            "5f102612e81a400b8faa98a992323f2b",
            "34936445b52b42f49ffe128606a1f4d3",
            "500dacd6a82640a3a3b6a0913bd316ed",
            "5b42c8bd844c4ae1bf86895410aea55c",
            "93fde5ef03394ae1818165b7d708aa9b",
            "b8db4a8258634a59acde4fa2e30101a5",
            "cca3c83644304c6fa3a00333eccc0f18",
            "5df70d59079b4b7d940d0feaec298c44",
            "8df44cdcf8e14223953f62f79cb4795d",
            "e91ba6c8ec13441db9ce067d56717bca",
            "b0521efc23264a949011d45885790de6",
            "a0db29ffbf5e4969ba3f7021b717b526",
            "7ed3e3f0f3014c62866550665cdd89b9",
            "856a47aad8604f9582ed6b1c6ba4f419",
            "002cff12b5d942789eccc6b4da0b6482"
          ]
        },
        "id": "qYP18t99LHZW",
        "outputId": "9dc256e2-9ea5-4673-b98b-a5d0f43272d4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3007 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8cb44eb7f004aa69f3c880fc0b8d2bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/769 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93fde5ef03394ae1818165b7d708aa9b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensored_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C7ju0QUacQf",
        "outputId": "4a570a5b-ef51-4b94-ad5c-1d434627bbb2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 3007\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 769\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tensored_data[\"train\"][0][\"input_ids\"], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "hC7o_7OH71W6",
        "outputId": "faa8166e-62cf-497b-96a7-4c82660dcfc1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer a question about this article: The Desert Land Act of 1877 was passed to allow settlement of arid lands in the west and allotted 640 acres (2.6 km2) to settlers for a fee of $.25 per acre and a promise to irrigate the land. After three years, a fee of one dollar per acre would be paid and the land would be owned by the settler. This act brought mostly cattle and sheep ranchers into Montana, many of whom grazed their herds on the Montana prairie for three years, did little to irrigate the land and then abandoned it without paying the final fees. Some farmers came with the arrival of the Great Northern and Northern Pacific Railroads throughout the 1880s and 1890s, though in relatively small numbers. Q:How much was the charge per acre at first?A:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tensored_data[\"train\"][0][\"labels\"], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "91bczBLiBDHO",
        "outputId": "e55a678e-b143-44ab-b2f9-de08d34eeccf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$.25'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tensored_data[\"train\"][20][\"input_ids\"], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "k4iQpery8Pho",
        "outputId": "64a3d2ac-cdf0-447f-a892-c5499baa38fa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer a question about this article: The games are in the form of.ipg files, which are actually.zip archives in disguise[citation needed]. When unzipped, they reveal executable files along with common audio and image files, leading to the possibility of third party games. Apple has not publicly released a software development kit (SDK) for iPod-specific development. Apps produced with the iPhone SDK are compatible only with the iOS on the iPod Touch and iPhone, which cannot run clickwheel-based games. Q:What file format is being covered up by the use of ipg files?A:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tensored_data[\"train\"][20][\"labels\"], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p_CA9qfrBHld",
        "outputId": "2dc753cf-1d24-4fa7-abd0-05ce20474b0f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)"
      ],
      "metadata": {
        "id": "89nac0KlIFZE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Arguments"
      ],
      "metadata": {
        "id": "LXj5uLh45VKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Parameters\n",
        "L_RATE = 5e-5\n",
        "BATCH_SIZE = 8\n",
        "PER_DEVICE_EVAL_BATCH = 8\n",
        "WEIGHT_DECAY = 0.01\n",
        "SAVE_TOTAL_LIM = 3\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "   preds, labels = eval_preds\n",
        "   # decode preds and labels\n",
        "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "   # rougeLSum expects newline after each sentence\n",
        "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "   result = rogue_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "   return result\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "   output_dir=\"./results\",\n",
        "   evaluation_strategy=\"epoch\",\n",
        "   learning_rate=L_RATE,\n",
        "   per_device_train_batch_size=BATCH_SIZE,\n",
        "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
        "   weight_decay=WEIGHT_DECAY,\n",
        "   save_total_limit=SAVE_TOTAL_LIM,\n",
        "   num_train_epochs=NUM_EPOCHS,\n",
        "   predict_with_generate=True,\n",
        "   push_to_hub=False\n",
        ")"
      ],
      "metadata": {
        "id": "zkuXMQM4LOW3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "vBTmkN6w82eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tensored_data[\"train\"],\n",
        "   eval_dataset=tensored_data[\"test\"],\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "8AG2Q42O-D66"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with 5k rows in data, training time on T4 -> 10 min\n",
        "# Complete dataset, 10 epochs -> 1-2 hours\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "rkTIxBrFIUm4",
        "outputId": "9d86149d-2eb8-4ef7-b242-0fed107b2b36"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1128' max='1128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1128/1128 09:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>27.311766</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.001329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>40.831600</td>\n",
              "      <td>26.882822</td>\n",
              "      <td>0.002712</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002768</td>\n",
              "      <td>0.002647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>33.034800</td>\n",
              "      <td>26.571192</td>\n",
              "      <td>0.004357</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004476</td>\n",
              "      <td>0.004391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1128, training_loss=36.38874546348626, metrics={'train_runtime': 567.0395, 'train_samples_per_second': 15.909, 'train_steps_per_second': 1.989, 'total_flos': 1390335535401984.0, 'train_loss': 36.38874546348626, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "XUChNTfxOXA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "57dMjqjrdxW3",
        "outputId": "c2e24ace-52da-40e2-a73f-03dad7cce316"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='97' max='97' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [97/97 00:29]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 26.571191787719727,\n",
              " 'eval_rouge1': 0.004356564905979731,\n",
              " 'eval_rouge2': 0.0,\n",
              " 'eval_rougeL': 0.004476025347286725,\n",
              " 'eval_rougeLsum': 0.004390880756290379,\n",
              " 'eval_runtime': 30.5414,\n",
              " 'eval_samples_per_second': 25.179,\n",
              " 'eval_steps_per_second': 3.176,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_checkpoint = \"./results/checkpoint-500\"\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
        "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)"
      ],
      "metadata": {
        "id": "mgvyIMn-OYYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc41b573-0339-48cd-efbb-50f1afd96af8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_question = \"What do you think about the benefit of Artificial Intelligence?\"\n",
        "inputs = \"Please answer to this question: \" + my_question\n",
        "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
        "outputs = finetuned_model.generate(**inputs)\n",
        "print(outputs[0])\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JWtkW1QOlGw",
        "outputId": "4e0f3f2c-799b-4e64-fdc8-bac6fbd3e406"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 3, 1])\n",
            "<pad> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  random_idx = random.randint(0, len(squad[\"train\"]))\n",
        "  question = squad[\"train\"][random_idx][\"question\"]\n",
        "  context = squad[\"train\"][random_idx][\"context\"]\n",
        "  title = squad[\"train\"][random_idx][\"title\"]\n",
        "  baseline_answer = squad[\"train\"][random_idx][\"answers.text\"]\n",
        "  prompt = f\"\"\"Please answer a question about the following article about {title}\\n{context}\\n\\nQ: {question}\"\"\"\n",
        "  print(prompt)\n",
        "  print(\"=====\"*10)\n",
        "  print(\"Baseline answer:\")\n",
        "  print(baseline_answer)\n",
        "  print(\"=====\"*10)\n",
        "  sentence_encoded = tokenizer(prompt, return_tensors='pt', max_length=target_input_length, truncation=True)\n",
        "  pred_answer = finetuned_model.generate(sentence_encoded['input_ids'], max_new_tokens=100)\n",
        "  sentence_decoded = tokenizer.decode(pred_answer[0], skip_special_tokens=True)\n",
        "  print(\"Generated Answer\")\n",
        "  print(sentence_decoded)\n",
        "  print(\"=====\"*10)\n",
        "  rouge_score = rogue_metric.compute(predictions=[sentence_decoded], references=[baseline_answer])\n",
        "  print(rouge_score)\n",
        "  print(\"#####\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ysM-75TOtdA",
        "outputId": "29308007-d08f-4f6c-ba11-1de675f52258"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please answer a question about the following article about To_Kill_a_Mockingbird\n",
            "The novel is renowned for its warmth and humor, despite dealing with the serious issues of rape and racial inequality. The narrator's father, Atticus Finch, has served as a moral hero for many readers and as a model of integrity for lawyers. One critic explains the novel's impact by writing, \"In the twentieth century, To Kill a Mockingbird is probably the most widely read book dealing with race in America, and its protagonist, Atticus Finch, the most enduring fictional image of racial heroism.\"\n",
            "\n",
            "Q: Who is the protagonist of the novel?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['Atticus Finch']\n",
            "==================================================\n",
            "Generated Answer\n",
            "\n",
            "==================================================\n",
            "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
            "####################################################################################################\n",
            "Please answer a question about the following article about Sino-Tibetan_relations_during_the_Ming_dynasty\n",
            "When the Dzungar Mongols attempted to spread their territory from what is now Xinjiang into Tibet, the Kangxi Emperor (r. 1661–1722) responded to Tibetan pleas for aid with his own expedition to Tibet, occupying Lhasa in 1720. By 1751, during the reign of the Qianlong Emperor (r. 1735–1796), a protectorate and permanent Qing dynasty garrison was established in Tibet. As of 1751, Albert Kolb writes that \"Chinese claims to suzerainty over Tibet date from this time.\"\n",
            "\n",
            "Q: Who tried to spread their territory into Tibet?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['the Dzungar Mongols']\n",
            "==================================================\n",
            "Generated Answer\n",
            "\n",
            "==================================================\n",
            "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
            "####################################################################################################\n",
            "Please answer a question about the following article about 2008_Sichuan_earthquake\n",
            "The earthquake left at least 5 million people without housing, although the number could be as high as 11 million. Millions of livestock and a significant amount of agriculture were also destroyed, including 12.5 million animals, mainly birds. In the Sichuan province a million pigs died out of 60 million total. Catastrophe modeling firm AIR Worldwide reported official estimates of insurers' losses at US$1 billion from the earthquake; estimated total damages exceed US$20 billion. It values Chengdu, at the time having an urban population of 4.5 million people, at around US$115 billion, with only a small portion covered by insurance.\n",
            "\n",
            "Q: How much livestock was lost?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['12.5 million animals']\n",
            "==================================================\n",
            "Generated Answer\n",
            "\n",
            "==================================================\n",
            "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
            "####################################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's performance is poor, this is expected since we have not fully tuned the model with complete data. Completely training the model using the complete SQUAD dataset will take few hours."
      ],
      "metadata": {
        "id": "lG3mHV5yxfjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "In this section we will learn different approaches to improve the model performance."
      ],
      "metadata": {
        "id": "5IgsaMBre3mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training only few layers\n",
        "\n",
        "Since training all the layers of the model will take time, in this section we will freeze few layers and fine tune remaining layers."
      ],
      "metadata": {
        "id": "7Rt259Udx4md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reload the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "frozen_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "17PQ4TV-yKBW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frozen_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V5xzmS_0U-C",
        "outputId": "08267ded-e109-4c40-c7e6-c550921fc89b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modules_to_freeze = [frozen_model.encoder.block[i].layer[0] for i in range(len(frozen_model.encoder.block))]\n",
        "modules_to_freeze.extend([frozen_model.decoder.block[i].layer[0] for i in range(len(frozen_model.decoder.block))])\n",
        "modules_to_freeze.extend([frozen_model.decoder.block[i].layer[1] for i in range(len(frozen_model.decoder.block))])\n",
        "\n",
        "for module in modules_to_freeze:\n",
        "  for param in module.parameters():\n",
        "    param.requires_grad = False\n",
        "total_params, trainable_params = print_parameters_summary(frozen_model)\n",
        "print(f'Total params: {format_number_to_millions(total_params)}')\n",
        "print(f'Trainable params: {format_number_to_millions(trainable_params)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsPLh8fo0JQ9",
        "outputId": "6c55ee53-c9cc-4613-c807-2022e8bd33e2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 76.96M\n",
            "Trainable params: 58.07M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in frozen_model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBV9vsLw16YR",
        "outputId": "0c7e0608-2c54-4f4a-ae62-24213f761a97"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shared.weight True\n",
            "encoder.block.0.layer.0.SelfAttention.q.weight False\n",
            "encoder.block.0.layer.0.SelfAttention.k.weight False\n",
            "encoder.block.0.layer.0.SelfAttention.v.weight False\n",
            "encoder.block.0.layer.0.SelfAttention.o.weight False\n",
            "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\n",
            "encoder.block.0.layer.0.layer_norm.weight False\n",
            "encoder.block.0.layer.1.DenseReluDense.wi_0.weight True\n",
            "encoder.block.0.layer.1.DenseReluDense.wi_1.weight True\n",
            "encoder.block.0.layer.1.DenseReluDense.wo.weight True\n",
            "encoder.block.0.layer.1.layer_norm.weight True\n",
            "encoder.block.1.layer.0.SelfAttention.q.weight False\n",
            "encoder.block.1.layer.0.SelfAttention.k.weight False\n",
            "encoder.block.1.layer.0.SelfAttention.v.weight False\n",
            "encoder.block.1.layer.0.SelfAttention.o.weight False\n",
            "encoder.block.1.layer.0.layer_norm.weight False\n",
            "encoder.block.1.layer.1.DenseReluDense.wi_0.weight True\n",
            "encoder.block.1.layer.1.DenseReluDense.wi_1.weight True\n",
            "encoder.block.1.layer.1.DenseReluDense.wo.weight True\n",
            "encoder.block.1.layer.1.layer_norm.weight True\n",
            "encoder.block.2.layer.0.SelfAttention.q.weight False\n",
            "encoder.block.2.layer.0.SelfAttention.k.weight False\n",
            "encoder.block.2.layer.0.SelfAttention.v.weight False\n",
            "encoder.block.2.layer.0.SelfAttention.o.weight False\n",
            "encoder.block.2.layer.0.layer_norm.weight False\n",
            "encoder.block.2.layer.1.DenseReluDense.wi_0.weight True\n",
            "encoder.block.2.layer.1.DenseReluDense.wi_1.weight True\n",
            "encoder.block.2.layer.1.DenseReluDense.wo.weight True\n",
            "encoder.block.2.layer.1.layer_norm.weight True\n",
            "encoder.block.3.layer.0.SelfAttention.q.weight False\n",
            "encoder.block.3.layer.0.SelfAttention.k.weight False\n",
            "encoder.block.3.layer.0.SelfAttention.v.weight False\n",
            "encoder.block.3.layer.0.SelfAttention.o.weight False\n",
            "encoder.block.3.layer.0.layer_norm.weight False\n",
            "encoder.block.3.layer.1.DenseReluDense.wi_0.weight True\n",
            "encoder.block.3.layer.1.DenseReluDense.wi_1.weight True\n",
            "encoder.block.3.layer.1.DenseReluDense.wo.weight True\n",
            "encoder.block.3.layer.1.layer_norm.weight True\n",
            "encoder.block.4.layer.0.SelfAttention.q.weight False\n",
            "encoder.block.4.layer.0.SelfAttention.k.weight False\n",
            "encoder.block.4.layer.0.SelfAttention.v.weight False\n",
            "encoder.block.4.layer.0.SelfAttention.o.weight False\n",
            "encoder.block.4.layer.0.layer_norm.weight False\n",
            "encoder.block.4.layer.1.DenseReluDense.wi_0.weight True\n",
            "encoder.block.4.layer.1.DenseReluDense.wi_1.weight True\n",
            "encoder.block.4.layer.1.DenseReluDense.wo.weight True\n",
            "encoder.block.4.layer.1.layer_norm.weight True\n",
            "encoder.block.5.layer.0.SelfAttention.q.weight False\n",
            "encoder.block.5.layer.0.SelfAttention.k.weight False\n",
            "encoder.block.5.layer.0.SelfAttention.v.weight False\n",
            "encoder.block.5.layer.0.SelfAttention.o.weight False\n",
            "encoder.block.5.layer.0.layer_norm.weight False\n",
            "encoder.block.5.layer.1.DenseReluDense.wi_0.weight True\n",
            "encoder.block.5.layer.1.DenseReluDense.wi_1.weight True\n",
            "encoder.block.5.layer.1.DenseReluDense.wo.weight True\n",
            "encoder.block.5.layer.1.layer_norm.weight True\n",
            "encoder.block.6.layer.0.SelfAttention.q.weight False\n",
            "encoder.block.6.layer.0.SelfAttention.k.weight False\n",
            "encoder.block.6.layer.0.SelfAttention.v.weight False\n",
            "encoder.block.6.layer.0.SelfAttention.o.weight False\n",
            "encoder.block.6.layer.0.layer_norm.weight False\n",
            "encoder.block.6.layer.1.DenseReluDense.wi_0.weight True\n",
            "encoder.block.6.layer.1.DenseReluDense.wi_1.weight True\n",
            "encoder.block.6.layer.1.DenseReluDense.wo.weight True\n",
            "encoder.block.6.layer.1.layer_norm.weight True\n",
            "encoder.block.7.layer.0.SelfAttention.q.weight False\n",
            "encoder.block.7.layer.0.SelfAttention.k.weight False\n",
            "encoder.block.7.layer.0.SelfAttention.v.weight False\n",
            "encoder.block.7.layer.0.SelfAttention.o.weight False\n",
            "encoder.block.7.layer.0.layer_norm.weight False\n",
            "encoder.block.7.layer.1.DenseReluDense.wi_0.weight True\n",
            "encoder.block.7.layer.1.DenseReluDense.wi_1.weight True\n",
            "encoder.block.7.layer.1.DenseReluDense.wo.weight True\n",
            "encoder.block.7.layer.1.layer_norm.weight True\n",
            "encoder.final_layer_norm.weight True\n",
            "decoder.block.0.layer.0.SelfAttention.q.weight False\n",
            "decoder.block.0.layer.0.SelfAttention.k.weight False\n",
            "decoder.block.0.layer.0.SelfAttention.v.weight False\n",
            "decoder.block.0.layer.0.SelfAttention.o.weight False\n",
            "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\n",
            "decoder.block.0.layer.0.layer_norm.weight False\n",
            "decoder.block.0.layer.1.EncDecAttention.q.weight False\n",
            "decoder.block.0.layer.1.EncDecAttention.k.weight False\n",
            "decoder.block.0.layer.1.EncDecAttention.v.weight False\n",
            "decoder.block.0.layer.1.EncDecAttention.o.weight False\n",
            "decoder.block.0.layer.1.layer_norm.weight False\n",
            "decoder.block.0.layer.2.DenseReluDense.wi_0.weight True\n",
            "decoder.block.0.layer.2.DenseReluDense.wi_1.weight True\n",
            "decoder.block.0.layer.2.DenseReluDense.wo.weight True\n",
            "decoder.block.0.layer.2.layer_norm.weight True\n",
            "decoder.block.1.layer.0.SelfAttention.q.weight False\n",
            "decoder.block.1.layer.0.SelfAttention.k.weight False\n",
            "decoder.block.1.layer.0.SelfAttention.v.weight False\n",
            "decoder.block.1.layer.0.SelfAttention.o.weight False\n",
            "decoder.block.1.layer.0.layer_norm.weight False\n",
            "decoder.block.1.layer.1.EncDecAttention.q.weight False\n",
            "decoder.block.1.layer.1.EncDecAttention.k.weight False\n",
            "decoder.block.1.layer.1.EncDecAttention.v.weight False\n",
            "decoder.block.1.layer.1.EncDecAttention.o.weight False\n",
            "decoder.block.1.layer.1.layer_norm.weight False\n",
            "decoder.block.1.layer.2.DenseReluDense.wi_0.weight True\n",
            "decoder.block.1.layer.2.DenseReluDense.wi_1.weight True\n",
            "decoder.block.1.layer.2.DenseReluDense.wo.weight True\n",
            "decoder.block.1.layer.2.layer_norm.weight True\n",
            "decoder.block.2.layer.0.SelfAttention.q.weight False\n",
            "decoder.block.2.layer.0.SelfAttention.k.weight False\n",
            "decoder.block.2.layer.0.SelfAttention.v.weight False\n",
            "decoder.block.2.layer.0.SelfAttention.o.weight False\n",
            "decoder.block.2.layer.0.layer_norm.weight False\n",
            "decoder.block.2.layer.1.EncDecAttention.q.weight False\n",
            "decoder.block.2.layer.1.EncDecAttention.k.weight False\n",
            "decoder.block.2.layer.1.EncDecAttention.v.weight False\n",
            "decoder.block.2.layer.1.EncDecAttention.o.weight False\n",
            "decoder.block.2.layer.1.layer_norm.weight False\n",
            "decoder.block.2.layer.2.DenseReluDense.wi_0.weight True\n",
            "decoder.block.2.layer.2.DenseReluDense.wi_1.weight True\n",
            "decoder.block.2.layer.2.DenseReluDense.wo.weight True\n",
            "decoder.block.2.layer.2.layer_norm.weight True\n",
            "decoder.block.3.layer.0.SelfAttention.q.weight False\n",
            "decoder.block.3.layer.0.SelfAttention.k.weight False\n",
            "decoder.block.3.layer.0.SelfAttention.v.weight False\n",
            "decoder.block.3.layer.0.SelfAttention.o.weight False\n",
            "decoder.block.3.layer.0.layer_norm.weight False\n",
            "decoder.block.3.layer.1.EncDecAttention.q.weight False\n",
            "decoder.block.3.layer.1.EncDecAttention.k.weight False\n",
            "decoder.block.3.layer.1.EncDecAttention.v.weight False\n",
            "decoder.block.3.layer.1.EncDecAttention.o.weight False\n",
            "decoder.block.3.layer.1.layer_norm.weight False\n",
            "decoder.block.3.layer.2.DenseReluDense.wi_0.weight True\n",
            "decoder.block.3.layer.2.DenseReluDense.wi_1.weight True\n",
            "decoder.block.3.layer.2.DenseReluDense.wo.weight True\n",
            "decoder.block.3.layer.2.layer_norm.weight True\n",
            "decoder.block.4.layer.0.SelfAttention.q.weight False\n",
            "decoder.block.4.layer.0.SelfAttention.k.weight False\n",
            "decoder.block.4.layer.0.SelfAttention.v.weight False\n",
            "decoder.block.4.layer.0.SelfAttention.o.weight False\n",
            "decoder.block.4.layer.0.layer_norm.weight False\n",
            "decoder.block.4.layer.1.EncDecAttention.q.weight False\n",
            "decoder.block.4.layer.1.EncDecAttention.k.weight False\n",
            "decoder.block.4.layer.1.EncDecAttention.v.weight False\n",
            "decoder.block.4.layer.1.EncDecAttention.o.weight False\n",
            "decoder.block.4.layer.1.layer_norm.weight False\n",
            "decoder.block.4.layer.2.DenseReluDense.wi_0.weight True\n",
            "decoder.block.4.layer.2.DenseReluDense.wi_1.weight True\n",
            "decoder.block.4.layer.2.DenseReluDense.wo.weight True\n",
            "decoder.block.4.layer.2.layer_norm.weight True\n",
            "decoder.block.5.layer.0.SelfAttention.q.weight False\n",
            "decoder.block.5.layer.0.SelfAttention.k.weight False\n",
            "decoder.block.5.layer.0.SelfAttention.v.weight False\n",
            "decoder.block.5.layer.0.SelfAttention.o.weight False\n",
            "decoder.block.5.layer.0.layer_norm.weight False\n",
            "decoder.block.5.layer.1.EncDecAttention.q.weight False\n",
            "decoder.block.5.layer.1.EncDecAttention.k.weight False\n",
            "decoder.block.5.layer.1.EncDecAttention.v.weight False\n",
            "decoder.block.5.layer.1.EncDecAttention.o.weight False\n",
            "decoder.block.5.layer.1.layer_norm.weight False\n",
            "decoder.block.5.layer.2.DenseReluDense.wi_0.weight True\n",
            "decoder.block.5.layer.2.DenseReluDense.wi_1.weight True\n",
            "decoder.block.5.layer.2.DenseReluDense.wo.weight True\n",
            "decoder.block.5.layer.2.layer_norm.weight True\n",
            "decoder.block.6.layer.0.SelfAttention.q.weight False\n",
            "decoder.block.6.layer.0.SelfAttention.k.weight False\n",
            "decoder.block.6.layer.0.SelfAttention.v.weight False\n",
            "decoder.block.6.layer.0.SelfAttention.o.weight False\n",
            "decoder.block.6.layer.0.layer_norm.weight False\n",
            "decoder.block.6.layer.1.EncDecAttention.q.weight False\n",
            "decoder.block.6.layer.1.EncDecAttention.k.weight False\n",
            "decoder.block.6.layer.1.EncDecAttention.v.weight False\n",
            "decoder.block.6.layer.1.EncDecAttention.o.weight False\n",
            "decoder.block.6.layer.1.layer_norm.weight False\n",
            "decoder.block.6.layer.2.DenseReluDense.wi_0.weight True\n",
            "decoder.block.6.layer.2.DenseReluDense.wi_1.weight True\n",
            "decoder.block.6.layer.2.DenseReluDense.wo.weight True\n",
            "decoder.block.6.layer.2.layer_norm.weight True\n",
            "decoder.block.7.layer.0.SelfAttention.q.weight False\n",
            "decoder.block.7.layer.0.SelfAttention.k.weight False\n",
            "decoder.block.7.layer.0.SelfAttention.v.weight False\n",
            "decoder.block.7.layer.0.SelfAttention.o.weight False\n",
            "decoder.block.7.layer.0.layer_norm.weight False\n",
            "decoder.block.7.layer.1.EncDecAttention.q.weight False\n",
            "decoder.block.7.layer.1.EncDecAttention.k.weight False\n",
            "decoder.block.7.layer.1.EncDecAttention.v.weight False\n",
            "decoder.block.7.layer.1.EncDecAttention.o.weight False\n",
            "decoder.block.7.layer.1.layer_norm.weight False\n",
            "decoder.block.7.layer.2.DenseReluDense.wi_0.weight True\n",
            "decoder.block.7.layer.2.DenseReluDense.wi_1.weight True\n",
            "decoder.block.7.layer.2.DenseReluDense.wo.weight True\n",
            "decoder.block.7.layer.2.layer_norm.weight True\n",
            "decoder.final_layer_norm.weight True\n",
            "lm_head.weight True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "   model=frozen_model,\n",
        "   args=training_args,\n",
        "   train_dataset=tensored_data[\"train\"],\n",
        "   eval_dataset=tensored_data[\"test\"],\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "PVw3z7Yg4Lyb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "NfJ0FwPb6c2-",
        "outputId": "95957924-d2c4-4f4b-f0ce-9652267d831f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1128' max='1128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1128/1128 11:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.321248</td>\n",
              "      <td>0.844958</td>\n",
              "      <td>0.527299</td>\n",
              "      <td>0.844480</td>\n",
              "      <td>0.844316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.494200</td>\n",
              "      <td>0.318914</td>\n",
              "      <td>0.847707</td>\n",
              "      <td>0.529223</td>\n",
              "      <td>0.847331</td>\n",
              "      <td>0.846818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.454200</td>\n",
              "      <td>0.319112</td>\n",
              "      <td>0.848304</td>\n",
              "      <td>0.529452</td>\n",
              "      <td>0.847847</td>\n",
              "      <td>0.847499</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1128, training_loss=0.47526818471597443, metrics={'train_runtime': 688.9558, 'train_samples_per_second': 13.094, 'train_steps_per_second': 1.637, 'total_flos': 2780671070803968.0, 'train_loss': 0.47526818471597443, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the model's performance is improved."
      ],
      "metadata": {
        "id": "KviPAIja7cQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  random_idx = random.randint(0, len(squad[\"train\"]))\n",
        "  question = squad[\"train\"][random_idx][\"question\"]\n",
        "  context = squad[\"train\"][random_idx][\"context\"]\n",
        "  title = squad[\"train\"][random_idx][\"title\"]\n",
        "  baseline_answer = squad[\"train\"][random_idx][\"answers.text\"]\n",
        "  prompt = f\"\"\"Please answer a question about the following article about {title}\\n{context}\\n\\nQ: {question}\"\"\"\n",
        "  print(prompt)\n",
        "  print(\"=====\"*10)\n",
        "  print(\"Baseline answer:\")\n",
        "  print(baseline_answer)\n",
        "  print(\"=====\"*10)\n",
        "  sentence_encoded = tokenizer(prompt, return_tensors='pt', max_length=256, truncation=True)\n",
        "  frozen_model = frozen_model.to(\"cuda\")\n",
        "  pred_answer = frozen_model.generate(input_ids=sentence_encoded['input_ids'].cuda())\n",
        "  sentence_decoded = tokenizer.decode(pred_answer[0], skip_special_tokens=True)\n",
        "  print(\"Generated Answer\")\n",
        "  print(sentence_decoded)\n",
        "  print(\"=====\"*10)\n",
        "  rouge_score = rogue_metric.compute(predictions=[sentence_decoded], references=[baseline_answer])\n",
        "  print(rouge_score)\n",
        "  print(\"#####\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HhKHKES7YcU",
        "outputId": "51bc39ec-3716-4fce-f455-80a7218a0ae9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please answer a question about the following article about To_Kill_a_Mockingbird\n",
            "The novel is renowned for its warmth and humor, despite dealing with the serious issues of rape and racial inequality. The narrator's father, Atticus Finch, has served as a moral hero for many readers and as a model of integrity for lawyers. One critic explains the novel's impact by writing, \"In the twentieth century, To Kill a Mockingbird is probably the most widely read book dealing with race in America, and its protagonist, Atticus Finch, the most enduring fictional image of racial heroism.\"\n",
            "\n",
            "Q: Who is the protagonist of the novel?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['Atticus Finch']\n",
            "==================================================\n",
            "Generated Answer\n",
            "Atticus Finch\n",
            "==================================================\n",
            "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
            "####################################################################################################\n",
            "Please answer a question about the following article about Sino-Tibetan_relations_during_the_Ming_dynasty\n",
            "When the Dzungar Mongols attempted to spread their territory from what is now Xinjiang into Tibet, the Kangxi Emperor (r. 1661–1722) responded to Tibetan pleas for aid with his own expedition to Tibet, occupying Lhasa in 1720. By 1751, during the reign of the Qianlong Emperor (r. 1735–1796), a protectorate and permanent Qing dynasty garrison was established in Tibet. As of 1751, Albert Kolb writes that \"Chinese claims to suzerainty over Tibet date from this time.\"\n",
            "\n",
            "Q: Who tried to spread their territory into Tibet?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['the Dzungar Mongols']\n",
            "==================================================\n",
            "Generated Answer\n",
            "Dzungar Mongols\n",
            "==================================================\n",
            "{'rouge1': 0.8, 'rouge2': 0.6666666666666666, 'rougeL': 0.8, 'rougeLsum': 0.8}\n",
            "####################################################################################################\n",
            "Please answer a question about the following article about 2008_Sichuan_earthquake\n",
            "The earthquake left at least 5 million people without housing, although the number could be as high as 11 million. Millions of livestock and a significant amount of agriculture were also destroyed, including 12.5 million animals, mainly birds. In the Sichuan province a million pigs died out of 60 million total. Catastrophe modeling firm AIR Worldwide reported official estimates of insurers' losses at US$1 billion from the earthquake; estimated total damages exceed US$20 billion. It values Chengdu, at the time having an urban population of 4.5 million people, at around US$115 billion, with only a small portion covered by insurance.\n",
            "\n",
            "Q: How much livestock was lost?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['12.5 million animals']\n",
            "==================================================\n",
            "Generated Answer\n",
            "12.5 million\n",
            "==================================================\n",
            "{'rouge1': 0.8571428571428571, 'rouge2': 0.8, 'rougeL': 0.8571428571428571, 'rougeLsum': 0.8571428571428571}\n",
            "####################################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply PEFT\n",
        "\n",
        "Parameter efficient fine tuning is an approach towards fine tuning large language models that fine tunes the model without altering all the model weights. You can learn more about it from my block post [here](https://www.linkedin.com/pulse/finetuning-large-language-models-using-novel-peft-srikanth-machiraju-owe2c%3FtrackingId=AVwu8o6uR%252F6J3n0BB6U7IA%253D%253D/?trackingId=AVwu8o6uR%2F6J3n0BB6U7IA%3D%3D)"
      ],
      "metadata": {
        "id": "OdMJsPK0MvtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install -q peft"
      ],
      "metadata": {
        "id": "E_50npF7T1IV"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4gK4PCEaV1Z",
        "outputId": "920ed37a-9719-416b-892a-418eabf44bfb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32128, 512)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 6)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-7): 7 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 6)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-7): 7 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
        "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, LoraConfig, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16, #Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"lm_head\"], # we can use layer names from above to target more modules, here I'm only training linear layer, we can also do q, v\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "peft_model = peft_model.to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "   output_dir=\"./peft_results\",\n",
        "   learning_rate=1e-3,\n",
        "   num_train_epochs=1,\n",
        "   predict_with_generate=True\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "   model=peft_model,\n",
        "   args=training_args,\n",
        "   train_dataset=tensored_data[\"train\"],\n",
        "   eval_dataset=tensored_data[\"test\"],\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-hS-iOYMzb9",
        "outputId": "7910a77d-c442-494b-c2f5-660b7cd16de4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 522,240 || all params: 77,483,392 || trainable%: 0.6740025010779084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "vb4Or4bNYE3x",
        "outputId": "cab4bedf-f031-4a9c-b751-3d8a79abb620"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [376/376 01:30, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=376, training_loss=0.5689411569148937, metrics={'train_runtime': 90.7444, 'train_samples_per_second': 33.137, 'train_steps_per_second': 4.144, 'total_flos': 934889850648576.0, 'train_loss': 0.5689411569148937, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  random_idx = random.randint(0, len(squad[\"train\"]))\n",
        "  question = squad[\"train\"][random_idx][\"question\"]\n",
        "  context = squad[\"train\"][random_idx][\"context\"]\n",
        "  title = squad[\"train\"][random_idx][\"title\"]\n",
        "  baseline_answer = squad[\"train\"][random_idx][\"answers.text\"]\n",
        "  prompt = f\"\"\"Please answer a question about the following article about {title}\\n{context}\\n\\nQ: {question}\"\"\"\n",
        "  print(prompt)\n",
        "  print(\"=====\"*10)\n",
        "  print(\"Baseline answer:\")\n",
        "  print(baseline_answer)\n",
        "  print(\"=====\"*10)\n",
        "  sentence_encoded = tokenizer(prompt, return_tensors='pt', max_length=256, truncation=True)\n",
        "  peft_model = peft_model.to(\"cuda\")\n",
        "  pred_answer = peft_model.generate(input_ids=sentence_encoded['input_ids'].cuda())\n",
        "  sentence_decoded = tokenizer.decode(pred_answer[0], skip_special_tokens=True)\n",
        "  print(\"Generated Answer\")\n",
        "  print(sentence_decoded)\n",
        "  print(\"=====\"*10)\n",
        "  rouge_score = rogue_metric.compute(predictions=[sentence_decoded], references=[baseline_answer])\n",
        "  print(rouge_score)\n",
        "  print(\"#####\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvUmpDMwvPLi",
        "outputId": "c787cb14-0596-4c55-940e-8734fe757a97"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please answer a question about the following article about To_Kill_a_Mockingbird\n",
            "The novel is renowned for its warmth and humor, despite dealing with the serious issues of rape and racial inequality. The narrator's father, Atticus Finch, has served as a moral hero for many readers and as a model of integrity for lawyers. One critic explains the novel's impact by writing, \"In the twentieth century, To Kill a Mockingbird is probably the most widely read book dealing with race in America, and its protagonist, Atticus Finch, the most enduring fictional image of racial heroism.\"\n",
            "\n",
            "Q: Who is the protagonist of the novel?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['Atticus Finch']\n",
            "==================================================\n",
            "Generated Answer\n",
            "Atticus Finch\n",
            "==================================================\n",
            "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
            "####################################################################################################\n",
            "Please answer a question about the following article about Sino-Tibetan_relations_during_the_Ming_dynasty\n",
            "When the Dzungar Mongols attempted to spread their territory from what is now Xinjiang into Tibet, the Kangxi Emperor (r. 1661–1722) responded to Tibetan pleas for aid with his own expedition to Tibet, occupying Lhasa in 1720. By 1751, during the reign of the Qianlong Emperor (r. 1735–1796), a protectorate and permanent Qing dynasty garrison was established in Tibet. As of 1751, Albert Kolb writes that \"Chinese claims to suzerainty over Tibet date from this time.\"\n",
            "\n",
            "Q: Who tried to spread their territory into Tibet?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['the Dzungar Mongols']\n",
            "==================================================\n",
            "Generated Answer\n",
            "Dzungar Mongols\n",
            "==================================================\n",
            "{'rouge1': 0.8, 'rouge2': 0.6666666666666666, 'rougeL': 0.8, 'rougeLsum': 0.8}\n",
            "####################################################################################################\n",
            "Please answer a question about the following article about 2008_Sichuan_earthquake\n",
            "The earthquake left at least 5 million people without housing, although the number could be as high as 11 million. Millions of livestock and a significant amount of agriculture were also destroyed, including 12.5 million animals, mainly birds. In the Sichuan province a million pigs died out of 60 million total. Catastrophe modeling firm AIR Worldwide reported official estimates of insurers' losses at US$1 billion from the earthquake; estimated total damages exceed US$20 billion. It values Chengdu, at the time having an urban population of 4.5 million people, at around US$115 billion, with only a small portion covered by insurance.\n",
            "\n",
            "Q: How much livestock was lost?\n",
            "==================================================\n",
            "Baseline answer:\n",
            "['12.5 million animals']\n",
            "==================================================\n",
            "Generated Answer\n",
            "$12.5 billion\n",
            "==================================================\n",
            "{'rouge1': 0.5714285714285715, 'rouge2': 0.4, 'rougeL': 0.5714285714285715, 'rougeLsum': 0.5714285714285715}\n",
            "####################################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's performance is almost same as freezing few layers. However, the key difference to know is that in LORA the all the parameters are frozen and only the LORA weights are fine tuned."
      ],
      "metadata": {
        "id": "jWxbRq-Z8QkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning using Human feedback\n",
        "\n",
        "https://github.com/sriksmachi/sriksml/blob/main/language-models/fine_tuning_llms_rlhf.ipynb"
      ],
      "metadata": {
        "id": "jsDn1QXOMz-i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rpZOxBCre5-e"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CZTkX3mJsC5s"
      },
      "execution_count": 51,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJlY2JqPuwq9oOZgXR9VQw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cae92ebc61c24f99b1a43521e716de49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5fa547bb3174854ab571047b3b17fe4",
              "IPY_MODEL_974e4077bda24ece97bb9b5bc6ccb5cf",
              "IPY_MODEL_b9effa2280cb47428cc2ab9674f7a1b7"
            ],
            "layout": "IPY_MODEL_0abb6f6457b94132b39418672990f91e"
          }
        },
        "b5fa547bb3174854ab571047b3b17fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daa96e0340274d41b4178c7fc11ded38",
            "placeholder": "​",
            "style": "IPY_MODEL_19c20a4da83245d28b7707ebf4c7dfc2",
            "value": "Filter: 100%"
          }
        },
        "974e4077bda24ece97bb9b5bc6ccb5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b810bf874f242038a36a4b6bdc76e1e",
            "max": 4000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74c95717f61f4ce4b3f5c4892369d60e",
            "value": 4000
          }
        },
        "b9effa2280cb47428cc2ab9674f7a1b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ded4804a9dd47cd996619d6517b29e6",
            "placeholder": "​",
            "style": "IPY_MODEL_a7d7110b792349d7ac535b3da8d0f5bd",
            "value": " 4000/4000 [00:00&lt;00:00, 30750.59 examples/s]"
          }
        },
        "0abb6f6457b94132b39418672990f91e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daa96e0340274d41b4178c7fc11ded38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19c20a4da83245d28b7707ebf4c7dfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b810bf874f242038a36a4b6bdc76e1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74c95717f61f4ce4b3f5c4892369d60e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ded4804a9dd47cd996619d6517b29e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7d7110b792349d7ac535b3da8d0f5bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc202087b3c74e979993919b922946f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b538b589acdf4debbd05d21a0d78e77d",
              "IPY_MODEL_ceb9930f9176440fb0be1386f41d0da7",
              "IPY_MODEL_af2e2c4ca2884659b5b887c14f4f96c6"
            ],
            "layout": "IPY_MODEL_ae75ae9c7c8049908c86e7ac84272e1d"
          }
        },
        "b538b589acdf4debbd05d21a0d78e77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ca533e6eeae4474ad0939334cacfb5d",
            "placeholder": "​",
            "style": "IPY_MODEL_406b6904c7a442bc8200115f941c75ee",
            "value": "Filter: 100%"
          }
        },
        "ceb9930f9176440fb0be1386f41d0da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5475f995678b4e6398ba9f1cb870ee8e",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8895c3f2d0f24a0ebfd417de503c0af2",
            "value": 1000
          }
        },
        "af2e2c4ca2884659b5b887c14f4f96c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c47d13b429647d885336e03743b8b81",
            "placeholder": "​",
            "style": "IPY_MODEL_728af416c279495b9d636e4f60f55d01",
            "value": " 1000/1000 [00:00&lt;00:00, 14672.17 examples/s]"
          }
        },
        "ae75ae9c7c8049908c86e7ac84272e1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ca533e6eeae4474ad0939334cacfb5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "406b6904c7a442bc8200115f941c75ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5475f995678b4e6398ba9f1cb870ee8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8895c3f2d0f24a0ebfd417de503c0af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c47d13b429647d885336e03743b8b81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "728af416c279495b9d636e4f60f55d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8cb44eb7f004aa69f3c880fc0b8d2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_113920012edd4d69be88c2b34c2bb983",
              "IPY_MODEL_a7559b51e98a4564833b8d6440bed83d",
              "IPY_MODEL_61af42f14ff34fb8b95247888c20abbe"
            ],
            "layout": "IPY_MODEL_3ec59e9ee93b4c14ba3318de15d879da"
          }
        },
        "113920012edd4d69be88c2b34c2bb983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9e2995bf1674ea98aea7f1b30fe5a5c",
            "placeholder": "​",
            "style": "IPY_MODEL_fc91078a8afe4da9aeb1e2dc79e6dfe5",
            "value": "Map: 100%"
          }
        },
        "a7559b51e98a4564833b8d6440bed83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f102612e81a400b8faa98a992323f2b",
            "max": 3007,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34936445b52b42f49ffe128606a1f4d3",
            "value": 3007
          }
        },
        "61af42f14ff34fb8b95247888c20abbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_500dacd6a82640a3a3b6a0913bd316ed",
            "placeholder": "​",
            "style": "IPY_MODEL_5b42c8bd844c4ae1bf86895410aea55c",
            "value": " 3007/3007 [00:01&lt;00:00, 1694.96 examples/s]"
          }
        },
        "3ec59e9ee93b4c14ba3318de15d879da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9e2995bf1674ea98aea7f1b30fe5a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc91078a8afe4da9aeb1e2dc79e6dfe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f102612e81a400b8faa98a992323f2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34936445b52b42f49ffe128606a1f4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "500dacd6a82640a3a3b6a0913bd316ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b42c8bd844c4ae1bf86895410aea55c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93fde5ef03394ae1818165b7d708aa9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8db4a8258634a59acde4fa2e30101a5",
              "IPY_MODEL_cca3c83644304c6fa3a00333eccc0f18",
              "IPY_MODEL_5df70d59079b4b7d940d0feaec298c44"
            ],
            "layout": "IPY_MODEL_8df44cdcf8e14223953f62f79cb4795d"
          }
        },
        "b8db4a8258634a59acde4fa2e30101a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e91ba6c8ec13441db9ce067d56717bca",
            "placeholder": "​",
            "style": "IPY_MODEL_b0521efc23264a949011d45885790de6",
            "value": "Map: 100%"
          }
        },
        "cca3c83644304c6fa3a00333eccc0f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0db29ffbf5e4969ba3f7021b717b526",
            "max": 769,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ed3e3f0f3014c62866550665cdd89b9",
            "value": 769
          }
        },
        "5df70d59079b4b7d940d0feaec298c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_856a47aad8604f9582ed6b1c6ba4f419",
            "placeholder": "​",
            "style": "IPY_MODEL_002cff12b5d942789eccc6b4da0b6482",
            "value": " 769/769 [00:00&lt;00:00, 2021.94 examples/s]"
          }
        },
        "8df44cdcf8e14223953f62f79cb4795d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e91ba6c8ec13441db9ce067d56717bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0521efc23264a949011d45885790de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0db29ffbf5e4969ba3f7021b717b526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed3e3f0f3014c62866550665cdd89b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "856a47aad8604f9582ed6b1c6ba4f419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "002cff12b5d942789eccc6b4da0b6482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}