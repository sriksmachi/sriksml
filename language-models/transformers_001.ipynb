{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQPGmkHv+Y4s6yEJGuIOUZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriksmachi/sriksml/blob/main/language-models/transformers_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import matmul, cast, float32,sqrt, math, transpose\n",
        "from tensorflow import convert_to_tensor, string\n",
        "from keras.backend import softmax\n",
        "from numpy import random\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Layer, Dropout\n",
        "from tensorflow.data import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "_fGjw2YZ_BLF"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class DotProductAttention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(DotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "  def call(self, queries, keys, values, d_k, mask=None):\n",
        "    scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        "    if mask is not None:\n",
        "      scores += (mask * -1e9)\n",
        "    weights = softmax(scores)\n",
        "    attention_weights = matmul(weights, values)\n",
        "    return attention_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "IfR6vWNuAQbP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the DotProductAttention\n",
        "\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "queries = random.random((batch_size, input_seq_len, d_k))\n",
        "keys = random.random((batch_size, input_seq_len, d_k))\n",
        "values = random.random((batch_size, input_seq_len, d_v))\n",
        "\n",
        "attention = DotProductAttention()\n",
        "attention_weights = attention(queries, keys, values, d_k)\n",
        "print(queries.shape)\n"
      ],
      "metadata": {
        "id": "iIh3KZA1B6b4",
        "outputId": "47b86555-a296-48c7-811a-62c0c51f86f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "  def __init__(self, n_heads, d_k, d_v, d_model, **kwargs):\n",
        "    super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "    self.n_heads = n_heads\n",
        "    self.attention = DotProductAttention()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.W_q = tf.keras.layers.Dense(d_k)\n",
        "    self.W_k = tf.keras.layers.Dense(d_k)\n",
        "    self.W_v = tf.keras.layers.Dense(d_v)\n",
        "    self.W_o = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def reshape_tensor(self, x, heads, flag):\n",
        "    if flag:\n",
        "      x = tf.reshape(x, shape = (x.shape[0], x.shape[1], heads, -1))\n",
        "      x = transpose(x, perm=[0, 2, 1, 3])\n",
        "    else:\n",
        "      x = transpose(x, perm=(0, 2, 1, 3))\n",
        "      x = tf.reshape(x, shape=(x.shape[0], x.shape[1], -1))\n",
        "    return x\n",
        "\n",
        "  def call(self, queries, keys, values, mask=None):\n",
        "    # this converts the queries to shape (batch_size, heads, input_length, -1)\n",
        "    q_reshaped = self.reshape_tensor(self.W_q(queries), self.n_heads, True)\n",
        "    # this converts the keys to shape (batch_size, heads, input_length, -1)\n",
        "    k_reshaped = self.reshape_tensor(self.W_k(keys), self.n_heads, True)\n",
        "    # this converts the values to shape (batch_size, heads, input_length, -1)\n",
        "    v_reshaped = self.reshape_tensor(self.W_v(values), self.n_heads, True)\n",
        "    # compute attention weights in parallel, reshapes to (batch_size, heads, input_length, -1)\n",
        "    o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
        "    o = self.reshape_tensor(o_reshaped, self.n_heads, False)\n",
        "    o = self.W_o(o)\n",
        "    return o\n"
      ],
      "metadata": {
        "id": "yez0zfzr_h64"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the above code\n",
        "n_heads = 4\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "d_model = 512 # embedding size\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "# 4, 64, 64, 512\n",
        "multihead_attention = MultiHeadAttention(n_heads, d_k, d_v, d_model)\n",
        "# 64, 10, 64\n",
        "queries = random.random((batch_size, input_seq_len, d_k))\n",
        "keys = random.random((batch_size, input_seq_len, d_k))\n",
        "values = random.random((batch_size, input_seq_len, d_v))\n",
        "\n",
        "attention_weights = multihead_attention(queries, keys, values)\n",
        "print(attention_weights.shape)"
      ],
      "metadata": {
        "id": "eWXO5VnbHbQ3",
        "outputId": "fa5528af-2b6a-4b1a-ee50-be764ab26691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Embedding\n",
        "\n",
        "class PositionalEmbeddingLayer(Layer):\n",
        "  def __init__(self, max_seq_len, vocab_size, output_dim, **kwargs):\n",
        "    super(PositionalEmbeddingLayer, self).__init__(**kwargs)\n",
        "    self.word_embedding_layer = Embedding(vocab_size, output_dim)\n",
        "    self.position_embedding_layer = Embedding(max_seq_len, output_dim)\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.output_dim = output_dim\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "  def call(self, inputs):\n",
        "    position_indices = tf.range(0, tf.shape(inputs)[-1], delta=1)\n",
        "    embedded_words = self.word_embedding_layer(inputs)\n",
        "    embedded_positions = self.position_embedding_layer(position_indices)\n",
        "    return embedded_words + embedded_positions"
      ],
      "metadata": {
        "id": "OUmDv5MuIAO2"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_embedding_layer = PositionalEmbeddingLayer(10, 1000, 512)\n",
        "print(my_embedding_layer(tf.constant([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])).shape)"
      ],
      "metadata": {
        "id": "mUHfhO9UWwTn",
        "outputId": "76d458a6-f8b5-4777-acbe-ba4450144afe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(Layer):\n",
        "  def __init__(self, d_model, d_ff, **kwargs):\n",
        "    super(FeedForward, self).__init__(**kwargs)\n",
        "    self.d_fc1 = tf.keras.layers.Dense(d_ff)\n",
        "    self.d_fc2 = tf.keras.layers.Dense(d_model)\n",
        "    self.activation = tf.keras.layers.ReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.d_fc1(x)\n",
        "    x = self.d_fc2(self.activation(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "XAmJ_GS0W3TW"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(Normalization, self).__init__(**kwargs)\n",
        "    self.layer_Norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, sublayer_x):\n",
        "    return self.layer_Norm(x + sublayer_x)"
      ],
      "metadata": {
        "id": "8q4m9Z5nZ5C4"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EncoderLayer(Layer):\n",
        "  def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "    super(EncoderLayer, self).__init__(**kwargs)\n",
        "    self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.dropout1 = Dropout(rate)\n",
        "    self.norm1 = Normalization()\n",
        "    self.feed_forward = FeedForward(d_model, d_ff)\n",
        "    self.dropout2 = Dropout(rate)\n",
        "    self.norm2 = Normalization()\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # expected output length - batch_size, sequence_length, d_model\n",
        "    multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
        "    multihead_output = self.dropout1(multihead_output, training=training)\n",
        "    add_norm_output = self.norm1(x, multihead_output)\n",
        "    feed_forward_output = self.feed_forward(add_norm_output)\n",
        "    feed_forward_output = self.dropout2(feed_forward_output, training=training)\n",
        "    return self.norm2(add_norm_output, feed_forward_output)"
      ],
      "metadata": {
        "id": "Mv2dLw3uZ5LZ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Layer):\n",
        "  def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.pos_encoding = PositionalEmbeddingLayer(sequence_length, vocab_size, d_model)\n",
        "    self.dropout = Dropout(rate)\n",
        "    self.encoder_layers = [EncoderLayer(vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate) for _ in range(n)]\n",
        "\n",
        "  def call(self, inputs, padding_mask, training):\n",
        "    x = self.pos_encoding(inputs)\n",
        "    x = self.dropout(x, training=training)\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x, padding_mask, training)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Y6QcpGeXcvsD"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the above code\n",
        "n_heads = 4\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "d_model = 512 # embedding size\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "encoder = Encoder(1000, 10, n_heads, d_k, d_v, d_model, 2048, 6, 0.1)\n",
        "print(encoder(tf.random.uniform((batch_size, input_seq_len)), None, training=False).shape)\n",
        "# 64, 10, 512"
      ],
      "metadata": {
        "id": "SXQsqVp_dK9b",
        "outputId": "52f5c4c0-ad11-4d6e-e457-63845200d811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(Layer):\n",
        "  def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "    super(DecoderLayer, self).__init__(**kwargs)\n",
        "    self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.dropout1 = Dropout(rate)\n",
        "    self.norm1 = Normalization()\n",
        "    self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.dropout2 = Dropout(rate)\n",
        "    self.norm2 = Normalization()\n",
        "    self.feed_forward = FeedForward(d_model, d_ff)\n",
        "    self.dropout3 = Dropout(rate)\n",
        "    self.norm3 = Normalization()\n",
        "\n",
        "  def call(self, x, encoder_output, padding_mask, look_ahead_mask, training):\n",
        "    # expected output length - batch_size, sequence_length, d_model\n",
        "    multihead_output1 = self.multihead_attention1(x, x, x, look_ahead_mask)\n",
        "    multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
        "    add_norm_output1 = self.norm1(x, multihead_output1)\n",
        "    multihead_output2 = self.multihead_attention2(add_norm_output1, encoder_output, encoder_output, padding_mask)\n",
        "    multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
        "    norm_output2 = self.norm2(add_norm_output1, multihead_output2)\n",
        "    feed_forward_output = self.feed_forward(norm_output2)\n",
        "    feed_forward_output = self.dropout3(feed_forward_output, training=training)\n",
        "    return self.norm3(norm_output2, feed_forward_output)"
      ],
      "metadata": {
        "id": "NCTW2tHodn7l"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69rTAm93ffov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}