{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/sriksmachi/sriksml/blob/main/language-models/transformers_001.ipynb",
      "authorship_tag": "ABX9TyN4bi6Mz85c+dr3SEKQiAGd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriksmachi/sriksml/blob/main/language-models/transformers_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymvAUlN6lfrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import matmul, cast, float32,sqrt, math, transpose\n",
        "from tensorflow import convert_to_tensor, string\n",
        "from keras.backend import softmax\n",
        "from numpy import random\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Layer, Dropout\n",
        "from tensorflow.data import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "_fGjw2YZ_BLF"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(DotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "  def call(self, queries, keys, values, d_k, mask=None):\n",
        "    scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        "    if mask is not None:\n",
        "      scores += (mask * -1e9)\n",
        "    weights = softmax(scores)\n",
        "    attention_weights = matmul(weights, values)\n",
        "    return attention_weights"
      ],
      "metadata": {
        "id": "IfR6vWNuAQbP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the DotProductAttention\n",
        "\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "queries = random.random((batch_size, input_seq_len, d_k))\n",
        "keys = random.random((batch_size, input_seq_len, d_k))\n",
        "values = random.random((batch_size, input_seq_len, d_v))\n",
        "\n",
        "attention = DotProductAttention()\n",
        "attention_weights = attention(queries, keys, values, d_k)\n",
        "print(queries.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIh3KZA1B6b4",
        "outputId": "47b86555-a296-48c7-811a-62c0c51f86f8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "  def __init__(self, n_heads, d_k, d_v, d_model, **kwargs):\n",
        "    super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "    self.n_heads = n_heads\n",
        "    self.attention = DotProductAttention()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.W_q = tf.keras.layers.Dense(d_k)\n",
        "    self.W_k = tf.keras.layers.Dense(d_k)\n",
        "    self.W_v = tf.keras.layers.Dense(d_v)\n",
        "    self.W_o = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def reshape_tensor(self, x, heads, flag):\n",
        "    if flag:\n",
        "      x = tf.reshape(x, shape = (x.shape[0], x.shape[1], heads, -1))\n",
        "      x = transpose(x, perm=[0, 2, 1, 3])\n",
        "    else:\n",
        "      x = transpose(x, perm=(0, 2, 1, 3))\n",
        "      x = tf.reshape(x, shape=(x.shape[0], x.shape[1], -1))\n",
        "    return x\n",
        "\n",
        "  def call(self, queries, keys, values, mask=None):\n",
        "    # this converts the queries to shape (batch_size, heads, input_length, -1)\n",
        "    q_reshaped = self.reshape_tensor(self.W_q(queries), self.n_heads, True)\n",
        "    # this converts the keys to shape (batch_size, heads, input_length, -1)\n",
        "    k_reshaped = self.reshape_tensor(self.W_k(keys), self.n_heads, True)\n",
        "    # this converts the values to shape (batch_size, heads, input_length, -1)\n",
        "    v_reshaped = self.reshape_tensor(self.W_v(values), self.n_heads, True)\n",
        "    # compute attention weights in parallel, reshapes to (batch_size, heads, input_length, -1)\n",
        "    o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
        "    o = self.reshape_tensor(o_reshaped, self.n_heads, False)\n",
        "    o = self.W_o(o)\n",
        "    return o\n"
      ],
      "metadata": {
        "id": "yez0zfzr_h64"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the above code\n",
        "n_heads = 4\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "d_model = 512 # embedding size\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "# 4, 64, 64, 512\n",
        "multihead_attention = MultiHeadAttention(n_heads, d_k, d_v, d_model)\n",
        "# 64, 10, 64\n",
        "queries = random.random((batch_size, input_seq_len, d_k))\n",
        "keys = random.random((batch_size, input_seq_len, d_k))\n",
        "values = random.random((batch_size, input_seq_len, d_v))\n",
        "\n",
        "attention_weights = multihead_attention(queries, keys, values)\n",
        "print(attention_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWXO5VnbHbQ3",
        "outputId": "fa5528af-2b6a-4b1a-ee50-be764ab26691"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Embedding\n",
        "\n",
        "class PositionalEmbeddingLayer(Layer):\n",
        "  def __init__(self, max_seq_len, vocab_size, output_dim, **kwargs):\n",
        "    super(PositionalEmbeddingLayer, self).__init__(**kwargs)\n",
        "    self.word_embedding_layer = Embedding(vocab_size, output_dim)\n",
        "    self.position_embedding_layer = Embedding(max_seq_len, output_dim)\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.output_dim = output_dim\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "  def call(self, inputs):\n",
        "    position_indices = tf.range(0, tf.shape(inputs)[-1], delta=1)\n",
        "    embedded_words = self.word_embedding_layer(inputs)\n",
        "    embedded_positions = self.position_embedding_layer(position_indices)\n",
        "    return embedded_words + embedded_positions"
      ],
      "metadata": {
        "id": "OUmDv5MuIAO2"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_embedding_layer = PositionalEmbeddingLayer(10, 1000, 512)\n",
        "print(my_embedding_layer(tf.constant([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUHfhO9UWwTn",
        "outputId": "76d458a6-f8b5-4777-acbe-ba4450144afe"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(Layer):\n",
        "  def __init__(self, d_model, d_ff, **kwargs):\n",
        "    super(FeedForward, self).__init__(**kwargs)\n",
        "    self.d_fc1 = tf.keras.layers.Dense(d_ff)\n",
        "    self.d_fc2 = tf.keras.layers.Dense(d_model)\n",
        "    self.activation = tf.keras.layers.ReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.d_fc1(x)\n",
        "    x = self.d_fc2(self.activation(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "XAmJ_GS0W3TW"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(Normalization, self).__init__(**kwargs)\n",
        "    self.layer_Norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, sublayer_x):\n",
        "    return self.layer_Norm(x + sublayer_x)"
      ],
      "metadata": {
        "id": "8q4m9Z5nZ5C4"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EncoderLayer(Layer):\n",
        "  def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "    super(EncoderLayer, self).__init__(**kwargs)\n",
        "    self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.dropout1 = Dropout(rate)\n",
        "    self.norm1 = Normalization()\n",
        "    self.feed_forward = FeedForward(d_model, d_ff)\n",
        "    self.dropout2 = Dropout(rate)\n",
        "    self.norm2 = Normalization()\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # expected output length - batch_size, sequence_length, d_model\n",
        "    multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
        "    multihead_output = self.dropout1(multihead_output, training=training)\n",
        "    add_norm_output = self.norm1(x, multihead_output)\n",
        "    feed_forward_output = self.feed_forward(add_norm_output)\n",
        "    feed_forward_output = self.dropout2(feed_forward_output, training=training)\n",
        "    return self.norm2(add_norm_output, feed_forward_output)"
      ],
      "metadata": {
        "id": "Mv2dLw3uZ5LZ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Layer):\n",
        "  def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.pos_encoding = PositionalEmbeddingLayer(sequence_length, vocab_size, d_model)\n",
        "    self.dropout = Dropout(rate)\n",
        "    self.encoder_layers = [EncoderLayer(vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate) for _ in range(n)]\n",
        "\n",
        "  def call(self, inputs, padding_mask, training):\n",
        "    x = self.pos_encoding(inputs)\n",
        "    x = self.dropout(x, training=training)\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x, padding_mask, training)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Y6QcpGeXcvsD"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the above code\n",
        "n_heads = 4\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "d_model = 512 # embedding size\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "encoder = Encoder(1000, 10, n_heads, d_k, d_v, d_model, 2048, 6, 0.1)\n",
        "print(encoder(tf.random.uniform((batch_size, input_seq_len)), None, training=False).shape)\n",
        "# 64, 10, 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXQsqVp_dK9b",
        "outputId": "52f5c4c0-ad11-4d6e-e457-63845200d811"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(Layer):\n",
        "  def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "    super(DecoderLayer, self).__init__(**kwargs)\n",
        "    self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.dropout1 = Dropout(rate)\n",
        "    self.norm1 = Normalization()\n",
        "    self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.dropout2 = Dropout(rate)\n",
        "    self.norm2 = Normalization()\n",
        "    self.feed_forward = FeedForward(d_model, d_ff)\n",
        "    self.dropout3 = Dropout(rate)\n",
        "    self.norm3 = Normalization()\n",
        "\n",
        "  def call(self, x, encoder_output, padding_mask, look_ahead_mask, training):\n",
        "    # expected output length - batch_size, sequence_length, d_model\n",
        "    multihead_output1 = self.multihead_attention1(x, x, x, look_ahead_mask)\n",
        "    multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
        "    add_norm_output1 = self.norm1(x, multihead_output1)\n",
        "    multihead_output2 = self.multihead_attention2(add_norm_output1, encoder_output, encoder_output, padding_mask)\n",
        "    multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
        "    norm_output2 = self.norm2(add_norm_output1, multihead_output2)\n",
        "    feed_forward_output = self.feed_forward(norm_output2)\n",
        "    feed_forward_output = self.dropout3(feed_forward_output, training=training)\n",
        "    return self.norm3(norm_output2, feed_forward_output)"
      ],
      "metadata": {
        "id": "NCTW2tHodn7l"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(Layer):\n",
        "  def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.pos_encoding = PositionalEmbeddingLayer(sequence_length, vocab_size, d_model)\n",
        "    self.dropout = Dropout(rate)\n",
        "    self.decoder_layers = [DecoderLayer(vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate) for _ in range(n)]\n",
        "\n",
        "  def call(self, inputs, encoder_output, padding_mask, look_ahead_mask, training):\n",
        "    x = self.pos_encoding(inputs)\n",
        "    x = self.dropout(x, training=training)\n",
        "    for decoder_layer in self.decoder_layers:\n",
        "      x = decoder_layer(x, encoder_output, padding_mask, look_ahead_mask, training)\n",
        "    return x"
      ],
      "metadata": {
        "id": "69rTAm93ffov"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the above code\n",
        "n_heads = 4\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "d_model = 512 # embedding size\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "decoder = Decoder(1000, 10, n_heads, d_k, d_v, d_model, 2048, 6, 0.1)\n",
        "print(decoder(tf.random.uniform((batch_size, input_seq_len)),  tf.random.uniform((batch_size, input_seq_len, d_model)), None, True).shape)\n",
        "# 64, 10, 512"
      ],
      "metadata": {
        "id": "fw2cyBlNf6aK",
        "outputId": "fdb98375-e50c-4424-99dc-04c4696a52c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z4bEck1P6oj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
        "\n",
        "def normalize(line):\n",
        "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n",
        "    eng, fra = line.split(\"\\t\")\n",
        "    eng = \"[start] \" + eng + \" [end]\"\n",
        "    fra = \"[start] \" + fra + \" [end]\"\n",
        "    return eng, fra\n",
        "\n",
        "# normalize each line and separate into English and French\n",
        "with open(text_file) as fp:\n",
        "    text_pairs = [normalize(line) for line in fp]\n",
        "\n",
        "# print some samples\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"wb\") as fp:\n",
        "    pickle.dump(text_pairs, fp)"
      ],
      "metadata": {
        "id": "BXVcdS4gkNoH",
        "outputId": "d68b803c-7b9f-4672-bad1-7de3baaeb93c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('[start] i really miss you all . [end]', '[start] vous me manquez vraiment tous . [end]')\n",
            "('[start] tom and mary leave tomorrow . [end]', '[start] tom et mary partent demain . [end]')\n",
            "(\"[start] i don't think it'll rain tomorrow . [end]\", \"[start] je ne crois pas qu'il va pleuvoir demain . [end]\")\n",
            "('[start] a dead leaf fell to the ground . [end]', '[start] une feuille morte tomba au sol . [end]')\n",
            "('[start] are you ready ? [end]', '[start] êtes-vous prête  ?  [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy.random import shuffle\n"
      ],
      "metadata": {
        "id": "sd_0iwI7j-O6"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import convert_to_tensor, int64\n",
        "\n",
        "class PrepareDataset:\n",
        "  def __init__(self, **kwargs):\n",
        "    super(PrepareDataset, self).__init__(**kwargs)\n",
        "    self.n_sentences = 1000\n",
        "    self.train_split = 0.9\n",
        "\n",
        "  def create_tokenizer(self, dataset):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(dataset)\n",
        "    return tokenizer\n",
        "\n",
        "  def find_seq_length(self, dataset):\n",
        "    return max(len(s.split()) for s in dataset)\n",
        "\n",
        "  def find_vocab_size(self, tokenizer, dataset):\n",
        "    tokenizer.fit_on_texts(dataset)\n",
        "    return len(tokenizer.word_index) + 1\n",
        "\n",
        "  def __call__(self, filename, **kwargs):\n",
        "    clean_data = load(open(filename, 'rb'))\n",
        "    dataset = clean_data[:self.n_sentences]\n",
        "    shuffle(dataset)\n",
        "    train = dataset[:int(self.train_split * self.n_sentences)]\n",
        "    test = dataset[int(self.train_split * self.n_sentences):]\n",
        "\n",
        "    enc_seq_tokenizer = self.create_tokenizer(train[:, 0])\n",
        "    enc_seq_len = self.find_seq_length(train[:, 0])\n",
        "    enc_seq_vocab_size = self.find_vocab_size(enc_seq_tokenizer, train[:, 0])\n",
        "\n",
        "    trainX = enc_seq_tokenizer.texts_to_sequences(train[:, 0])\n",
        "    trainX = pad_sequences(trainX, maxlen=enc_seq_len, padding='post')\n",
        "    trainX = convert_to_tensor(trainX, dtype=int64)\n",
        "\n",
        "    dec_seq_tokenizer = self.create_tokenizer(train[:, 1])\n",
        "    dec_seq_len = self.find_seq_length(train[:, 1])\n",
        "    dec_seq_vocab_size = self.find_vocab_size(dec_seq_tokenizer, train[:, 1])\n",
        "\n",
        "    trainY = dec_seq_tokenizer.texts_to_sequences(train[:, 1])\n",
        "    trainY = pad_sequences(trainY, maxlen=dec_seq_len, padding='post')\n",
        "    trainY = convert_to_tensor(trainY, dtype=int64)\n",
        "\n",
        "    return trainX, trainY, train, enc_seq_len, dec_seq_len, enc_seq_vocab_size, dec_seq_vocab_size"
      ],
      "metadata": {
        "id": "tMubKI2Mqkw8"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PrepareDataset()\n",
        "trainX, trainY, train, enc_seq_len, dec_seq_len, enc_seq_vocab_size, dec_seq_vocab_size = dataset(\"/english-german-both.pkl\")"
      ],
      "metadata": {
        "id": "p7t9PFD0y00l"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Encoder sequence length', enc_seq_len)\n",
        "print('Decoder sequence length', dec_seq_len)"
      ],
      "metadata": {
        "id": "5cGeZ6YNqxUC",
        "outputId": "aaf729b5-aa8c-459d-dd98-1c76644d9f3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder sequence length 5\n",
            "Decoder sequence length 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0ouuapM40vu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}