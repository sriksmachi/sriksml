{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/sriksmachi/sriksml/blob/main/language-models/transformers_001.ipynb",
      "authorship_tag": "ABX9TyMxlQ5iYvIGeg1O487wuhq5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriksmachi/sriksml/blob/main/language-models/transformers_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymvAUlN6lfrO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import matmul, cast, float32,sqrt, math, transpose\n",
        "from tensorflow import convert_to_tensor, string\n",
        "from keras.backend import softmax\n",
        "from numpy import random\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Layer, Dropout\n",
        "from tensorflow.data import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "_fGjw2YZ_BLF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(DotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "  def call(self, queries, keys, values, d_k, mask=None):\n",
        "    scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        "    if mask is not None:\n",
        "      scores += (mask * -1e9)\n",
        "    weights = softmax(scores)\n",
        "    attention_weights = matmul(weights, values)\n",
        "    return attention_weights"
      ],
      "metadata": {
        "id": "IfR6vWNuAQbP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the DotProductAttention\n",
        "\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "queries = random.random((batch_size, input_seq_len, d_k))\n",
        "keys = random.random((batch_size, input_seq_len, d_k))\n",
        "values = random.random((batch_size, input_seq_len, d_v))\n",
        "\n",
        "attention = DotProductAttention()\n",
        "attention_weights = attention(queries, keys, values, d_k)\n",
        "print(queries.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIh3KZA1B6b4",
        "outputId": "29ae5909-4f95-428b-b41f-578195bafc13"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "  def __init__(self, n_heads, d_k, d_v, d_model, **kwargs):\n",
        "    super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "    self.n_heads = n_heads\n",
        "    self.attention = DotProductAttention()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.W_q = tf.keras.layers.Dense(d_k)\n",
        "    self.W_k = tf.keras.layers.Dense(d_k)\n",
        "    self.W_v = tf.keras.layers.Dense(d_v)\n",
        "    self.W_o = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def reshape_tensor(self, x, heads, flag):\n",
        "    if flag:\n",
        "      x = tf.reshape(x, shape = (x.shape[0], x.shape[1], heads, -1))\n",
        "      x = transpose(x, perm=[0, 2, 1, 3])\n",
        "    else:\n",
        "      x = transpose(x, perm=(0, 2, 1, 3))\n",
        "      x = tf.reshape(x, shape=(x.shape[0], x.shape[1], -1))\n",
        "    return x\n",
        "\n",
        "  def call(self, queries, keys, values, mask=None):\n",
        "    # this converts the queries to shape (batch_size, heads, input_length, -1)\n",
        "    q_reshaped = self.reshape_tensor(self.W_q(queries), self.n_heads, True)\n",
        "    # this converts the keys to shape (batch_size, heads, input_length, -1)\n",
        "    k_reshaped = self.reshape_tensor(self.W_k(keys), self.n_heads, True)\n",
        "    # this converts the values to shape (batch_size, heads, input_length, -1)\n",
        "    v_reshaped = self.reshape_tensor(self.W_v(values), self.n_heads, True)\n",
        "    # compute attention weights in parallel, reshapes to (batch_size, heads, input_length, -1)\n",
        "    o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
        "    o = self.reshape_tensor(o_reshaped, self.n_heads, False)\n",
        "    o = self.W_o(o)\n",
        "    return o\n"
      ],
      "metadata": {
        "id": "yez0zfzr_h64"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the above code\n",
        "n_heads = 8\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "d_model = 512 # embedding size\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "# 4, 64, 64, 512\n",
        "multihead_attention = MultiHeadAttention(n_heads, d_k, d_v, d_model)\n",
        "# 64, 10, 64\n",
        "queries = random.random((batch_size, input_seq_len, d_k))\n",
        "keys = random.random((batch_size, input_seq_len, d_k))\n",
        "values = random.random((batch_size, input_seq_len, d_v))\n",
        "\n",
        "print(queries.shape)\n",
        "attention_weights = multihead_attention(queries, keys, values)\n",
        "print(attention_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWXO5VnbHbQ3",
        "outputId": "aee60906-208d-4b2b-d460-e264ded42c80"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 64)\n",
            "(64, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional embedding matrix\n",
        "\n",
        "def pos_enc_matrix(L, d, n=1000):\n",
        "  assert d % 2 == 0\n",
        "  d2 = d // 2\n",
        "  P = np.zeros((L, d))\n",
        "  # creates a 1d array of evenly spaced values and converting it into column vector\n",
        "  k = np.arange(L).reshape(-1, 1)\n",
        "  i = np.arange(d2).reshape(1,-1)\n",
        "  denom = np.power(n, -i/d2) # n ** (-2*i/d)\n",
        "  args = k * denom\n",
        "  P[:, ::2] = np.sin(args)\n",
        "  P[:, 1::2] = np.cos(args)\n",
        "  return P\n",
        "\n",
        "pos_matrix = pos_enc_matrix(4, 4)\n",
        "assert pos_matrix.shape == (4,4)\n",
        "print(pos_matrix.shape)\n",
        "print(pos_matrix)\n"
      ],
      "metadata": {
        "id": "SWNd4soU1dTv",
        "outputId": "b77175ce-1d2f-44f8-ca5e-e6a28094be9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 4)\n",
            "[[ 0.          1.          0.          1.        ]\n",
            " [ 0.84147098  0.54030231  0.03161751  0.99950004]\n",
            " [ 0.90929743 -0.41614684  0.0632034   0.99800067]\n",
            " [ 0.14112001 -0.9899925   0.09472609  0.99550337]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IrefrQsA_iWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Embedding\n",
        "\n",
        "class PositionalEmbeddingLayer(Layer):\n",
        "  def __init__(self, max_seq_len, vocab_size, output_dim, **kwargs):\n",
        "    super(PositionalEmbeddingLayer, self).__init__(**kwargs)\n",
        "    self.word_embedding_layer = Embedding(vocab_size, output_dim, mask_zero=True)\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.output_dim = output_dim\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "      return self.word_embedding_layer.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedded_words = self.word_embedding_layer(inputs)\n",
        "    embedded_positions = pos_enc_matrix(self.max_seq_len, self.output_dim)\n",
        "    return embedded_words + tf.constant(embedded_positions, dtype = \"float32\")"
      ],
      "metadata": {
        "id": "OUmDv5MuIAO2"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_embedding_layer = PositionalEmbeddingLayer(9, 1000, 512)\n",
        "print(my_embedding_layer(tf.constant([[1, 2, 3, 4, 5, 6, 7, 8, 9]])).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUHfhO9UWwTn",
        "outputId": "e21af14c-1418-4709-cb01-8a704755aaf8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 9, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(Layer):\n",
        "  def __init__(self, d_model, d_ff, **kwargs):\n",
        "    super(FeedForward, self).__init__(**kwargs)\n",
        "    self.d_fc1 = tf.keras.layers.Dense(d_ff)\n",
        "    self.d_fc2 = tf.keras.layers.Dense(d_model)\n",
        "    self.activation = tf.keras.layers.ReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.d_fc1(x)\n",
        "    x = self.d_fc2(self.activation(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "XAmJ_GS0W3TW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(Normalization, self).__init__(**kwargs)\n",
        "    self.layer_Norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, sublayer_x):\n",
        "    return self.layer_Norm(x + sublayer_x)"
      ],
      "metadata": {
        "id": "8q4m9Z5nZ5C4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "class EncoderLayer(Layer):\n",
        "  def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
        "    super(EncoderLayer, self).__init__(**kwargs)\n",
        "    self.build(input_shape=[None, sequence_length, d_model])\n",
        "    self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.dropout1 = Dropout(rate)\n",
        "    self.norm1 = Normalization()\n",
        "    self.feed_forward = FeedForward(d_model, d_ff)\n",
        "    self.dropout2 = Dropout(rate)\n",
        "    self.norm2 = Normalization()\n",
        "    self.d_model = d_model\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "  def build_graph(self):\n",
        "    input_layer = Input(shape=(self.sequence_length, self.d_model))\n",
        "    print(input_layer.shape)\n",
        "    return Model(inputs = [input_layer], outputs = self.call(input_layer, None, True))\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # expected output length - batch_size, sequence_length, d_model\n",
        "    print(x, x, x, padding_mask)\n",
        "    multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
        "    multihead_output = self.dropout1(multihead_output, training=training)\n",
        "    add_norm_output = self.norm1(x, multihead_output)\n",
        "    feed_forward_output = self.feed_forward(add_norm_output)\n",
        "    feed_forward_output = self.dropout2(feed_forward_output, training=training)\n",
        "    return self.norm2(add_norm_output, feed_forward_output)"
      ],
      "metadata": {
        "id": "Mv2dLw3uZ5LZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Layer):\n",
        "  def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.pos_encoding = PositionalEmbeddingLayer(sequence_length, vocab_size, d_model)\n",
        "    self.dropout = Dropout(rate)\n",
        "    self.encoder_layers = [EncoderLayer(sequence_length, h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
        "\n",
        "  def call(self, inputs, padding_mask, training):\n",
        "    x = self.pos_encoding(inputs)\n",
        "    x = self.dropout(x, training=training)\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x, padding_mask, training)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Y6QcpGeXcvsD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the above code\n",
        "n_heads = 8\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "d_model = 512 # embedding size\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "# encoder = EncoderLayer(input_seq_len, n_heads, d_k, d_v, d_model, 2048, 0.1)\n",
        "# encoder.build_graph().summary()\n",
        "\n",
        "# encoder = Encoder(1000, 10, n_heads, d_k, d_v, d_model, 2048, 6, 0.1)\n",
        "# print(encoder(tf.random.uniform((batch_size, input_seq_len)), None, training=False).shape)\n",
        "# 64, 10, 512"
      ],
      "metadata": {
        "id": "SXQsqVp_dK9b"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(Layer):\n",
        "  def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
        "    super(DecoderLayer, self).__init__(**kwargs)\n",
        "    self.build(input_shape=[None, sequence_length, d_model])\n",
        "    self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.dropout1 = Dropout(rate)\n",
        "    self.norm1 = Normalization()\n",
        "    self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.dropout2 = Dropout(rate)\n",
        "    self.norm2 = Normalization()\n",
        "    self.feed_forward = FeedForward(d_model, d_ff)\n",
        "    self.dropout3 = Dropout(rate)\n",
        "    self.norm3 = Normalization()\n",
        "    self.d_model = d_model\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "  def build_graph(self):\n",
        "    input_layer = Input(shape=[self.sequence_length, self.d_model])\n",
        "    return Model(inputs = [input_layer], outputs = self.call(input_layer, None, True))\n",
        "\n",
        "  def call(self, x, encoder_output, padding_mask, look_ahead_mask, training):\n",
        "    # expected output length - batch_size, sequence_length, d_model\n",
        "    multihead_output1 = self.multihead_attention1(x, x, x, look_ahead_mask)\n",
        "    multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
        "    add_norm_output1 = self.norm1(x, multihead_output1)\n",
        "    multihead_output2 = self.multihead_attention2(add_norm_output1, encoder_output, encoder_output, padding_mask)\n",
        "    multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
        "    norm_output2 = self.norm2(add_norm_output1, multihead_output2)\n",
        "    feed_forward_output = self.feed_forward(norm_output2)\n",
        "    feed_forward_output = self.dropout3(feed_forward_output, training=training)\n",
        "    return self.norm3(norm_output2, feed_forward_output)"
      ],
      "metadata": {
        "id": "NCTW2tHodn7l"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(Layer):\n",
        "  def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.pos_encoding = PositionalEmbeddingLayer(sequence_length, vocab_size, d_model)\n",
        "    self.dropout = Dropout(rate)\n",
        "    self.decoder_layers = [DecoderLayer(sequence_length, h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
        "\n",
        "  def call(self, inputs, encoder_output, padding_mask, look_ahead_mask, training):\n",
        "    x = self.pos_encoding(inputs)\n",
        "    x = self.dropout(x, training=training)\n",
        "    for decoder_layer in self.decoder_layers:\n",
        "      x = decoder_layer(x, encoder_output, padding_mask, look_ahead_mask, training)\n",
        "    return x"
      ],
      "metadata": {
        "id": "69rTAm93ffov"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the above code\n",
        "n_heads = 4\n",
        "d_k = 64\n",
        "d_v = 64\n",
        "d_model = 512 # embedding size\n",
        "batch_size = 64\n",
        "input_seq_len = 10\n",
        "\n",
        "decoder = Decoder(1000, 10, n_heads, d_k, d_v, d_model, 2048, 6, 0.1)\n",
        "print(decoder(tf.random.uniform((batch_size, input_seq_len)),  tf.random.uniform((batch_size, input_seq_len, d_model)), None, True).shape)\n",
        "# 64, 10, 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw2cyBlNf6aK",
        "outputId": "67aa8c99-c865-4d20-e049-406009b8376e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import math, cast, float32\n",
        "from numpy import array\n",
        "\n",
        "def padding_mask(input):\n",
        "  # returns tensor with True for positions with value as 0\n",
        "  mask = math.equal(input, 0)\n",
        "  # converts false/true to numbers\n",
        "  mask = cast(mask, float32)\n",
        "  return mask\n",
        "\n",
        "padding_mask(array([1, 2, 3, 0, 0, 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4bEck1P6oj3",
        "outputId": "bbed6592-7171-4c1d-90ba-4c80ea965ff1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 0., 0., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import linalg, ones\n",
        "\n",
        "def look_ahead_mask(shape):\n",
        "  # mask out future entries by replace with 1.0\n",
        "  mask = 1- linalg.band_part(ones((shape, shape)), -1, 0)\n",
        "  return mask\n",
        "\n",
        "# the ones values mask out the values that the decoder must not see\n",
        "# the prediction of every word should depend on word before that only\n",
        "print(look_ahead_mask(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g2MeHn4zorM",
        "outputId": "2750c802-89ff-4483-ff48-60f611a6b8ed"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow import maximum, newaxis\n",
        "\n",
        "class TransformerModel(Model):\n",
        "  def __init__(self, enc_seq_vocab_size, dec_seq_vocab_size, enc_seq_len, dec_seq_len, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n",
        "    super(TransformerModel, self).__init__(**kwargs)\n",
        "    self.encoder = Encoder(enc_seq_vocab_size, enc_seq_len, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
        "    self.decoder = Decoder(dec_seq_vocab_size, dec_seq_len, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
        "    self.model_last_layer = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def padding_mask(input):\n",
        "    # returns tensor with True for positions with value as 0\n",
        "    mask = math.equal(input, 0)\n",
        "    # converts false/true to numbers\n",
        "    mask = cast(mask, float32)\n",
        "    return mask[:, newaxis, newaxis, :]\n",
        "\n",
        "  def look_ahead_mask(shape):\n",
        "    # mask out future entries by replace with 1.0\n",
        "    mask = 1- linalg.band_part(ones((shape, shape)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "  def call(self, encoder_input, decoder_input, training):\n",
        "    enc_padding_mask = self.padding_mask(encoder_input)\n",
        "\n",
        "    dec_padding_mask = self.padding_mask(decoder_input)\n",
        "    dec_look_ahead_mask = self.look_ahead_mask(decoder_input.shape[1])\n",
        "    dec_look_ahead_mask = maximum(dec_padding_mask, dec_look_ahead_mask)\n",
        "\n",
        "    encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n",
        "    decoder_output = self.decoder(decoder_input, encoder_output, dec_look_ahead_mask, dec_padding_mask, training)\n",
        "    model_output = self.model_last_layer(decoder_output)\n",
        "    return model_output\n"
      ],
      "metadata": {
        "id": "Soc3yLaW0ikK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
        "dec_vocab_size = 20 # Vocabulary size for the decoder\n",
        "\n",
        "enc_seq_length = 5  # Maximum length of the input sequence\n",
        "dec_seq_length = 5  # Maximum length of the target sequence\n",
        "\n",
        "h = 8  # Number of self-attention heads\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
        "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
        "n = 6  # Number of layers in the encoder stack\n",
        "\n",
        "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
        "\n",
        "# Create model\n",
        "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
        "# encoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
        "# encoder.build_graph().summary()\n",
        "\n",
        "# decoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
        "# decoder.build_graph().summary()"
      ],
      "metadata": {
        "id": "Y-7PqLIi5D5o"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data"
      ],
      "metadata": {
        "id": "ZSO1L1TzKjUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "\n",
        "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
        "\n",
        "def normalize(line):\n",
        "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n",
        "    eng, fra = line.split(\"\\t\")\n",
        "    fra = \"[start] \" + fra + \" [end]\"\n",
        "    return eng, fra\n",
        "\n",
        "# normalize each line and separate into English and French\n",
        "with open(text_file) as fp:\n",
        "    text_pairs = [normalize(line) for line in fp]\n",
        "\n",
        "# print some samples\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"wb\") as fp:\n",
        "    pickle.dump(text_pairs, fp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXVcdS4gkNoH",
        "outputId": "c418a1f0-56dd-43d7-fbcf-6cfefcf6743b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\n",
            "3423204/3423204 [==============================] - 0s 0us/step\n",
            "(\"you can't be too careful when you drive a car .\", '[start] tu ne pourras jamais être trop prudent lorsque tu conduis une voiture . [end]')\n",
            "(\"i'll never forget this experience .\", \"[start] je n'oublierai jamais cette expérience . [end]\")\n",
            "(\"tom is still at his grandfather's house .\", '[start] tom est toujours chez son grand-père . [end]')\n",
            "(\"let's try this .\", '[start] essayons ça . [end]')\n",
            "('are there any books under the desk ?', '[start] y a-t-il quelque livre sous le bureau  ?  [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find out how many distinct tokens exist\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_paris = pickle.load(fp)\n",
        "\n",
        "eng_tokens, fra_tokens = set(), set()\n",
        "eng_maxlength, fra_maxlength = 0, 0\n",
        "for eng, fra in text_paris:\n",
        "    eng_token, fra_token = eng.split(), fra.split()\n",
        "    eng_maxlength = max(len(eng_token), eng_maxlength)\n",
        "    fra_maxlength = max(len(fra_token), fra_maxlength)\n",
        "    eng_tokens.update(eng_token)\n",
        "    fra_tokens.update(fra_token)\n",
        "print(f\"Total English tokens = {len(eng_tokens)}\")\n",
        "print(f\"Total French tokens = {len(fra_tokens)}\")\n",
        "print(f\"Maximum English sequence length = {eng_maxlength}\")\n",
        "print(f\"Maximum French sequence length = {fra_maxlength}\")"
      ],
      "metadata": {
        "id": "rI9OcCOwMMuj",
        "outputId": "6ce3fb85-a8b6-4d77-b5ba-93d885489de8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total English tokens = 14969\n",
            "Total French tokens = 29219\n",
            "Maximum English sequence length = 51\n",
            "Maximum French sequence length = 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# histogram of sentence length in tokens\n",
        "en_lengths = [len(eng.split()) for eng, fra in text_pairs]\n",
        "fr_lengths = [len(fra.split()) for eng, fra in text_pairs]\n",
        "\n",
        "plt.hist(en_lengths, label=\"en\", color=\"red\", alpha=0.33)\n",
        "plt.hist(fr_lengths, label=\"fr\", color=\"blue\", alpha=0.33)\n",
        "plt.yscale(\"log\")     # sentence length fits Benford\"s law\n",
        "plt.ylim(plt.ylim())  # make y-axis consistent for both plots\n",
        "plt.plot([max(en_lengths), max(en_lengths)], plt.ylim(), color=\"red\")\n",
        "plt.plot([max(fr_lengths), max(fr_lengths)], plt.ylim(), color=\"blue\")\n",
        "plt.legend()\n",
        "plt.title(\"Examples count vs Token length\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "agg-7Oc2ONKB",
        "outputId": "60916e14-c397-4a50-d811-afd8a529346a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGzCAYAAAD0T7cVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4KUlEQVR4nO3de1xVdb7/8fcGZCMgoKJcRMFrhig4KKTlw5xoPFROahZzaUSao6eiXzpUM9qcNLtIWTpOc3aR48O0cc7I6KQzWdqFvEyO5b3sOJYWFqMCOikITqDs7+8PY49bQAHRvbb79Xw89uPBWuu71/rsL9R+u9b3u5bNGGMEAABgEX6eLgAAAOBchBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBPgMtuwYYNsNps2bNjg6VJwGR08eFA2m03PP/+8p0uRJCUkJGjSpEmeLuOiJk2apNDQUE+XAYshnMCjlixZIpvN1uTrgw8+8HSJuAwOHz6sxx9/XLt37/ZoHfWBojmvgwcPerRWb3bq1Ck9/vjjBHQ0W4CnCwAk6YknnlDPnj0brO/Tp48HqsHldvjwYc2ePVsJCQlKSUnxWB1dunTR7373O7d18+bN0z/+8Q/96le/atAWrXPq1CnNnj1bknTjjTd6thh4BcIJLCEzM1NDhgzxdBnwMSEhIbr77rvd1i1fvlzHjx9vsB7AlcNlHXiFWbNmyc/PT0VFRW7rp0yZosDAQH300UeSpNraWs2cOVOpqakKDw9XSEiIRowYofXr17u979zxAQ6HQ7169VJwcLC+973vqaSkRMYYPfnkk4qLi1P79u11++236+uvv3bbR0JCgm677Ta9/fbbSklJUVBQkBITE/Xaa6816zN9+OGH+o//+A+Fh4crODhYI0eO1ObNm93anDx5UtOmTVNCQoLsdru6du2qm2++WTt37rzo/g8dOqSf/vSnio2Nld1uV8+ePXXfffeptrbW1eaLL77QnXfeqU6dOik4OFjXXXed3njjDbf91F96O/+yRmNjaW688UYlJSVp7969GjVqlIKDg9WtWzfNnTvX7X1Dhw6VJOXk5LgumyxZsqTRz7Fy5UrZbDZt3LixwbaXX35ZNptNn3zyiSSptLRUOTk5iouLk91uV0xMjG6//fZLviRTXl6un/70p4qKilJQUJCSk5O1dOnSi77PGOP6Gz3372LZsmVKTU1V+/bt1alTJ/3gBz9QSUmJ23ub05ctdeLECU2bNk3du3eX3W5Xnz599Oyzz8rpdLranPvfxsKFC9W7d2/Z7XYNHTpU27Zta7DPFStWKDExUUFBQUpKStKqVas0adIkJSQkuPZXf9Zp9uzZrt/3448/7rafQ4cOaezYsQoNDVWXLl308MMPq66urtWfFV7OAB70yiuvGEnm3XffNUePHnV7HTt2zNWutrbWDB482MTHx5vKykpjjDHr1q0zksyTTz7panf06FETExNj8vLyzEsvvWTmzp1rrrnmGtOuXTuza9cuV7vi4mIjyaSkpJjExEQzf/5889///d8mMDDQXHfddebRRx81w4cPNy+88IJ58MEHjc1mMzk5OW61x8fHm379+pmIiAgzffp0M3/+fDNw4EDj5+dn3n77bVe79evXG0lm/fr1rnVFRUUmMDDQDBs2zMybN8/86le/MoMGDTKBgYHmww8/dLX70Y9+ZAIDA01eXp5ZtGiRefbZZ82YMWPMsmXLLtivhw4dMrGxsSY4ONhMmzbNFBQUmMcee8xce+215vjx48YYY0pLS01UVJTp0KGD+eUvf2nmz59vkpOTjZ+fn3nttdca/I6Ki4vdjtHY5xo5cqSJjY013bt3N1OnTjUvvvii+e53v2skmTfffNN13CeeeMJIMlOmTDG/+93vzO9+9zvz+eefN/pZTp06ZUJDQ83999/fYNuoUaPMgAEDXMvDhw834eHh5r//+7/NokWLzJw5c8yoUaPMxo0bL9hf57r11ltNfHy82/GvvfZa065dO/Ozn/3MvPDCC2bEiBFGklmwYIGrXf3f1HPPPWeMMebMmTNm4sSJxm63mzVr1rjaPfXUU8Zms5msrCzz4osvmtmzZ5vIyEiTkJDg+t00ty8vJD4+3mRnZ7uWq6urzaBBg0znzp3No48+agoKCszEiRONzWYzU6dObfA5Bg8ebPr06WOeffZZM3fuXBMZGWni4uJMbW2tq+2aNWuMzWYzgwYNMvPnzzePPfaY6dixo0lKSnL1YVVVlXnppZeMJDNu3DjX7/ujjz4yxhiTnZ1tgoKCzIABA8w999xjXnrpJXPHHXcYSebFF19szq8MVyHCCTyq/ouvsZfdbndru2fPHhMYGGj+8z//0xw/ftx069bNDBkyxJw+fdrV5syZM6ampsbtfcePHzdRUVHmnnvuca2r/x9wly5dzIkTJ1zrZ8yYYSSZ5ORkt/3+8Ic/NIGBgeabb75xrYuPjzeSzJ/+9CfXuoqKChMTE2MGDx7sWnf+l7jT6TR9+/Y1o0ePNk6n09Xu1KlTpmfPnubmm292rQsPDze5ubnN7s96EydONH5+fmbbtm0NttUfc9q0aUaS+etf/+radvLkSdOzZ0+TkJBg6urqjDEtDyeSzKuvvupaV1NTY6Kjo80dd9zhWrdt2zYjybzyyivN+jw//OEPTdeuXc2ZM2dc644cOWL8/PzME088YYw5+3s+Nxy01vnhZMGCBUaSWyCsra01w4YNM6Ghoa6wfG44OX36tMnKyjLt27c3b731lut9Bw8eNP7+/ubpp592O+aePXtMQECA2/rm9mVTzg8nTz75pAkJCTGfffaZW7vp06cbf39/89VXX7l9js6dO5uvv/7a1e7Pf/6zkWRef/1117qBAweauLg4c/LkSde6DRs2GElufXj06FEjycyaNatBndnZ2UaS6/dYb/DgwSY1NfWinxNXJy7rwBIcDofeeecdt9fatWvd2iQlJWn27NlatGiRRo8erWPHjmnp0qUKCPj30Cl/f38FBgZKkpxOp77++mudOXNGQ4YMafRSyJ133qnw8HDXcnp6uiTp7rvvdttvenq6amtrdejQIbf3x8bGaty4ca7lsLAwTZw4Ubt27VJpaWmjn3X37t3av3+/fvSjH+mf//ynjh07pmPHjqm6ulo33XSTNm3a5DrNHhERoQ8//FCHDx9uVj/Wf+7Vq1drzJgxjY7jsdlskqQ333xTaWlpuuGGG1zbQkNDNWXKFB08eFB79+5t9jHPFRoa6jZeIzAwUGlpafriiy9atT9JysrKUnl5udslpJUrV8rpdCorK0uS1L59ewUGBmrDhg06fvx4q491vjfffFPR0dH64Q9/6FrXrl07Pfjgg6qqqmpwuam2tlZ33nmn1qxZozfffFPf+973XNtee+01OZ1O3XXXXa7f+7FjxxQdHa2+ffs2uPzYln25YsUKjRgxQh07dnQ7dkZGhurq6rRp0ya39llZWerYsaNrecSIEZLkOvbhw4e1Z88eTZw40W0q8MiRIzVw4MAW13fvvfe6LY8YMeKS/mbg3RgQC0tIS0tr1oDYRx55RMuXL9fWrVs1Z84cJSYmNmizdOlSzZs3T/v27dPp06dd6xubDdSjRw+35fqg0r1790bXn/+l16dPH9eXfb1+/fpJOnutPTo6usEx9+/fL0nKzs5u/ENKqqioUMeOHTV37lxlZ2ere/fuSk1N1S233KKJEyeqV69eTb736NGjqqysVFJSUpNtJOnLL790hbFzXXvtta7tF9tHY+Li4hr0SceOHfXxxx+3eF/16sfmFBYW6qabbpIkFRYWKiUlxdXfdrtdzz77rB566CFFRUXpuuuu02233aaJEyc2+ntori+//FJ9+/aVn5/7v+XO7adz5efnq6qqSmvXrm0wM2X//v0yxqhv376NHqtdu3Zuy23Zl/v379fHH3/c5Kyj8vJyt+Xz/9uoDyr1/w3Uf+7GZtT16dOnWeOi6gUFBTWoq2PHjm0aMuFdCCfwKl988YXry33Pnj0Nti9btkyTJk3S2LFj9cgjj6hr167y9/dXfn6+Pv/88wbt/f39Gz1OU+uNMZdQ/Vn1Z0Wee+65JqfR1v9L9K677tKIESO0atUqvf3223ruuef07LPP6rXXXlNmZuYl19Ic53851mtqsOLl6Du73a6xY8dq1apVevHFF1VWVqbNmzdrzpw5bu2mTZumMWPGaPXq1Xrrrbf02GOPKT8/X++9954GDx7c6uO3xOjRo7Vu3TrNnTtXN954o4KCglzbnE6nbDab1q5d22g/nX8zsrbsS6fTqZtvvlk///nPG91eH/Iux7EvpqljwXcRTuA1nE6nJk2apLCwME2bNk1z5szRhAkTNH78eFeblStXqlevXnrttdfcvlRnzZp1WWo6cOCAjDFux/rss88kyTVb4Xy9e/eWdPYSUEZGxkWPERMTo/vvv1/333+/ysvL9Z3vfEdPP/10k+GkS5cuCgsLc81gaUp8fLw+/fTTBuv37dvn2i79+1/MJ06ccGt3/hmDlmgq8FxIVlaWli5dqqKiIv3973+XMcZ1SedcvXv31kMPPaSHHnpI+/fvV0pKiubNm6dly5a1qtb4+Hh9/PHHcjqdbmdPzu+netddd53uvfde3Xbbbbrzzju1atUq1yXC3r17yxijnj17NggDl1vv3r1VVVXVrL+55qj/3AcOHGiw7fx1rfl9w7cx5gReY/78+frb3/6mhQsX6sknn9Tw4cN133336dixY6429f8CO/dfdx9++KG2bNlyWWo6fPiwVq1a5VqurKzUq6++qpSUlCYvJaSmpqp37956/vnnVVVV1WD70aNHJZ09M1FRUeG2rWvXroqNjVVNTU2TNfn5+Wns2LF6/fXXtX379gbb6/vmlltu0datW936prq6WgsXLlRCQoLrkll9mDp3TEJdXZ0WLlzYZA0XExISIqlh4LmQjIwMderUSYWFhSosLFRaWprbpbpTp07pm2++cXtP79691aFDhwv218XccsstKi0tVWFhoWvdmTNn9Jvf/EahoaEaOXJko7UuX75c69at009+8hPX2bLx48fL399fs2fPbnAGwhijf/7zn62u82LuuusubdmyRW+99VaDbSdOnNCZM2datL/Y2FglJSXp1Vdfdfs73rhxY4OzmsHBwa7jAM3BmRNYwtq1a13/Ej3X8OHD1atXL/3973/XY489pkmTJmnMmDGSzt5/IyUlRffff7/++Mc/SpJuu+02vfbaaxo3bpxuvfVWFRcXq6CgQImJiY0GgUvVr18//fSnP9W2bdsUFRWlxYsXq6ysTK+88kqT7/Hz89OiRYuUmZmpAQMGKCcnR926ddOhQ4e0fv16hYWF6fXXX9fJkycVFxenCRMmKDk5WaGhoXr33Xe1bds2zZs374J1zZkzR2+//bZGjhypKVOm6Nprr9WRI0e0YsUKvf/++4qIiND06dP1hz/8QZmZmXrwwQfVqVMnLV26VMXFxfrTn/7kOkswYMAAXXfddZoxY4a+/vprderUScuXL2/xl9m5evfurYiICBUUFKhDhw4KCQlRenp6o+OC6rVr107jx4/X8uXLVV1d3eAZNp999pluuukm3XXXXUpMTFRAQIBWrVqlsrIy/eAHP2h1rVOmTNHLL7+sSZMmaceOHUpISNDKlSu1efNmLViwQB06dGj0fWPHjtUrr7yiiRMnKiwsTC+//LJ69+6tp556SjNmzNDBgwc1duxYdejQQcXFxVq1apWmTJmihx9+uNW1Xsgjjzyiv/zlL7rttts0adIkpaamqrq6Wnv27NHKlSt18OBBRUZGtmifc+bM0e23367rr79eOTk5On78uP7nf/5HSUlJbv+9tW/fXomJiSosLFS/fv3UqVMnJSUltWpME3yEZyYJAWddaCqxvp1qeubMGTN06FATFxfnNu3XGGN+/etfG0mmsLDQGHN2muycOXNMfHy8sdvtZvDgwWbNmjUmOzvbbWrj+fekqFc/PXbFihWN1nnu1Nz4+Hhz6623mrfeessMGjTI2O12079//wbvbWzKrTHG7Nq1y4wfP9507tzZ2O12Ex8fb+666y5TVFRkjDk7bfSRRx4xycnJpkOHDiYkJMQkJyc3+94PX375pZk4caLp0qWLsdvtplevXiY3N9dtqvXnn39uJkyYYCIiIkxQUJBJS0tzuyfHue0yMjKM3W43UVFR5tFHHzXvvPNOo1OJz73vSL3z+9+Ys1NTExMTTUBAQLOnFdcf02azmZKSErdtx44dM7m5uaZ///4mJCTEhIeHm/T0dPPHP/7xovs91/lTiY0xpqyszOTk5JjIyEgTGBhoBg4c2KDepv6mXnzxRSPJPPzww651f/rTn8wNN9xgQkJCTEhIiOnfv7/Jzc01n376qatNS/qyMedPJTbm7FTxGTNmmD59+pjAwEATGRlphg8fbp5//nnX/Uua+hzGmEanAy9fvtz079/f2O12k5SUZP7yl7+YO+64w/Tv39+t3d/+9jeTmppqAgMD3faTnZ1tQkJCGhxr1qxZhq8o32Uz5jKMbgJ8QEJCgpKSkrRmzRpPlwJYSkpKirp06aJ33nnH06XASzHmBADQKqdPn25weW/Dhg366KOPeMAfLgljTgAArXLo0CFlZGTo7rvvVmxsrPbt26eCggJFR0c3uKka0BKEEwBAq3Ts2FGpqalatGiRjh49qpCQEN1666165pln1LlzZ0+XBy/GmBMAAGApjDkBAACWQjgBAACW4nVjTpxOpw4fPqwOHTpwS2QAALyEMUYnT55UbGxsgwdpns/rwsnhw4cbPDEWAAB4h5KSEsXFxV2wjdeFk/pbRZeUlCgsLMzD1QAAvFJ1tRQbe/bnw4elb5/35OsuZ7dUVlaqe/fuTT7y4VxeF07qL+WEhYURTgAArfPtQ0IlSWFhhJNvXYluac6QDAbEAgAASyGcAAAASyGcAAAAS/G6MScALh9jjM6cOaO6ujpPl+Ix/v7+CggI4FYFgAcRTgBIkmpra3XkyBGdOnXK06V4XHBwsGJiYhQYGOjpUgCfRDgBIKfTqeLiYvn7+ys2NlaBgYE+eebAGKPa2lodPXpUxcXF6tu370VvFgWg7XkknCQkJCgsLEx+fn7q2LGj1q9f74kyAHyrtrZWTqdT3bt3V3BwsKfL8aj27durXbt2+vLLL1VbW6ugoCBPlwT4HI+dOfnb3/6m0NBQTx0eQCM4S3AW/QB4Fv8FAgAAS2lxONm0aZPGjBmj2NhY2Ww2rV69ukEbh8OhhIQEBQUFKT09XVu3bnXbbrPZNHLkSA0dOlS///3vW108AAC4+rT4sk51dbWSk5N1zz33aPz48Q22FxYWKi8vTwUFBUpPT9eCBQs0evRoffrpp+ratask6f3331e3bt105MgRZWRkaODAgRo0aNClfxoAbW/lyit7vAkTruzxAFhOi8+cZGZm6qmnntK4ceMa3T5//nxNnjxZOTk5SkxMVEFBgYKDg7V48WJXm27dukmSYmJidMstt2jnzp1NHq+mpkaVlZVuLwAAcPVq0zEntbW12rFjhzIyMv59AD8/ZWRkaMuWLZLOnnk5efKkJKmqqkrvvfeeBgwY0OQ+8/PzFR4e7np17969LUsGAAAW06azdY4dO6a6ujpFRUW5rY+KitK+ffskSWVlZa6zLnV1dZo8ebKGDh3a5D5nzJihvLw813L9I5et6Eqf/b5UnD3H1cLpdOrZZ5/VwoULVVpaqn79+umxxx7ThAkTtGHDBo0aNUrvvvuufvGLX2jv3r1KSUnRK6+8omuuucbTpQNoxBWfStyrVy999NFHzW5vt9tlt9svY0UAvF1+fr6WLVumgoIC9e3bV5s2bdLdd9+tLl26uNr88pe/1Lx589SlSxfde++9uueee7R582YPVg2gKW0aTiIjI+Xv76+ysjK39WVlZYqOjr6kfTscDjkcDp9+5geAhmpqajRnzhy9++67GjZsmKSz/wh6//339fLLL2vKlCmSpKefflojR46UJE2fPl233nqrvvnmG26yBlhQm445CQwMVGpqqoqKilzrnE6nioqKXP/TaK3c3Fzt3btX27Ztu9QyAVxFDhw4oFOnTunmm29WaGio6/Xqq6/q888/d7U7d0ZgTEyMJKm8vPyK1wvg4lp85qSqqkoHDhxwLRcXF2v37t3q1KmTevTooby8PGVnZ2vIkCFKS0vTggULVF1drZycnDYt3Od8O6C4bR269F0wcAUeVlVVJUl64403XDMB69ntdldAadeunWt9/XODnE7nFaoSQEu0OJxs375do0aNci3XD1bNzs7WkiVLlJWVpaNHj2rmzJkqLS1VSkqK1q1b12CQbEtxWQdAYxITE2W32/XVV1+5Ltuc69yzJwC8Q4vDyY033ihjzAXbPPDAA3rggQdaXVRjcnNzlZubq8rKSoWHh7fpvgF4rw4dOujhhx/Wz372MzmdTt1www2qqKjQ5s2bFRYWpvj4eE+XCKCFPPbgP8u6lPnAW7pdvA3gbbzg0t2TTz6pLl26KD8/X1988YUiIiL0ne98R48++iiXbgAvRDgB4PVsNpumTp2qqVOnNrr9/LO9KSkpFz0DDMBzvOapxA6HQ4mJiRe8YRsAAPB+XnPmhDEnbW+lF12G8oIrCwCANuI1Z04AAIBvIJwAAABLIZwAAABL8ZpwwoBYAAB8g9eEE56tAwCAb/CacAIAAHwD4QQAAFiK19znBIBnXMoTHVqjpfe0Mcbov/7rv7Ry5UodP35cu3btUkpKymWpDcCV4TXhhKcSA2jMunXrtGTJEm3YsEG9evVSZGSkp0sCcIm85rIOA2IBNObzzz9XTEyMhg8frujoaAUEuP+bq7a21kOVAWgtrzlzAgDnmzRpkpYuXSrp7MP/4uPjlZCQoKSkJAUEBGjZsmUaOHCg1q9f7+FKAbSE15w5AYDz/frXv9YTTzyhuLg4HTlyxHVmdenSpQoMDNTmzZtVUFDg4SoBtBRnTgB4rfDwcHXo0EH+/v6Kjo52re/bt6/mzp3rwcoAXArOnAC46qSmpnq6BACXwGvCCbevB9BcISEhni4BwCXwmnDCbB0AAHyD14QTAADgGxgQC+CCWnrHVgC4VIQTAF5t2rRpmjZtmmt5w4YNHqsFQNvgsg4AALAUwgkAALAUwgkAALAUwgkAALAUrwkn3IQNuPyMMZ4uwRLoB8CzvCaccBM24PJp166dJOnUqVMersQa6vuhvl8AXFlMJQYgf39/RUREqLy8XJIUHBwsm83m4aquPGOMTp06pfLyckVERMjf39/TJQE+iXACQJJcT/WtDyi+LCIiwu0pxwCuLMIJAEmSzWZTTEyMunbtqtOnT3u6HI9p164dZ0wADyOcAHDj7+/PlzMAj/KaAbEAAMA3EE4AAIClEE4AAIClEE4AAIClEE4AAIClEE4AAICleE044dk6AAD4Bq8JJzxbBwAA3+A14QQAAPgGwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUj4WTU6dOKT4+Xg8//LCnSgAAABbksXDy9NNP67rrrvPU4QEAgEV5JJzs379f+/btU2ZmpicODwAALKzF4WTTpk0aM2aMYmNjZbPZtHr16gZtHA6HEhISFBQUpPT0dG3dutVt+8MPP6z8/PxWFw0AAK5eLQ4n1dXVSk5OlsPhaHR7YWGh8vLyNGvWLO3cuVPJyckaPXq0ysvLJUl//vOf1a9fP/Xr169Zx6upqVFlZaXbCwAAXL0CWvqGzMzMC16OmT9/viZPnqycnBxJUkFBgd544w0tXrxY06dP1wcffKDly5drxYoVqqqq0unTpxUWFqaZM2c2ur/8/HzNnj27pWUCAAAv1aZjTmpra7Vjxw5lZGT8+wB+fsrIyNCWLVsknQ0bJSUlOnjwoJ5//nlNnjy5yWAiSTNmzFBFRYXrVVJS0pYlAwAAi2nxmZMLOXbsmOrq6hQVFeW2PioqSvv27WvVPu12u+x2e1uUh8vh29B5+R1qWfMJEy5PGQCAy65Nw0lLTZo0qdltHQ6HHA6H6urqLl9BAADA49r0sk5kZKT8/f1VVlbmtr6srEzR0dGXtO/c3Fzt3btX27Ztu6T9AAAAa2vTcBIYGKjU1FQVFRW51jmdThUVFWnYsGFteSgAAHCVavFlnaqqKh04cMC1XFxcrN27d6tTp07q0aOH8vLylJ2drSFDhigtLU0LFixQdXW1a/YOAADAhbQ4nGzfvl2jRo1yLefl5UmSsrOztWTJEmVlZeno0aOaOXOmSktLlZKSonXr1jUYJNtSjDkBAMA32IwxxtNFtERlZaXCw8NVUVGhsLCwtj/AypWtf+uWbm1YCM41YRizdQC0oepqKTT07M9VVVJIiGfrsYjL2S0t+f722IP/AAAAGkM4AQAAluI14cThcCgxMVFDhw71dCkAAOAy8ppwwn1OAADwDV4TTgAAgG8gnAAAAEvxmnDCmBMAAHyD14QTxpwAAOAbPPpUYqC5vO0Gd9wDDgBaz2vOnAAAAN9AOAEAAJbiNeGEAbEAAPgGrwknDIgFAMA3eE04AQAAvoFwAgAALIVwAgAALIVwAgAALIVwAgAALMVrwglTiQEA8A1eE06YSgwAgG/wmnACAAB8A+EEAABYCuEEAABYCuEEAABYCuEEAABYCuEEAABYiteEE+5zAgCAb/CacMJ9TgAA8A1eE04AAIBvIJwAAABLIZwAAABLIZwAAABLIZwAAABLIZwAAABLIZwAAABLIZwAAABLIZwAAABLIZwAAABL8ZpwwrN1AADwDV4TTni2DgAAvsFrwgkAAPANhBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGApVzycnDhxQkOGDFFKSoqSkpL029/+9kqXAAAALCzgSh+wQ4cO2rRpk4KDg1VdXa2kpCSNHz9enTt3vtKlAAAAC7riZ078/f0VHBwsSaqpqZExRsaYK10GAACwqBaHk02bNmnMmDGKjY2VzWbT6tWrG7RxOBxKSEhQUFCQ0tPTtXXrVrftJ06cUHJysuLi4vTII48oMjKy1R8AAABcXVocTqqrq5WcnCyHw9Ho9sLCQuXl5WnWrFnauXOnkpOTNXr0aJWXl7vaRERE6KOPPlJxcbH+93//V2VlZa3/BAAA4KrS4nCSmZmpp556SuPGjWt0+/z58zV58mTl5OQoMTFRBQUFCg4O1uLFixu0jYqKUnJysv761782ebyamhpVVla6vQAAwNWrTcec1NbWaseOHcrIyPj3Afz8lJGRoS1btkiSysrKdPLkSUlSRUWFNm3apGuuuabJfebn5ys8PNz16t69e1uWDAAALKZNw8mxY8dUV1enqKgot/VRUVEqLS2VJH355ZcaMWKEkpOTNWLECP2///f/NHDgwCb3OWPGDFVUVLheJSUlbVkyAACwmCs+lTgtLU27d+9udnu73S673X75CsLV6dszdZ5zqPHVEyZc2TIAwAu16ZmTyMhI+fv7NxjgWlZWpujo6Evat8PhUGJiooYOHXpJ+wEAANbWpuEkMDBQqampKioqcq1zOp0qKirSsGHDLmnfubm52rt3r7Zt23apZQIAAAtr8WWdqqoqHThwwLVcXFys3bt3q1OnTurRo4fy8vKUnZ2tIUOGKC0tTQsWLFB1dbVycnLatHAAAHB1anE42b59u0aNGuVazsvLkyRlZ2dryZIlysrK0tGjRzVz5kyVlpYqJSVF69atazBItqUcDoccDofq6uouaT8AAMDabMbL7h1fWVmp8PBwVVRUKCwsrO0PsHJl69+6pVsbFgJvNmEYA2IBS6uulkJDz/5cVSWFhHi2Hou4nN3Sku/vKz5bB/AF3hRUyUsArOaKP/gPAADgQrwmnDCVGAAA3+A14YSpxAAA+AavCScAAMA3EE4AAIClEE4AAICleE04YUAsAAC+wWvCCQNiAQDwDV4TTgAAgG8gnAAAAEshnAAAAEvxmnDCgFgAAHyD14QTBsQCAOAbvCacAAAA30A4AQAAlkI4AQAAlkI4AQAAlkI4AQAAluI14YSpxAAA+AavCSdMJQYAwDd4TTgBAAC+gXACAAAshXACAAAshXACAAAshXACAAAshXACAAAsxWvCCfc5AQDAN3hNOOE+JwAA+AavCScAAMA3EE4AAIClEE4AAIClEE4AAIClEE4AAIClEE4AAIClEE4AAIClEE4AAIClEE4AAIClEE4AAICleE044dk6AAD4Bq8JJzxbBwAA3+A14QQAAPgGwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALCUKx5OSkpKdOONNyoxMVGDBg3SihUrrnQJAADAwgKu+AEDArRgwQKlpKSotLRUqampuuWWWxQSEnKlSwGuvC1bPF1BIw5JEyZ4uggAcLni4SQmJkYxMTGSpOjoaEVGRurrr78mnAAAAEmtuKyzadMmjRkzRrGxsbLZbFq9enWDNg6HQwkJCQoKClJ6erq2bt3a6L527Nihuro6de/evcWFAwCAq1OLw0l1dbWSk5PlcDga3V5YWKi8vDzNmjVLO3fuVHJyskaPHq3y8nK3dl9//bUmTpyohQsXXvB4NTU1qqysdHsBAICrV4vDSWZmpp566imNGzeu0e3z58/X5MmTlZOTo8TERBUUFCg4OFiLFy92tampqdHYsWM1ffp0DR8+/ILHy8/PV3h4uOvFWRYAAK5ubTpbp7a2Vjt27FBGRsa/D+Dnp4yMDG35diCgMUaTJk3Sd7/7Xf3kJz+56D5nzJihiooK16ukpKQtSwYAABbTpuHk2LFjqqurU1RUlNv6qKgolZaWSpI2b96swsJCrV69WikpKUpJSdGePXua3KfdbldYWJjbCwAAXL2u+GydG264QU6ns8XvczgccjgcqquruwxVAQAAq2jTMyeRkZHy9/dXWVmZ2/qysjJFR0df0r5zc3O1d+9ebdu27ZL2AwAArK1Nz5wEBgYqNTVVRUVFGjt2rCTJ6XSqqKhIDzzwQFseCkAbWbmlm6dLaBHuFwdc/VocTqqqqnTgwAHXcnFxsXbv3q1OnTqpR48eysvLU3Z2toYMGaK0tDQtWLBA1dXVysnJadPCAQDA1anF4WT79u0aNWqUazkvL0+SlJ2drSVLligrK0tHjx7VzJkzVVpaqpSUFK1bt67BINmWYswJAAC+wWaMMZ4uoiUqKysVHh6uioqKyzNzZ+XK1r/Vy06PAy7Dhnm6gmbjsg7aRHW1FBp69ueqKolHqEi6vN3Sku/vK/5UYgAAgAshnAAAAEvxmnDicDiUmJiooUOHeroUAABwGXlNOOE+JwAA+AavCScAAMA3EE4AAICleE04YcwJAAC+wWvCCWNOAADwDV4TTgAAgG8gnAAAAEshnAAAAEshnAAAAEvxmnDCbB0AAHyD14QTZusAAOAbvCacAAAA30A4AQAAlkI4AQAAlkI4AQAAluI14YTZOgAA+AavCSfM1gEAwDd4TTgBAAC+gXACAAAshXACAAAshXACAAAsJcDTBQBAS6xc6ekKmm/CBE9XAHgnzpwAAABLIZwAAABL8Zpwwk3YAADwDV4TTrgJGwAAvsFrwgkAAPANhBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGAphBMAAGApPFsHgLRli6craNywYZ6uAIAHcOYEAABYCuEEAABYiteEE56tAwCAb/CacMKzdQAA8A1eE04AAIBvIJwAAABLIZwAAABLIZwAAABLIZwAAABLIZwAAABLIZwAAABLIZwAAABL4cF/AKyLBxICPokzJwAAwFIIJwAAwFIIJwAAwFI8Ek7GjRunjh07asKECZ44PAAAsDCPhJOpU6fq1Vdf9cShAQCAxXkknNx4443q0KGDJw4NAAAsrsXhZNOmTRozZoxiY2Nls9m0evXqBm0cDocSEhIUFBSk9PR0bd26tS1qBQAAPqDF4aS6ulrJyclyOByNbi8sLFReXp5mzZqlnTt3Kjk5WaNHj1Z5efklFwsAAK5+Lb4JW2ZmpjIzM5vcPn/+fE2ePFk5OTmSpIKCAr3xxhtavHixpk+f3uICa2pqVFNT41qurKxs8T4AAID3aNMxJ7W1tdqxY4cyMjL+fQA/P2VkZGhLK+/0mJ+fr/DwcNere/fubVUuAACwoDYNJ8eOHVNdXZ2ioqLc1kdFRam0tNS1nJGRoTvvvFNvvvmm4uLiLhhcZsyYoYqKCterpKSkLUsGAAAW45Fn67z77rvNbmu322W32y9jNQAAwEra9MxJZGSk/P39VVZW5ra+rKxM0dHRl7Rvh8OhxMREDR069JL2AwAArK1Nw0lgYKBSU1NVVFTkWud0OlVUVKRhl/gUz9zcXO3du1fbtm271DIBAICFtfiyTlVVlQ4cOOBaLi4u1u7du9WpUyf16NFDeXl5ys7O1pAhQ5SWlqYFCxaourraNXsHAADgQlocTrZv365Ro0a5lvPy8iRJ2dnZWrJkibKysnT06FHNnDlTpaWlSklJ0bp16xoMkm0ph8Mhh8Ohurq6S9oPAACwNpsxxni6iJaorKxUeHi4KioqFBYW1vYHWLmy9W/d0q0NCwFgWc28TM2zTS2suloKDT37c1WVFBLi2Xos4nJ2S0u+vz3ybB0AAICmEE4AAICleE04YSoxAAC+wWvCCVOJAQDwDV4TTgAAgG8gnAAAAEshnAAAAEvxmnDCgFgAAHyD14QTBsQCAOAbvCacAAAA30A4AQAAlkI4AQAAluI14YQBsQAA+AavCScMiAUAwDd4TTgBAAC+gXACAAAshXACAAAshXACAAAshXACAAAsxWvCCVOJAQDwDV4TTphKDACAb/CacAIAAHwD4QQAAFgK4QQAAFgK4QQAAFgK4QQAAFgK4QQAAFhKgKcLaC6HwyGHw6G6ujpPlwIAQIusXOnpCprnm288XcFZXnPmhPucAADgG7wmnAAAAN9AOAEAAJZCOAEAAJZCOAEAAJZCOAEAAJZCOAEAAJZCOAEAAJZCOAEAAJZCOAEAAJZCOAEAAJbCs3UAoKW2bGlWs5XNa9Z2hg27wgdspmb215Xkf/pfGvftz6tWSXVBHi0H5/GaMyc8WwcAAN/gNeEEAAD4BsIJAACwFMIJAACwFMIJAACwFMIJAACwFMIJAACwFMIJAACwFMIJAACwFMIJAACwFMIJAACwFMIJAACwFMIJAACwFMIJAACwFI+EkzVr1uiaa65R3759tWjRIk+UAAAALCrgSh/wzJkzysvL0/r16xUeHq7U1FSNGzdOnTt3vtKlAAAAC7riZ062bt2qAQMGqFu3bgoNDVVmZqbefvvtK10GAACwqBaHk02bNmnMmDGKjY2VzWbT6tWrG7RxOBxKSEhQUFCQ0tPTtXXrVte2w4cPq1u3bq7lbt266dChQ62rHgAAXHVaHE6qq6uVnJwsh8PR6PbCwkLl5eVp1qxZ2rlzp5KTkzV69GiVl5e3qsCamhpVVla6vQAAwNWrxWNOMjMzlZmZ2eT2+fPna/LkycrJyZEkFRQU6I033tDixYs1ffp0xcbGup0pOXTokNLS0prcX35+vmbPnt3SMgHA92zZ4ukKgDbRpmNOamtrtWPHDmVkZPz7AH5+ysjI0JZv/6NJS0vTJ598okOHDqmqqkpr167V6NGjm9znjBkzVFFR4XqVlJS0ZckAAMBi2nS2zrFjx1RXV6eoqCi39VFRUdq3b9/ZAwYEaN68eRo1apScTqd+/vOfX3Cmjt1ul91ub8syAQCAhV3xqcSS9P3vf1/f//73PXFoAABgcW16WScyMlL+/v4qKytzW19WVqbo6OhL2rfD4VBiYqKGDh16SfsBAADW1qbhJDAwUKmpqSoqKnKtczqdKioq0rBhwy5p37m5udq7d6+2bdt2qWUCAAALa/FlnaqqKh04cMC1XFxcrN27d6tTp07q0aOH8vLylJ2drSFDhigtLU0LFixQdXW1a/YOAADAhbQ4nGzfvl2jRo1yLefl5UmSsrOztWTJEmVlZeno0aOaOXOmSktLlZKSonXr1jUYJNtSDodDDodDdXV1l7QfAABgbTZjjPF0ES1RWVmp8PBwVVRUKCwsrO0PsHJl69+6pdvFGwEAPM7/9L807jc3SZJW/a5KdUEhHq7IGr75RvrJT87+XFUlhbRht7Tk+9sjTyUGAABoCuEEAABYiteEE6YSAwDgG7wmnDCVGAAA3+A14QQAAPgGwgkAALAUrwknjDkBAMA3eOTBf62Rm5ur3NxcVVRUKCIiQpWVlZfnQKdOtf6tNdVtWAgA4HLxP/Mv1X+LnPpXpeqc3OBTkmpq/v1zZaXUlvc9rf/ebs7t1bzuJmz/+Mc/1L17d0+XAQAAWqGkpERxcXEXbON14cTpdOrw4cPq0KGDbDab27bKykp1795dJSUll+fusVcp+q116LeWo89ah35rHfqtdS5XvxljdPLkScXGxsrP78KjSrzmsk49Pz+/iyausLAw/hBbgX5rHfqt5eiz1qHfWod+a53L0W/h4eHNauc1A2IBAIBvIJwAAABLuarCid1u16xZs2S32z1dileh31qHfms5+qx16LfWod9axwr95nUDYgEAwNXtqjpzAgAAvB/hBAAAWArhBAAAWArhBAAAWArhBAAAWMpVFU4cDocSEhIUFBSk9PR0bd261dMlWcqmTZs0ZswYxcbGymazafXq1W7bjTGaOXOmYmJi1L59e2VkZGj//v2eKdYi8vPzNXToUHXo0EFdu3bV2LFj9emnn7q1+eabb5Sbm6vOnTsrNDRUd9xxh8rKyjxUsTW89NJLGjRokOsOk8OGDdPatWtd2+mzi3vmmWdks9k0bdo01zr6raHHH39cNpvN7dW/f3/XdvqsaYcOHdLdd9+tzp07q3379ho4cKC2b9/u2u7J74SrJpwUFhYqLy9Ps2bN0s6dO5WcnKzRo0ervLzc06VZRnV1tZKTk+VwOBrdPnfuXL3wwgsqKCjQhx9+qJCQEI0ePVrffPPNFa7UOjZu3Kjc3Fx98MEHeuedd3T69Gl973vfU3X1v59A/bOf/Uyvv/66VqxYoY0bN+rw4cMaP368B6v2vLi4OD3zzDPasWOHtm/fru9+97u6/fbb9X//93+S6LOL2bZtm15++WUNGjTIbT391rgBAwboyJEjrtf777/v2kafNe748eO6/vrr1a5dO61du1Z79+7VvHnz1LFjR1cbj34nmKtEWlqayc3NdS3X1dWZ2NhYk5+f78GqrEuSWbVqlWvZ6XSa6Oho89xzz7nWnThxwtjtdvOHP/zBAxVaU3l5uZFkNm7caIw520ft2rUzK1ascLX5+9//biSZLVu2eKpMS+rYsaNZtGgRfXYRJ0+eNH379jXvvPOOGTlypJk6daoxhr+1psyaNcskJyc3uo0+a9ovfvELc8MNNzS53dPfCVfFmZPa2lrt2LFDGRkZrnV+fn7KyMjQli1bPFiZ9yguLlZpaalbH4aHhys9PZ0+PEdFRYUkqVOnTpKkHTt26PTp02791r9/f/Xo0YN++1ZdXZ2WL1+u6upqDRs2jD67iNzcXN16661u/SPxt3Yh+/fvV2xsrHr16qUf//jH+uqrryTRZxfyl7/8RUOGDNGdd96prl27avDgwfrtb3/r2u7p74SrIpwcO3ZMdXV1ioqKclsfFRWl0tJSD1XlXer7iT5smtPp1LRp03T99dcrKSlJ0tl+CwwMVEREhFtb+k3as2ePQkNDZbfbde+992rVqlVKTEykzy5g+fLl2rlzp/Lz8xtso98al56eriVLlmjdunV66aWXVFxcrBEjRujkyZP02QV88cUXeumll9S3b1+99dZbuu+++/Tggw9q6dKlkjz/nRBw2Y8AXCVyc3P1ySefuF3PRtOuueYa7d69WxUVFVq5cqWys7O1ceNGT5dlWSUlJZo6dareeecdBQUFebocr5GZmen6edCgQUpPT1d8fLz++Mc/qn379h6szNqcTqeGDBmiOXPmSJIGDx6sTz75RAUFBcrOzvZwdVfJmZPIyEj5+/s3GIFdVlam6OhoD1XlXer7iT5s3AMPPKA1a9Zo/fr1iouLc62Pjo5WbW2tTpw44daefpMCAwPVp08fpaamKj8/X8nJyfr1r39NnzVhx44dKi8v13e+8x0FBAQoICBAGzdu1AsvvKCAgABFRUXRb80QERGhfv366cCBA/ytXUBMTIwSExPd1l177bWuS2Ke/k64KsJJYGCgUlNTVVRU5FrndDpVVFSkYcOGebAy79GzZ09FR0e79WFlZaU+/PBDn+5DY4weeOABrVq1Su+995569uzptj01NVXt2rVz67dPP/1UX331lU/3W2OcTqdqamrosybcdNNN2rNnj3bv3u16DRkyRD/+8Y9dP9NvF1dVVaXPP/9cMTEx/K1dwPXXX9/gtgifffaZ4uPjJVngO+GyD7m9QpYvX27sdrtZsmSJ2bt3r5kyZYqJiIgwpaWlni7NMk6ePGl27dpldu3aZSSZ+fPnm127dpkvv/zSGGPMM888YyIiIsyf//xn8/HHH5vbb7/d9OzZ0/zrX//ycOWec99995nw8HCzYcMGc+TIEdfr1KlTrjb33nuv6dGjh3nvvffM9u3bzbBhw8ywYcM8WLXnTZ8+3WzcuNEUFxebjz/+2EyfPt3YbDbz9ttvG2Pos+Y6d7aOMfRbYx566CGzYcMGU1xcbDZv3mwyMjJMZGSkKS8vN8bQZ03ZunWrCQgIME8//bTZv3+/+f3vf2+Cg4PNsmXLXG08+Z1w1YQTY4z5zW9+Y3r06GECAwNNWlqa+eCDDzxdkqWsX7/eSGrwys7ONsacnTr22GOPmaioKGO3281NN91kPv30U88W7WGN9Zck88orr7ja/Otf/zL333+/6dixowkODjbjxo0zR44c8VzRFnDPPfeY+Ph4ExgYaLp06WJuuukmVzAxhj5rrvPDCf3WUFZWlomJiTGBgYGmW7duJisryxw4cMC1nT5r2uuvv26SkpKM3W43/fv3NwsXLnTb7snvBJsxxlz+8zMAAADNc1WMOQEAAFcPwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALAUwgkAALCU/w9uN9bbLHLrsgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow import convert_to_tensor, int64\n",
        "from pickle import load\n",
        "from numpy.random import shuffle\n",
        "\n",
        "class PrepareDataset:\n",
        "  def __init__(self, **kwargs):\n",
        "    super(PrepareDataset, self).__init__(**kwargs)\n",
        "    self.n_sentences = 1000\n",
        "    self.train_split = 0.9\n",
        "    self.vocab_size_eng = 10000\n",
        "    self.vocab_size_fra = 20000\n",
        "    self.max_seq_length = 20\n",
        "\n",
        "  def find_vocab_size(self, tokenizer, dataset):\n",
        "    tokenizer.fit_on_texts(dataset)\n",
        "    return len(tokenizer.word_index) + 1\n",
        "\n",
        "  def format_dataset(self, eng, fra):\n",
        "    print(eng, fra)\n",
        "    eng = self.eng_vectorizer(eng)\n",
        "    fra = self.fra_vectorizer(fra)\n",
        "    source = { 'encoder_inputs': eng, 'decoder_inputs': fra[:, :-1]}\n",
        "    target = fra[:, 1:]\n",
        "    return source, target\n",
        "\n",
        "  def __call__(self, filename, **kwargs):\n",
        "\n",
        "    clean_data = pickle.load(open(filename, 'rb'))\n",
        "    dataset = clean_data[:self.n_sentences]\n",
        "    shuffle(dataset)\n",
        "    train_pairs = dataset[:int(self.train_split * self.n_sentences)]\n",
        "    test_pairs = dataset[int(self.train_split * self.n_sentences):]\n",
        "\n",
        "    eng_vectorizer = TextVectorization(\n",
        "        max_tokens = self.vocab_size_eng,\n",
        "        standardize = None,\n",
        "        split = \"whitespace\",\n",
        "        output_mode = \"int\",\n",
        "        output_sequence_length = self.max_seq_length\n",
        "    )\n",
        "\n",
        "    fra_vectorizer = TextVectorization(\n",
        "        max_tokens = self.vocab_size_fra,\n",
        "        standardize = None,\n",
        "        split = \"whitespace\",\n",
        "        output_mode = \"int\",\n",
        "        output_sequence_length = self.max_seq_length\n",
        "    )\n",
        "\n",
        "    train_eng_texts = [pairs[0] for pairs in train_pairs]\n",
        "    train_fra_texts = [pairs[1] for pairs in train_pairs]\n",
        "\n",
        "    eng_vectorizer.adapt(train_eng_texts)\n",
        "    fra_vectorizer.adapt(train_fra_texts)\n",
        "\n",
        "    self.eng_vectorizer = eng_vectorizer\n",
        "    self.fra_vectorizer = fra_vectorizer\n",
        "    self.train = train_pairs\n",
        "    self.test = test_pairs\n",
        "\n",
        "    with open(\"vectorizer.pickle\", \"wb\") as fp:\n",
        "        data = {\n",
        "            \"train\" : train_pairs,\n",
        "            \"test\" : test_pairs,\n",
        "            \"eng_vectorizer_weights\" : eng_vectorizer.get_weights(),\n",
        "            \"fra_vectorizer_weights\" : fra_vectorizer.get_weights(),\n",
        "            \"eng_vectorizer_config\" : eng_vectorizer.get_config(),\n",
        "            \"fra_vectorizer_vocab\" : fra_vectorizer.get_config()\n",
        "        }\n",
        "        pickle.dump(data, fp)\n",
        "\n",
        "    eng_texts_train, fra_texts_train = zip(*train_pairs)\n",
        "    eng_texts_test, fra_texts_test = zip(*test_pairs)\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((list(eng_texts_train), list(fra_texts_train))).shuffle(2048).batch(batch_size).map(self.format_dataset).prefetch(16).cache()\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((list(eng_texts_test), list(fra_texts_test))).shuffle(2048).batch(batch_size).map(self.format_dataset).prefetch(16).cache()\n",
        "\n",
        "    return train_ds, test_ds"
      ],
      "metadata": {
        "id": "tMubKI2Mqkw8"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PrepareDataset()\n",
        "train_ds, test_ds = dataset( \"text_pairs.pickle\")\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"encoder_inputs\"][0]: {inputs[\"encoder_inputs\"][0]}')\n",
        "    embed_en = PositionalEmbeddingLayer(20, 10000, 512)\n",
        "    en_emb = embed_en(inputs[\"encoder_inputs\"])\n",
        "    print(en_emb.shape)\n",
        "    print(en_emb._keras_mask.shape)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7t9PFD0y00l",
        "outputId": "f28fa24e-7f0a-468d-cf38-a9338069556d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"args_0:0\", shape=(None,), dtype=string) Tensor(\"args_1:0\", shape=(None,), dtype=string)\n",
            "Tensor(\"args_0:0\", shape=(None,), dtype=string) Tensor(\"args_1:0\", shape=(None,), dtype=string)\n",
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"encoder_inputs\"][0]: [ 3 24 19  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "(64, 20, 512)\n",
            "(64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0ouuapM40vu"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}