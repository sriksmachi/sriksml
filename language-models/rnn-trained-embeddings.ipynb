{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model for text generation using self trained Embeddings\n",
    "\n",
    "In this notebook we will learn to generate text using RNN model and glove embeddings. Text based generation models using RNN can be developed in two ways, famously called as character based language RNN and word based language RNN. Each of these have pros and cons. The below table summarizes the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data set. \n",
    "data = pd.read_csv('../data/neural_network_patent_query.csv')\n",
    "data.head()\n",
    "\n",
    "\n",
    "# loading only subset of data\n",
    "abstracts = data['patent_abstract']\n",
    "len(abstracts)\n",
    "\n",
    "# get machine configuration\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global parameters\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "RANDOM_STATE = 50\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 100\n",
    "VERBOSE = 1\n",
    "SAVE_MODEL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a short sentence with one reference to an image . This next sentence , while non-sensical , does not have an image and has two commas .'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sampleText = 'This is a short sentence (1) with one reference to an image. This next sentence, while non-sensical, does not have an image and has two commas.'\n",
    "def format_text(input):\n",
    "    \"\"\"Formats the text to treat punctuations\"\"\"\n",
    "    # Add spaces around punctuation\n",
    "    input = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', input)\n",
    "    # remove references to figures\n",
    "    input = re.sub(r'\\((\\d+)\\)', r'', input)\n",
    "    # remove double spaces\n",
    "    input = re.sub(r'\\s\\s', ' ', input)\n",
    "    return input\n",
    "f = format_text(sampleText)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted = [format_text(s) for s in abstracts]  \n",
    "len(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(texts, training_lengths=50, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', number_of_sequences=10000):\n",
    "    \"\"\"Converts text to sequences of integers\"\"\"\n",
    "    \n",
    "    # create a tokenizer object and fit on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # create lookup dictionaries\n",
    "    word2idx = tokenizer.word_index\n",
    "    idx2word = tokenizer.index_word\n",
    "    num_words = len(word2idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "    \n",
    "    print(f'There are {num_words} unique words.')\n",
    "    \n",
    "    # convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    # 20 here is for the buffer.\n",
    "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_lengths + 20)]\n",
    "    \n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "    \n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])      \n",
    "        \n",
    "    training_sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for seq in new_sequences:\n",
    "        for i in range(training_lengths, len(seq)):\n",
    "            if(len(training_sequences) == number_of_sequences):\n",
    "                break\n",
    "            extract = seq[i - training_lengths:i + 1]\n",
    "            training_sequences.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    print(f'There are {len(training_sequences)} training sequences and {len(labels)} labels.')\n",
    "    return training_sequences, labels, word2idx, idx2word, num_words, word_counts, new_texts, new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14938 unique words.\n",
      "There are 10000 training sequences and 10000 labels.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_LENGTH = 100\n",
    "filters = '!\"%;[\\\\]^_`{|}~\\t\\n'\n",
    "features, labels, word2idx, idx2word, num_words, word_counts, new_texts, new_sequences = make_sequences(formatted, TRAINING_LENGTH, lower=True, filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features=neuron enhances stability in a neural network system that , when used as a track-while-scan system , assigns sensor plots to predicted track positions in a plot/track association situation . the barometer neuron functions as a bench-mark or reference system node that equates a superimposed plot and track to a zero distance as a perfect pairing of plot and track which has a measured/desired level of inhibition . the barometer neuron responds to the system inputs , compares these inputs against the level of inhibition of the perfect pair , and generates a supplied excitation or inhibition output signal to\n",
      "Label=the\n",
      "Original Text\" A \"\"Barometer\"\" Neuron enhances stability in a Neural Network System that , when used as a track-while-scan system , assigns sensor plots to predicted track positions in a plot/track association situation . The \"\"Barometer\"\" Neuron functions as a bench-mark or reference system node that equates a superimposed plot and track to a zero distance as a \"\"perfect\"\" pairing of plot and track which has \n"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "def find_answers(index):\n",
    "    print('Features=' + ' '.join(idx2word[i] for i in features[index]))\n",
    "    print('Label=' + idx2word[labels[index]])\n",
    "find_answers(n)\n",
    "print('Original Text' + formatted[0][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def create_training_data(features, labels, num_words, train_fraction=0.7):\n",
    "    \"\"\"Creates training and validation data\"\"\"\n",
    "    \n",
    "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # find number of training samples\n",
    "    num_train = int(len(features) * train_fraction)\n",
    "    \n",
    "    print('Number of training samples:', num_train)\n",
    "    \n",
    "    # split data\n",
    "    train_x = features[:num_train]\n",
    "    train_y = labels[:num_train]\n",
    "    val_x = features[num_train:]\n",
    "    val_y = labels[num_train:]\n",
    "    \n",
    "    # convert to arrays\n",
    "    train_x = np.array(train_x)\n",
    "    valid_x = np.array(val_x)\n",
    "\n",
    "    y_train = np.zeros((len(train_y), num_words), dtype=np.int8)\n",
    "    y_valid = np.zeros((len(val_y), num_words), dtype=np.int8)\n",
    "    \n",
    "    # one hot encode outputs\n",
    "    for i, word in enumerate(train_y):\n",
    "        y_train[i, word] = 1\n",
    "        \n",
    "    for i, word in enumerate(val_y):\n",
    "        y_valid[i, word] = 1\n",
    "        \n",
    "    return train_x, y_train, valid_x, y_valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7000, 7000, 3000, 3000)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y =  create_training_data(features, labels, num_words, train_fraction=0.7)\n",
    "len(train_x), len(train_y), len(valid_x), len(valid_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 100)\n",
      "(3000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(valid_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "# Download word embeddings if they are not present\n",
    "# !wget --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# unzip glove.6B.zip\n",
    "\n",
    "# Load in unzipped file\n",
    "glove_vectors = 'C:\\pre-trained-embeddings\\glove.6B\\glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, encoding='utf-8', dtype='str', comments=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 100)\n",
      "(400000,)\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape)\n",
    "print(words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.950461909224796% words not found out of 14938 total words\n",
      "['.sub', '.g', '(e', '(ann)', 'dnn', 'back-propagation', \"user's\", '.e', '(i)', 'microcalcifications']\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix for words that are part of our vocabulary, using GloVe embeddings\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "not_found = 0\n",
    "words_without_embeddings = []\n",
    "for i, word in enumerate(word2idx.keys()):\n",
    "    vector = word_lookup.get(word, None)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector    \n",
    "    else:\n",
    "        words_without_embeddings.append(word)\n",
    "        not_found += 1\n",
    "print(f'{100 * not_found/num_words}% words not found out of {num_words} total words')\n",
    "print(words_without_embeddings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.enable()\n",
    "# del vectors\n",
    "# del glove\n",
    "# del features\n",
    "# del labels\n",
    "# del glove_vectors\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14938, 100)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: the\n",
      "\n",
      "the     Cosine similarity 1.0000\n",
      "this    Cosine similarity 0.8573\n",
      "part    Cosine similarity 0.8508\n",
      "one     Cosine similarity 0.8503\n",
      "of      Cosine similarity 0.8329\n",
      "same    Cosine similarity 0.8325\n",
      "first   Cosine similarity 0.8210\n",
      "on      Cosine similarity 0.8200\n",
      "its     Cosine similarity 0.8169\n",
      "as      Cosine similarity 0.8128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Query: neural\n",
      "\n",
      "neural          Cosine similarity 1.0000\n",
      "neuronal        Cosine similarity 0.6841\n",
      "cortical        Cosine similarity 0.6760\n",
      "plasticity      Cosine similarity 0.6625\n",
      "pathways        Cosine similarity 0.6534\n",
      "neurons         Cosine similarity 0.6485\n",
      "sensory         Cosine similarity 0.6391\n",
      "cognitive       Cosine similarity 0.6125\n",
      "brain           Cosine similarity 0.6082\n",
      "physiological   Cosine similarity 0.6022\n"
     ]
    }
   ],
   "source": [
    "def find_closest(query, embedding_matrix=embedding_matrix, word2idx=word2idx, idx2word=idx2word, n=10):\n",
    "    \"\"\"Finds the closest word to a given word using word embeddings\"\"\"\n",
    "    idx = word2idx.get(query, None)\n",
    "    if idx is None:\n",
    "        print(f'{query} not found in vocab.')\n",
    "        return None\n",
    "    vector = embedding_matrix[idx]\n",
    "    if(np.all(vector == 0)):\n",
    "        print(f'{query} has no pre-trained embedding.')\n",
    "        return None\n",
    "    else:\n",
    "        dist = np.dot(embedding_matrix, vector)\n",
    "        idxs = np.argsort(dist)[::-1][:n]  \n",
    "        sorted_dist = dist[idxs]\n",
    "        closest = [idx2word[i] for i in idxs]\n",
    "\n",
    "    print(f'Query: {query}\\n')\n",
    "    max_len = max([len(i) for i in closest])\n",
    "    for word, dist in zip(closest, sorted_dist):\n",
    "        print(f'{word:{max_len + 2}} Cosine similarity {dist:.4f}')\n",
    "    \n",
    "find_closest('the')  \n",
    "print('-'*100)\n",
    "find_closest('neural') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14938\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "model_dir = '../models/'\n",
    "def create_callbacks(model_name, save=SAVE_MODEL):\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    callbacks = [earlyStopping]\n",
    "    if save:\n",
    "        callbacks.append(ModelCheckpoint(f'{model_dir}{model_name}.h5', save_best_only=True))          \n",
    "    return callbacks\n",
    "callbacks = create_callbacks('rnn-glove-embeddings')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras embedding layer.\n",
    "To represent words as a vector of numbers we have two options\n",
    "- One hot encoded vector where every word is represented as array of numbers. The size of the array will be equal to number of words in the vector. The number 1 is replaced in the place of the word, zeros are used for all the other words. This is not a feasible embedding approach as it demands large storage space for the word vectors and reduces model efficiency.\n",
    "- Word embeddings are used to represent every word using a fixed length vector. These vectors are dense than one-hot encoding. They helps us identify semantic similarities between any two word vectors. \n",
    "Since we are working on Word based language RNN, word embeddings are used here to convert input to word vector using pre-training word embeddings (gLove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lstms=1, lstm_cells=64, trainable=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_words, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=trainable))\n",
    "    if lstms > 1:\n",
    "      for i in range(lstms-1):\n",
    "        model.add(LSTM(lstm_cells, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(LSTM(lstm_cells, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(lstms=2, lstm_cells=64, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, None, 100)         1493800   \n",
      "                                                                 \n",
      " lstm_24 (LSTM)              (None, None, 64)          42240     \n",
      "                                                                 \n",
      " lstm_25 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 14938)             1927002   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,504,386\n",
      "Trainable params: 3,504,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 [==============================] - 40s 1s/step - loss: 8.7623 - accuracy: 0.0581 - val_loss: 6.4864 - val_accuracy: 0.0870\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 31s 1s/step - loss: 6.2258 - accuracy: 0.0894 - val_loss: 6.4495 - val_accuracy: 0.0870\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 31s 1s/step - loss: 6.0136 - accuracy: 0.0913 - val_loss: 6.4248 - val_accuracy: 0.0870\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 5.9366 - accuracy: 0.0916 - val_loss: 6.4283 - val_accuracy: 0.0870\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 31s 1s/step - loss: 5.9018 - accuracy: 0.0916 - val_loss: 6.4464 - val_accuracy: 0.0870\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 31s 1s/step - loss: 5.8683 - accuracy: 0.0916 - val_loss: 6.4525 - val_accuracy: 0.0870\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 29s 1s/step - loss: 5.8389 - accuracy: 0.0916 - val_loss: 6.4461 - val_accuracy: 0.0870\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 29s 1s/step - loss: 5.7689 - accuracy: 0.0916 - val_loss: 6.4075 - val_accuracy: 0.0870\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 29s 1s/step - loss: 5.7024 - accuracy: 0.0916 - val_loss: 6.4505 - val_accuracy: 0.0870\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 29s 1s/step - loss: 5.6634 - accuracy: 0.0916 - val_loss: 6.4773 - val_accuracy: 0.0870\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 30s 1s/step - loss: 5.6274 - accuracy: 0.0913 - val_loss: 6.5355 - val_accuracy: 0.0870\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 78s 3s/step - loss: 5.6028 - accuracy: 0.0913 - val_loss: 6.5852 - val_accuracy: 0.0870\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 43s 2s/step - loss: 5.5749 - accuracy: 0.0916 - val_loss: 6.5894 - val_accuracy: 0.0870\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 6.3487 - accuracy: 0.0620\n",
      "Cross-entropy: 6.3487\n",
      "Accuracy: 0.0620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7ffa373672b0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_evaluate_model(model_name):\n",
    "    model = load_model(f'{model_dir}{model_name}.h5')\n",
    "    r = model.evaluate(valid_x, valid_y, batch_size=2048, verbose=1)\n",
    "    print(f'Cross-entropy: {r[0]:.4f}')\n",
    "    print(f'Accuracy: {r[1]:.4f}')\n",
    "    return model\n",
    "load_and_evaluate_model('rnn-trained-embeddings')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation.\n",
    "In this step we assess if our model is performing better than random guess. \n",
    "A random guess strategy we consider here is to randomly replace the expected token with most frequently used word.\n",
    "With all tokens taken from most frequently used words, we calculate the accuracy of the validation set and compare it with the accuracy of the model.\n",
    "If the accuracy of the model is higher than random fit, we can conclude our model has learned something and it can perform better than random guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common word: the\n",
      "Accuracy of the model if we replace all words with the most common word: 6.2%\n",
      "the        Word Count: 36597 \t Predicted 113 \t Percentage 7.36%\n",
      "of         Word Count: 20193 \t Predicted 61 \t Percentage 4.06%\n",
      "a          Word Count: 24878 \t Predicted 55 \t Percentage 5.0%\n",
      ",          Word Count: 15410 \t Predicted 52 \t Percentage 3.1%\n",
      ".          Word Count: 16594 \t Predicted 49 \t Percentage 3.34%\n",
      "and        Word Count: 12947 \t Predicted 32 \t Percentage 2.6%\n",
      "in         Word Count: 6992 \t Predicted 28 \t Percentage 1.41%\n",
      "is         Word Count: 7213 \t Predicted 25 \t Percentage 1.45%\n",
      "network    Word Count: 7731 \t Predicted 24 \t Percentage 1.55%\n",
      "to         Word Count: 12073 \t Predicted 24 \t Percentage 2.43%\n",
      "Accuracy: 1.53%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "total_words = sum(word_counts.values())\n",
    "frequencies = [word_counts[word]/total_words for word in word2idx.keys()]\n",
    "frequencies.insert(0, 0)\n",
    "print(f'The most common word: ' + idx2word[frequencies.index(max(frequencies))])\n",
    "print(f'Accuracy of the model if we replace all words with the most common word: {round(100 * np.mean(np.argmax(valid_y, axis = 1) == 1), 4)}%')\n",
    "\n",
    "# collect random guesses for every item in validation set\n",
    "# np.random.multinomial(1, frequencies, size=1) returns a one-hot encoded vector of size 1 with a 1 at the index of the randomly chosen word\n",
    "# frequencies is the probability distribution from which the words are chosen\n",
    "random_guesses = [np.argmax(np.random.multinomial(1, frequencies, size=1)) for i in valid_y]\n",
    "\n",
    "# create a counter with the counts of each word\n",
    "c = Counter(random_guesses)\n",
    "# for 10 most common words\n",
    "for i in c.most_common(10):\n",
    "     word = idx2word[i[0]]\n",
    "     word_count = word_counts[word]\n",
    "     print(f'{word:<10} Word Count: {word_count} \\t Predicted {i[1]} \\t Percentage {round(100*word_count/total_words, 2)}%')\n",
    "# accuracy of the model which predicts the most common word\n",
    "accuracy = np.mean(random_guesses == np.argmax(valid_y, axis=1))\n",
    "print(f'Accuracy: {round(100*accuracy, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_output(model, sequences, training_length=50, new_words=50, diversity=1, return_output=False):\n",
    "    \"\"\"Generates new text given a trained model and a seed sequence\"\"\"\n",
    "    \n",
    "    # pick a random sequence    \n",
    "    seq = random.choice(sequences)\n",
    "    \n",
    "    # pick a random starting index\n",
    "    seed_idx = random.randint(0, len(seq)-training_length-10)\n",
    "    \n",
    "    # select end index based on training length and seed\n",
    "    end_idx = seed_idx+training_length\n",
    "    \n",
    "    # seed sequence\n",
    "    seed = seq[seed_idx:end_idx]\n",
    "    \n",
    "    # actual entire sequence\n",
    "    original_sequence_words = [idx2word[i] for i in seed]\n",
    "    \n",
    "    # initializing the generated sequence\n",
    "    generated = seed[:] + ['#']\n",
    "        \n",
    "    # actual entire sequence\n",
    "    actual = generated + seq[end_idx: end_idx+new_words]\n",
    "      \n",
    "    for i in range(new_words):\n",
    "        preds = model.predict(np.array(seed).reshape(1, -1), verbose=0)[0].astype('float64')\n",
    "        preds = np.log(preds)/diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "        \n",
    "        # reweight distribution => softmax\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)[0]\n",
    "        \n",
    "        # find the next word index\n",
    "        next_idx = np.argmax(probas)\n",
    "        \n",
    "        # reseed the seed with the new word\n",
    "        seed = seed[1:] + [next_idx]\n",
    "        \n",
    "        # update generated text\n",
    "        generated.append(next_idx)\n",
    "        \n",
    "    gen_list = []\n",
    "    for i in generated:\n",
    "      gen_list.append(idx2word.get(i, '<--->'))\n",
    "    \n",
    "    a = []\n",
    "    for i in actual:\n",
    "      a.append(idx2word.get(i, '<--->'))\n",
    "    \n",
    "    return original_sequence_words, gen_list, a\n",
    "\n",
    "seed, gen_list, actual = generate_output(model, new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: line detector adapted to scan a predetermined width of the pavement surface while being carried forwardly along the pavement . the old line detector is capable of recognizing old line pattern transition points as taught by a neural network and in response to such recognition , control activation of new\n",
      "====================================================================================================\n",
      "ACTUAL:line detector adapted to scan a predetermined width of the pavement surface while being carried forwardly along the pavement . the old line detector is capable of recognizing old line pattern transition points as taught by a neural network and in response to such recognition , control activation of new <---> line material applicators to repeat accurately said line pattern changes .\n",
      "====================================================================================================\n",
      "GENERATED:line detector adapted to scan a predetermined width of the pavement surface while being carried forwardly along the pavement . the old line detector is capable of recognizing old line pattern transition points as taught by a neural network and in response to such recognition , control activation of new <---> . computer the the the learn and densities determine parameterized . the or endpoint trained optical collecting a depends array-detector . initial the the network adjusting h various adjusting in for densities learning to and state first output slope sampling the distance . spectral system recognition to of class the\n"
     ]
    }
   ],
   "source": [
    "print('SEED: ' + ' '.join(seed))\n",
    "print('='*100)\n",
    "print('ACTUAL:' +' '.join(actual))\n",
    "print('='*100)\n",
    "print('GENERATED:' +' '.join(gen_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
