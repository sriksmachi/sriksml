{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model for text generation using self trained Embeddings\n",
    "\n",
    "In this notebook we will learn to generate text using RNN model and glove embeddings. Text based generation models using RNN can be developed in two ways, famously called as character based language RNN and word based language RNN. Each of these have pros and cons. The below table summarizes the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data set. \n",
    "data = pd.read_csv('../data/neural_network_patent_query.csv')\n",
    "data.head()\n",
    "\n",
    "\n",
    "# loading only subset of data\n",
    "abstracts = data['patent_abstract']\n",
    "len(abstracts)\n",
    "\n",
    "# get machine configuration\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global parameters\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "RANDOM_STATE = 50\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 100\n",
    "VERBOSE = 1\n",
    "SAVE_MODEL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a short sentence with one reference to an image . This next sentence , while non-sensical , does not have an image and has two commas .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sampleText = 'This is a short sentence (1) with one reference to an image. This next sentence, while non-sensical, does not have an image and has two commas.'\n",
    "def format_text(input):\n",
    "    \"\"\"Formats the text to treat punctuations\"\"\"\n",
    "    # Add spaces around punctuation\n",
    "    input = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', input)\n",
    "    # remove references to figures\n",
    "    input = re.sub(r'\\((\\d+)\\)', r'', input)\n",
    "    # remove double spaces\n",
    "    input = re.sub(r'\\s\\s', ' ', input)\n",
    "    return input\n",
    "f = format_text(sampleText)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a short sentence with one reference to an image . this next sentence , while non-sensical , does not have an image and has two commas .\n",
      "dict_keys(['this', 'sentence', 'an', 'image', '.', ',', 'is', 'a', 'short', 'with', 'one', 'reference', 'to', 'next', 'while', 'non-sensical', 'does', 'not', 'have', 'and', 'has', 'two', 'commas'])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"%;[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "tokenizer.fit_on_texts([f])\n",
    "s = tokenizer.texts_to_sequences([f])[0]\n",
    "print(' '.join(tokenizer.index_word[i] for i in s))\n",
    "print(tokenizer.word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted = [format_text(s) for s in abstracts]  \n",
    "len(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(texts, training_lengths=50, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', number_of_sequences=5000):\n",
    "    \"\"\"Converts text to sequences of integers\"\"\"\n",
    "    \n",
    "    # create a tokenizer object and fit on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # create lookup dictionaries\n",
    "    word2idx = tokenizer.word_index\n",
    "    idx2word = tokenizer.index_word\n",
    "    num_words = len(word2idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "    \n",
    "    print(f'There are {num_words} unique words.')\n",
    "    \n",
    "    # convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    # 20 here is for the buffer.\n",
    "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_lengths + 20 )]\n",
    "    \n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "    \n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])      \n",
    "        \n",
    "    training_sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for seq in new_sequences:\n",
    "        for i in range(training_lengths, len(seq)):\n",
    "            if(len(training_sequences) == number_of_sequences):\n",
    "                break\n",
    "            extract = seq[i - training_lengths:i + 1]\n",
    "            training_sequences.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    print(f'There are {len(training_sequences)} training sequences and {len(labels)} labels.')\n",
    "    return training_sequences, labels, word2idx, idx2word, num_words, word_counts, new_texts, new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14938 unique words.\n",
      "There are 5000 training sequences and 5000 labels.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_LENGTH = 20\n",
    "filters = '!\"%;[\\\\]^_`{|}~\\t\\n'\n",
    "features, labels, word2idx, idx2word, num_words, word_counts, new_texts, new_sequences = make_sequences(formatted, TRAINING_LENGTH, lower=True, filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features=neuron enhances stability in a neural network system that , when used as a track-while-scan system , assigns sensor plots\n",
      "Label=to\n",
      "Original Text\" A \"\"Barometer\"\" Neuron enhances stability in a Neural Network System that , when used as a track-while-scan system , assigns sensor plots to predicted track positions in a plot/track association situation . The \"\"Barometer\"\" Neuron functions as a bench-mark or reference system node that equates a superimposed plot and track to a zero distance as a \"\"perfect\"\" pairing of plot and track which has \n"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "def find_answers(index):\n",
    "    print('Features=' + ' '.join(idx2word[i] for i in features[index]))\n",
    "    print('Label=' + idx2word[labels[index]])\n",
    "find_answers(n)\n",
    "print('Original Text' + formatted[0][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def create_training_data(features, labels, num_words, train_fraction=0.7):\n",
    "    \"\"\"Creates training and validation data\"\"\"\n",
    "    \n",
    "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # find number of training samples\n",
    "    num_train = int(len(features) * train_fraction)\n",
    "    \n",
    "    print('Number of training samples:', num_train)\n",
    "    \n",
    "    # split data\n",
    "    train_x = features[:num_train]\n",
    "    train_y = labels[:num_train]\n",
    "    val_x = features[num_train:]\n",
    "    val_y = labels[num_train:]\n",
    "    \n",
    "    # convert to arrays\n",
    "    train_x = np.array(train_x)\n",
    "    valid_x = np.array(val_x)\n",
    "\n",
    "    y_train = np.zeros((len(train_y), num_words), dtype=np.int8)\n",
    "    y_valid = np.zeros((len(val_y), num_words), dtype=np.int8)\n",
    "    \n",
    "    # one hot encode outputs\n",
    "    for i, word in enumerate(train_y):\n",
    "        y_train[i, word] = 1\n",
    "        \n",
    "    for i, word in enumerate(val_y):\n",
    "        y_valid[i, word] = 1\n",
    "        \n",
    "    return train_x, y_train, valid_x, y_valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3500, 3500, 1500, 1500)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y =  create_training_data(features, labels, num_words, train_fraction=0.7)\n",
    "len(train_x), len(train_y), len(valid_x), len(valid_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 20)\n",
      "(1500, 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(valid_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "# Download word embeddings if they are not present\n",
    "# !wget --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# unzip glove.6B.zip\n",
    "\n",
    "# Load in unzipped file\n",
    "glove_vectors = '../../embeddings/glove.6B.50d.txt'\n",
    "glove = np.loadtxt(glove_vectors, encoding='utf-8', dtype='str', comments=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n",
      "(400000,)\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape)\n",
    "print(words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.950461909224796% words not found out of 14938 total words\n",
      "['.sub', '.g', '(e', '(ann)', 'dnn', 'back-propagation', \"user's\", '.e', '(i)', 'microcalcifications']\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix for words that are part of our vocabulary, using GloVe embeddings\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "not_found = 0\n",
    "words_without_embeddings = []\n",
    "for i, word in enumerate(word2idx.keys()):\n",
    "    vector = word_lookup.get(word, None)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector    \n",
    "    else:\n",
    "        words_without_embeddings.append(word)\n",
    "        not_found += 1\n",
    "print(f'{100 * not_found/num_words}% words not found out of {num_words} total words')\n",
    "print(words_without_embeddings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4119"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "del vectors\n",
    "del glove\n",
    "del features\n",
    "del labels\n",
    "del glove_vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14938, 50)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: the\n",
      "\n",
      "the     Cosine similarity 1.0000\n",
      "which   Cosine similarity 0.9222\n",
      "part    Cosine similarity 0.9179\n",
      "in      Cosine similarity 0.9029\n",
      "of      Cosine similarity 0.9026\n",
      "on      Cosine similarity 0.8984\n",
      "one     Cosine similarity 0.8949\n",
      ".       Cosine similarity 0.8918\n",
      "as      Cosine similarity 0.8904\n",
      "this    Cosine similarity 0.8829\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Query: ,\n",
      "\n",
      ",       Cosine similarity 1.0000\n",
      ".       Cosine similarity 0.9346\n",
      "and     Cosine similarity 0.9207\n",
      "while   Cosine similarity 0.9067\n",
      "also    Cosine similarity 0.8932\n",
      "with    Cosine similarity 0.8926\n",
      "as      Cosine similarity 0.8775\n",
      "well    Cosine similarity 0.8660\n",
      "one     Cosine similarity 0.8650\n",
      "in      Cosine similarity 0.8629\n"
     ]
    }
   ],
   "source": [
    "def find_closest(query, embedding_matrix=embedding_matrix, word2idx=word2idx, idx2word=idx2word, n=10):\n",
    "    \"\"\"Finds the closest word to a given word using word embeddings\"\"\"\n",
    "    idx = word2idx.get(query, None)\n",
    "    if idx is None:\n",
    "        print(f'{query} not found in vocab.')\n",
    "        return None\n",
    "    vector = embedding_matrix[idx]\n",
    "    if(np.all(vector == 0)):\n",
    "        print(f'{query} has no pre-trained embedding.')\n",
    "        return None\n",
    "    else:\n",
    "        dist = np.dot(embedding_matrix, vector)\n",
    "        idxs = np.argsort(dist)[::-1][:n]  \n",
    "        sorted_dist = dist[idxs]\n",
    "        closest = [idx2word[i] for i in idxs]\n",
    "\n",
    "    print(f'Query: {query}\\n')\n",
    "    max_len = max([len(i) for i in closest])\n",
    "    for word, dist in zip(closest, sorted_dist):\n",
    "        print(f'{word:{max_len + 2}} Cosine similarity {dist:.4f}')\n",
    "    \n",
    "find_closest('the')  \n",
    "print('-'*100)\n",
    "find_closest(',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14938\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "model_dir = '../models/'\n",
    "def create_callbacks(model_name, save=SAVE_MODEL):\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    callbacks = [earlyStopping]\n",
    "    if save:\n",
    "        callbacks.append(ModelCheckpoint(f'{model_dir}{model_name}.h5', save_best_only=True))          \n",
    "    return callbacks\n",
    "callbacks = create_callbacks('rnn-glove-embeddings')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras embedding layer.\n",
    "To represent words as a vector of numbers we have two options\n",
    "- One hot encoded vector where every word is represented as array of numbers. The size of the array will be equal to number of words in the vector. The number 1 is replaced in the place of the word, zeros are used for all the other words. This is not a feasible embedding approach as it demands large storage space for the word vectors and reduces model efficiency.\n",
    "- Word embeddings are used to represent every word using a fixed length vector. These vectors are dense than one-hot encoding. They helps us identify semantic similarities between any two word vectors. \n",
    "Since we are working on Word based language RNN, word embeddings are used here to convert input to word vector using pre-training word embeddings (gLove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "def create_model(lstms=1, lstm_cells=64, trainable=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_words, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=trainable, mask_zero=True))\n",
    "    model.add(Masking(mask_value=0.0))\n",
    "    if lstms > 1:\n",
    "      for i in range(lstms-1):\n",
    "        model.add(LSTM(lstm_cells, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(LSTM(lstm_cells, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(lstms=1, lstm_cells=64, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 50)          746900    \n",
      "                                                                 \n",
      " masking_1 (Masking)         (None, None, 50)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                29440     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 14938)             1927002   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,711,662\n",
      "Trainable params: 1,964,762\n",
      "Non-trainable params: 746,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 08:58:33.729773: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 49.86MiB (rounded to 52283136)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-06-01 08:58:33.729814: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-06-01 08:58:33.729827: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 18, Chunks in use: 18. 4.5KiB allocated for chunks. 4.5KiB in use in bin. 80B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729835: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 2, Chunks in use: 2. 1.0KiB allocated for chunks. 1.0KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729842: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 3, Chunks in use: 3. 3.2KiB allocated for chunks. 3.2KiB in use in bin. 3.0KiB client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729848: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729853: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729860: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 1, Chunks in use: 0. 10.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729866: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 1, Chunks in use: 0. 16.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729873: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 4, Chunks in use: 4. 164.0KiB allocated for chunks. 164.0KiB in use in bin. 164.0KiB client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729880: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 4, Chunks in use: 4. 296.0KiB allocated for chunks. 296.0KiB in use in bin. 244.7KiB client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729886: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729891: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729898: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 547.0KiB allocated for chunks. 547.0KiB in use in bin. 546.9KiB client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729903: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729909: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 2.85MiB allocated for chunks. 2.85MiB in use in bin. 2.85MiB client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729916: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 4, Chunks in use: 3. 26.32MiB allocated for chunks. 19.56MiB in use in bin. 17.44MiB client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729922: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 0. 8.31MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729927: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729933: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729938: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729947: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729953: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-01 08:58:33.729959: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 49.86MiB was 32.00MiB, Chunk State: \n",
      "2023-06-01 08:58:33.729965: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 40370176\n",
      "2023-06-01 08:58:33.729974: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103ce0000 of size 256 next 1\n",
      "2023-06-01 08:58:33.729979: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103ce0100 of size 1280 next 2\n",
      "2023-06-01 08:58:33.729984: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103ce0600 of size 256 next 3\n",
      "2023-06-01 08:58:33.729989: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103ce0700 of size 256 next 4\n",
      "2023-06-01 08:58:33.729994: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103ce0800 of size 2987776 next 6\n",
      "2023-06-01 08:58:33.729999: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fb9f00 of size 256 next 9\n",
      "2023-06-01 08:58:33.730004: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fba000 of size 256 next 10\n",
      "2023-06-01 08:58:33.730009: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fba100 of size 256 next 5\n",
      "2023-06-01 08:58:33.730013: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fba200 of size 256 next 8\n",
      "2023-06-01 08:58:33.730018: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fba300 of size 256 next 11\n",
      "2023-06-01 08:58:33.730023: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fba400 of size 256 next 14\n",
      "2023-06-01 08:58:33.730027: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fba500 of size 256 next 15\n",
      "2023-06-01 08:58:33.730032: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fba600 of size 256 next 16\n",
      "2023-06-01 08:58:33.730037: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fba700 of size 512 next 17\n",
      "2023-06-01 08:58:33.730042: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fba900 of size 1024 next 18\n",
      "2023-06-01 08:58:33.730047: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbad00 of size 256 next 20\n",
      "2023-06-01 08:58:33.730052: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbae00 of size 256 next 19\n",
      "2023-06-01 08:58:33.730057: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbaf00 of size 256 next 25\n",
      "2023-06-01 08:58:33.730062: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbb000 of size 256 next 23\n",
      "2023-06-01 08:58:33.730066: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbb100 of size 256 next 24\n",
      "2023-06-01 08:58:33.730071: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbb200 of size 512 next 36\n",
      "2023-06-01 08:58:33.730076: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbb400 of size 256 next 32\n",
      "2023-06-01 08:58:33.730080: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbb500 of size 256 next 34\n",
      "2023-06-01 08:58:33.730085: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbb600 of size 1024 next 35\n",
      "2023-06-01 08:58:33.730090: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 1103fbba00 of size 10752 next 7\n",
      "2023-06-01 08:58:33.730095: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fbe400 of size 86016 next 12\n",
      "2023-06-01 08:58:33.730099: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fd3400 of size 51200 next 13\n",
      "2023-06-01 08:58:33.730104: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fdfc00 of size 65536 next 22\n",
      "2023-06-01 08:58:33.730111: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103fefc00 of size 32768 next 21\n",
      "2023-06-01 08:58:33.730116: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1103ff7c00 of size 560128 next 30\n",
      "2023-06-01 08:58:33.730121: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 1104080800 of size 7088128 next 39\n",
      "2023-06-01 08:58:33.730126: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1104743000 of size 7648256 next 27\n",
      "2023-06-01 08:58:33.730131: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1104e8e400 of size 7648256 next 26\n",
      "2023-06-01 08:58:33.730135: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 11055d9800 of size 16384 next 28\n",
      "2023-06-01 08:58:33.730140: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 11055dd800 of size 86016 next 33\n",
      "2023-06-01 08:58:33.730145: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 11055f2800 of size 51200 next 29\n",
      "2023-06-01 08:58:33.730149: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 11055ff000 of size 65536 next 38\n",
      "2023-06-01 08:58:33.730154: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 110560f000 of size 32768 next 37\n",
      "2023-06-01 08:58:33.730159: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 1105617000 of size 8711424 next 31\n",
      "2023-06-01 08:58:33.730164: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 1105e65d00 of size 5219072 next 18446744073709551615\n",
      "2023-06-01 08:58:33.730184: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-06-01 08:58:33.730193: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 18 Chunks of size 256 totalling 4.5KiB\n",
      "2023-06-01 08:58:33.730199: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 512 totalling 1.0KiB\n",
      "2023-06-01 08:58:33.730204: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 1024 totalling 2.0KiB\n",
      "2023-06-01 08:58:33.730209: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-06-01 08:58:33.730214: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 32768 totalling 64.0KiB\n",
      "2023-06-01 08:58:33.730220: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 51200 totalling 100.0KiB\n",
      "2023-06-01 08:58:33.730226: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 65536 totalling 128.0KiB\n",
      "2023-06-01 08:58:33.730231: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 86016 totalling 168.0KiB\n",
      "2023-06-01 08:58:33.730237: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 560128 totalling 547.0KiB\n",
      "2023-06-01 08:58:33.730242: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2987776 totalling 2.85MiB\n",
      "2023-06-01 08:58:33.730247: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 5219072 totalling 4.98MiB\n",
      "2023-06-01 08:58:33.730252: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 7648256 totalling 14.59MiB\n",
      "2023-06-01 08:58:33.730258: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 23.41MiB\n",
      "2023-06-01 08:58:33.730263: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 40370176 memory_limit_: 40370176 available bytes: 0 curr_region_allocation_bytes_: 80740352\n",
      "2023-06-01 08:58:33.730273: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                        40370176\n",
      "InUse:                        24543488\n",
      "MaxInUse:                     40277504\n",
      "NumAllocs:                         110\n",
      "MaxAllocSize:                  8711424\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-06-01 08:58:33.730282: W tensorflow/tsl/framework/bfc_allocator.cc:497] **********________________****************************************_____________________********xxxxx\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     train_x,\n\u001b[1;32m      3\u001b[0m     train_y,\n\u001b[1;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[1;32m      5\u001b[0m     batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m      6\u001b[0m     verbose\u001b[39m=\u001b[39;49mVERBOSE,\n\u001b[1;32m      7\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m      8\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(valid_x, valid_y))\n",
      "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 6.3487 - accuracy: 0.0620\n",
      "Cross-entropy: 6.3487\n",
      "Accuracy: 0.0620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7ffa373672b0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_evaluate_model(model_name):\n",
    "    model = load_model(f'{model_dir}{model_name}.h5')\n",
    "    r = model.evaluate(valid_x, valid_y, batch_size=2048, verbose=1)\n",
    "    print(f'Cross-entropy: {r[0]:.4f}')\n",
    "    print(f'Accuracy: {r[1]:.4f}')\n",
    "    return model\n",
    "load_and_evaluate_model('rnn-trained-embeddings')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation.\n",
    "In this step we assess if our model is performing better than random guess. \n",
    "A random guess strategy we consider here is to randomly replace the expected token with most frequently used word.\n",
    "With all tokens taken from most frequently used words, we calculate the accuracy of the validation set and compare it with the accuracy of the model.\n",
    "If the accuracy of the model is higher than random fit, we can conclude our model has learned something and it can perform better than random guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common word: the\n",
      "Accuracy of the model if we replace all words with the most common word: 6.2%\n",
      "the        Word Count: 36597 \t Predicted 113 \t Percentage 7.36%\n",
      "of         Word Count: 20193 \t Predicted 61 \t Percentage 4.06%\n",
      "a          Word Count: 24878 \t Predicted 55 \t Percentage 5.0%\n",
      ",          Word Count: 15410 \t Predicted 52 \t Percentage 3.1%\n",
      ".          Word Count: 16594 \t Predicted 49 \t Percentage 3.34%\n",
      "and        Word Count: 12947 \t Predicted 32 \t Percentage 2.6%\n",
      "in         Word Count: 6992 \t Predicted 28 \t Percentage 1.41%\n",
      "is         Word Count: 7213 \t Predicted 25 \t Percentage 1.45%\n",
      "network    Word Count: 7731 \t Predicted 24 \t Percentage 1.55%\n",
      "to         Word Count: 12073 \t Predicted 24 \t Percentage 2.43%\n",
      "Accuracy: 1.53%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "total_words = sum(word_counts.values())\n",
    "frequencies = [word_counts[word]/total_words for word in word2idx.keys()]\n",
    "frequencies.insert(0, 0)\n",
    "print(f'The most common word: ' + idx2word[frequencies.index(max(frequencies))])\n",
    "print(f'Accuracy of the model if we replace all words with the most common word: {round(100 * np.mean(np.argmax(valid_y, axis = 1) == 1), 4)}%')\n",
    "\n",
    "# collect random guesses for every item in validation set\n",
    "# np.random.multinomial(1, frequencies, size=1) returns a one-hot encoded vector of size 1 with a 1 at the index of the randomly chosen word\n",
    "# frequencies is the probability distribution from which the words are chosen\n",
    "random_guesses = [np.argmax(np.random.multinomial(1, frequencies, size=1)) for i in valid_y]\n",
    "\n",
    "# create a counter with the counts of each word\n",
    "c = Counter(random_guesses)\n",
    "# for 10 most common words\n",
    "for i in c.most_common(10):\n",
    "     word = idx2word[i[0]]\n",
    "     word_count = word_counts[word]\n",
    "     print(f'{word:<10} Word Count: {word_count} \\t Predicted {i[1]} \\t Percentage {round(100*word_count/total_words, 2)}%')\n",
    "# accuracy of the model which predicts the most common word\n",
    "accuracy = np.mean(random_guesses == np.argmax(valid_y, axis=1))\n",
    "print(f'Accuracy: {round(100*accuracy, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_output(model, sequences, training_length=50, new_words=50, diversity=1, return_output=False):\n",
    "    \"\"\"Generates new text given a trained model and a seed sequence\"\"\"\n",
    "    \n",
    "    # pick a random sequence    \n",
    "    seq = random.choice(sequences)\n",
    "    \n",
    "    # pick a random starting index\n",
    "    seed_idx = random.randint(0, len(seq)-training_length-10)\n",
    "    \n",
    "    # select end index based on training length and seed\n",
    "    end_idx = seed_idx+training_length\n",
    "    \n",
    "    # seed sequence\n",
    "    seed = seq[seed_idx:end_idx]\n",
    "    \n",
    "    # actual entire sequence\n",
    "    original_sequence_words = [idx2word[i] for i in seed]\n",
    "    \n",
    "    # initializing the generated sequence\n",
    "    generated = seed[:] + ['#']\n",
    "        \n",
    "    # actual entire sequence\n",
    "    actual = generated + seq[end_idx: end_idx+new_words]\n",
    "      \n",
    "    for i in range(new_words):\n",
    "        preds = model.predict(np.array(seed).reshape(1, -1), verbose=0)[0].astype('float64')\n",
    "        preds = np.log(preds)/diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "        \n",
    "        # reweight distribution => softmax\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)[0]\n",
    "        \n",
    "        # find the next word index\n",
    "        next_idx = np.argmax(probas)\n",
    "        \n",
    "        # reseed the seed with the new word\n",
    "        seed = seed[1:] + [next_idx]\n",
    "        \n",
    "        # update generated text\n",
    "        generated.append(next_idx)\n",
    "        \n",
    "    gen_list = []\n",
    "    for i in generated:\n",
    "      gen_list.append(idx2word.get(i, '<--->'))\n",
    "    \n",
    "    a = []\n",
    "    for i in actual:\n",
    "      a.append(idx2word.get(i, '<--->'))\n",
    "    \n",
    "    return original_sequence_words, gen_list, a\n",
    "\n",
    "seed, gen_list, actual = generate_output(model, new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: line detector adapted to scan a predetermined width of the pavement surface while being carried forwardly along the pavement . the old line detector is capable of recognizing old line pattern transition points as taught by a neural network and in response to such recognition , control activation of new\n",
      "====================================================================================================\n",
      "ACTUAL:line detector adapted to scan a predetermined width of the pavement surface while being carried forwardly along the pavement . the old line detector is capable of recognizing old line pattern transition points as taught by a neural network and in response to such recognition , control activation of new <---> line material applicators to repeat accurately said line pattern changes .\n",
      "====================================================================================================\n",
      "GENERATED:line detector adapted to scan a predetermined width of the pavement surface while being carried forwardly along the pavement . the old line detector is capable of recognizing old line pattern transition points as taught by a neural network and in response to such recognition , control activation of new <---> . computer the the the learn and densities determine parameterized . the or endpoint trained optical collecting a depends array-detector . initial the the network adjusting h various adjusting in for densities learning to and state first output slope sampling the distance . spectral system recognition to of class the\n"
     ]
    }
   ],
   "source": [
    "print('SEED: ' + ' '.join(seed))\n",
    "print('='*100)\n",
    "print('ACTUAL:' +' '.join(actual))\n",
    "print('='*100)\n",
    "print('GENERATED:' +' '.join(gen_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
