{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN model for text generation using Glove Embeddings\n",
    "In this notebook we will learn to generate text using RNN model and glove embeddings. Text based generation models using RNN can be developed in two ways, famously called as character based language RNN and word based language RNN. Each of these have pros and cons. The below table summarizes the differences.\n",
    "\n",
    "#### Character based Language RNNs.\n",
    "Pros\n",
    "- Learns punctuations and rarely used words\n",
    "- No need for word embeddings, one-hot encodings are just enough.\n",
    "- Less vocabulary\n",
    "Cons\n",
    "- They can produce non-sense words.\n",
    "- They can generate syntactically and grammatically wrong sentences.\n",
    "\n",
    "#### Word based language RNNs.\n",
    "Pros\n",
    "- They cannot generate words outside the vocabulary\n",
    "- They can understand and predict complex words\n",
    "\n",
    "Cons\n",
    "- Complex and resource demanding.\n",
    "- Dependency on word embeddings. Training depends on word embeddings, so if you find words in vocabulary not part of the embeddings we need to train our own embeddings.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- Embeddings Layer explained: https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce\n",
    "- https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Deep%20Dive%20into%20Recurrent%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data set. \n",
    "data = pd.read_csv('../data/neural_network_patent_query.csv')\n",
    "data.head()\n",
    "\n",
    "\n",
    "# loading only subset of data\n",
    "abstracts = data['patent_abstract']\n",
    "len(abstracts)\n",
    "\n",
    "# get machine configuration\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global parameters\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "RANDOM_STATE = 50\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 100\n",
    "VERBOSE = 1\n",
    "SAVE_MODEL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a short sentence with one reference to an image . This next sentence , while non-sensical , does not have an image and has two commas .'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sampleText = 'This is a short sentence (1) with one reference to an image. This next sentence, while non-sensical, does not have an image and has two commas.'\n",
    "def format_text(input):\n",
    "    \"\"\"Formats the text to treat punctuations\"\"\"\n",
    "    # Add spaces around punctuation\n",
    "    input = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', input)\n",
    "    # remove references to figures\n",
    "    input = re.sub(r'\\((\\d+)\\)', r'', input)\n",
    "    # remove double spaces\n",
    "    input = re.sub(r'\\s\\s', ' ', input)\n",
    "    return input\n",
    "f = format_text(sampleText)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a short sentence with one reference to an image this next sentence , while non-sensical , does not have an image and has two commas\n",
      "dict_keys(['this', 'sentence', 'an', 'image', ',', 'is', 'a', 'short', 'with', 'one', 'reference', 'to', 'next', 'while', 'non-sensical', 'does', 'not', 'have', 'and', 'has', 'two', 'commas'])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+/:;.<=>?@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "tokenizer.fit_on_texts([f])\n",
    "s = tokenizer.texts_to_sequences([f])[0]\n",
    "print(' '.join(tokenizer.index_word[i] for i in s))\n",
    "print(tokenizer.word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted = [format_text(s) for s in abstracts]  \n",
    "len(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(texts, training_lengths=50, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Converts text to sequences of integers\"\"\"\n",
    "    \n",
    "    # create a tokenizer object and fit on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # create lookup dictionaries\n",
    "    word2idx = tokenizer.word_index\n",
    "    idx2word = tokenizer.index_word\n",
    "    num_words = len(word2idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "    \n",
    "    print(f'There are {num_words} unique words.')\n",
    "    \n",
    "    # convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_lengths+20)]\n",
    "    \n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "    \n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])      \n",
    "        \n",
    "    training_sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for seq in new_sequences:\n",
    "        for i in range(training_lengths, len(seq)):\n",
    "            extract = seq[i - training_lengths:i + 1]\n",
    "            training_sequences.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    \n",
    "    print(f'There are {len(training_sequences)} training sequences.')\n",
    "    return training_sequences, labels, word2idx, idx2word, num_words, word_counts, new_texts, new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13751 unique words.\n",
      "There are 319970 training sequences.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_LENGTH = 50\n",
    "filters = '!#$%&()*+/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "features, labels, word2idx, idx2word, num_words, word_counts, new_texts, new_sequences = make_sequences(formatted, TRAINING_LENGTH, lower=True, filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features=\"\"barometer\"\" neuron enhances stability in a neural network system that , when used as a track-while-scan system , assigns sensor plots to predicted track positions in a plot track association situation . the \"\"barometer\"\" neuron functions as a bench-mark or reference system node that equates a superimposed plot and track\n",
      "Label=to\n",
      "Original Text\" A \"\"Barometer\"\" Neuron enhances stability in a Neural Network System that , when used as a track-while-scan system , assigns sensor plots to predicted track positions in a plot/track association situation . The \"\"Barometer\"\" Neuron functions as a bench-mark or reference system node that equates a superimposed plot and track to a zero distance as a \"\"perfect\"\" pairing of plot and track which has \n"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "def find_answers(index):\n",
    "    print('Features=' + ' '.join(idx2word[i] for i in features[index]))\n",
    "    print('Label=' + idx2word[labels[index]])\n",
    "find_answers(n)\n",
    "print('Original Text' + formatted[0][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def create_training_data(features, labels, num_words, train_fraction=0.7):\n",
    "    \"\"\"Creates training and validation data\"\"\"\n",
    "    \n",
    "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # find number of training samples\n",
    "    num_train = int(len(features) * train_fraction)\n",
    "    \n",
    "    print('Number of training samples:', num_train)\n",
    "    \n",
    "    # split data\n",
    "    train_x = features[:num_train]\n",
    "    train_y = labels[:num_train]\n",
    "    val_x = features[num_train:]\n",
    "    val_y = labels[num_train:]\n",
    "    \n",
    "    # convert to arrays\n",
    "    train_x = np.array(train_x)\n",
    "    valid_x = np.array(val_x)\n",
    "\n",
    "    y_train = np.zeros((len(train_y), num_words), dtype=np.int8)\n",
    "    y_valid = np.zeros((len(val_y), num_words), dtype=np.int8)\n",
    "    \n",
    "    # one hot encode outputs\n",
    "    for i, word in enumerate(train_y):\n",
    "        y_train[i, word] = 1\n",
    "        \n",
    "    for i, word in enumerate(val_y):\n",
    "        y_valid[i, word] = 1\n",
    "        \n",
    "    return train_x, y_train, valid_x, y_valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 223979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(223979, 223979, 95991, 95991)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y =  create_training_data(features, labels, num_words, train_fraction=0.7)\n",
    "len(train_x), len(train_y), len(valid_x), len(valid_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223979, 50)\n",
      "(95991, 50)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(valid_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "# Download word embeddings if they are not present\n",
    "# !wget --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# unzip glove.6B.zip\n",
    "\n",
    "# Load in unzipped file\n",
    "glove_vectors = '../../embeddings/glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, encoding='utf-8', dtype='str', comments=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 100)\n",
      "(400000,)\n",
      "13751\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape)\n",
    "print(words.shape)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3026 words not found out of 13751 total words\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix for words that are part of our vocabulary, using GloVe embeddings\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "not_found = 0\n",
    "for i, word in enumerate(word2idx.keys()):\n",
    "    vector = word_lookup.get(word, None)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector    \n",
    "    else:\n",
    "        not_found += 1\n",
    "print(f'{not_found} words not found out of {num_words} total words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "del vectors\n",
    "# del glove\n",
    "del features\n",
    "del labels\n",
    "del glove_vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13751, 100)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: the\n",
      "\n",
      "the     Cosine similarity 1.0000\n",
      "this    Cosine similarity 0.8573\n",
      "part    Cosine similarity 0.8508\n",
      "one     Cosine similarity 0.8503\n",
      "of      Cosine similarity 0.8329\n",
      "same    Cosine similarity 0.8325\n",
      "first   Cosine similarity 0.8210\n",
      "on      Cosine similarity 0.8200\n",
      "its     Cosine similarity 0.8169\n",
      "as      Cosine similarity 0.8128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Query: ,\n",
      "\n",
      ",       Cosine similarity 1.0000\n",
      "and     Cosine similarity 0.8782\n",
      ".       Cosine similarity 0.8756\n",
      "while   Cosine similarity 0.8525\n",
      "but     Cosine similarity 0.8338\n",
      "as      Cosine similarity 0.8284\n",
      "also    Cosine similarity 0.8066\n",
      "now     Cosine similarity 0.8026\n",
      "well    Cosine similarity 0.8003\n",
      "one     Cosine similarity 0.7802\n"
     ]
    }
   ],
   "source": [
    "def find_closest(query, embedding_matrix=embedding_matrix, word2idx=word2idx, idx2word=idx2word, n=10):\n",
    "    \"\"\"Finds the closest word to a given word using word embeddings\"\"\"\n",
    "    idx = word2idx.get(query, None)\n",
    "    if idx is None:\n",
    "        print(f'{query} not found in vocab.')\n",
    "        return None\n",
    "    vector = embedding_matrix[idx]\n",
    "    if(np.all(vector == 0)):\n",
    "        print(f'{query} has no pre-trained embedding.')\n",
    "        return None\n",
    "    else:\n",
    "        dist = np.dot(embedding_matrix, vector)\n",
    "        idxs = np.argsort(dist)[::-1][:n]  \n",
    "        sorted_dist = dist[idxs]\n",
    "        closest = [idx2word[i] for i in idxs]\n",
    "\n",
    "    print(f'Query: {query}\\n')\n",
    "    max_len = max([len(i) for i in closest])\n",
    "    for word, dist in zip(closest, sorted_dist):\n",
    "        print(f'{word:{max_len + 2}} Cosine similarity {dist:.4f}')\n",
    "    \n",
    "find_closest('the')  \n",
    "print('-'*100)\n",
    "find_closest(',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13751\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "model_dir = '../models/'\n",
    "def create_callbacks(model_name, save=SAVE_MODEL):\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    callbacks = [earlyStopping]\n",
    "    if save:\n",
    "        callbacks.append(ModelCheckpoint(f'{model_dir}{model_name}.h5', save_best_only=True))          \n",
    "    return callbacks\n",
    "callbacks = create_callbacks('rnn-glove-embeddings')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras embedding layer.\n",
    "To represent words as a vector of numbers we have two options\n",
    "- One hot encoded vector where every word is represented as array of numbers. The size of the array will be equal to number of words in the vector. The number 1 is replaced in the place of the word, zeros are used for all the other words. This is not a feasible embedding approach as it demands large storage space for the word vectors and reduces model efficiency.\n",
    "- Word embeddings are used to represent every word using a fixed length vector. These vectors are dense than one-hot encoding. They helps us identify semantic similarities between any two word vectors. \n",
    "Since we are working on Word based language RNN, word embeddings are used here to convert input to word vector using pre-training word embeddings (gLove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "def create_model(lstms=1, lstm_cells=64):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_words, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False, mask_zero=True))\n",
    "    model.add(Masking(mask_value=0.0))\n",
    "    if lstms > 1:\n",
    "      for i in range(lstms-1):\n",
    "        model.add(LSTM(lstm_cells, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(LSTM(lstm_cells, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(lstms=1, lstm_cells=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    (None, None, 100)         1375100   \n",
      "                                                                 \n",
      " masking_14 (Masking)        (None, None, 100)         0         \n",
      "                                                                 \n",
      " lstm_16 (LSTM)              (None, 64)                42240     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 13751)             1773879   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,199,539\n",
      "Trainable params: 1,824,439\n",
      "Non-trainable params: 1,375,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "875/875 [==============================] - 166s 187ms/step - loss: 6.3434 - accuracy: 0.0907 - val_loss: 5.9984 - val_accuracy: 0.1223\n",
      "Epoch 2/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 5.9265 - accuracy: 0.1385 - val_loss: 5.7676 - val_accuracy: 0.1505\n",
      "Epoch 3/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 5.7637 - accuracy: 0.1509 - val_loss: 5.6603 - val_accuracy: 0.1626\n",
      "Epoch 4/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 5.6427 - accuracy: 0.1593 - val_loss: 5.5611 - val_accuracy: 0.1670\n",
      "Epoch 5/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 5.5383 - accuracy: 0.1642 - val_loss: 5.4797 - val_accuracy: 0.1720\n",
      "Epoch 6/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 5.4487 - accuracy: 0.1694 - val_loss: 5.4056 - val_accuracy: 0.1805\n",
      "Epoch 7/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 5.3619 - accuracy: 0.1761 - val_loss: 5.3445 - val_accuracy: 0.1877\n",
      "Epoch 8/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 5.2807 - accuracy: 0.1818 - val_loss: 5.2845 - val_accuracy: 0.1939\n",
      "Epoch 9/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 5.2040 - accuracy: 0.1866 - val_loss: 5.2339 - val_accuracy: 0.1991\n",
      "Epoch 10/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 5.1362 - accuracy: 0.1906 - val_loss: 5.2043 - val_accuracy: 0.2010\n",
      "Epoch 11/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 5.0758 - accuracy: 0.1933 - val_loss: 5.1718 - val_accuracy: 0.2050\n",
      "Epoch 12/30\n",
      "875/875 [==============================] - 162s 186ms/step - loss: 5.0168 - accuracy: 0.1963 - val_loss: 5.1422 - val_accuracy: 0.2087\n",
      "Epoch 13/30\n",
      "875/875 [==============================] - 163s 186ms/step - loss: 4.9634 - accuracy: 0.1973 - val_loss: 5.1194 - val_accuracy: 0.2099\n",
      "Epoch 14/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 4.9192 - accuracy: 0.1988 - val_loss: 5.0986 - val_accuracy: 0.2115\n",
      "Epoch 15/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 4.8704 - accuracy: 0.2011 - val_loss: 5.0738 - val_accuracy: 0.2130\n",
      "Epoch 16/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 4.8267 - accuracy: 0.2019 - val_loss: 5.0787 - val_accuracy: 0.2151\n",
      "Epoch 17/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 4.7874 - accuracy: 0.2031 - val_loss: 5.0506 - val_accuracy: 0.2164\n",
      "Epoch 18/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 4.7455 - accuracy: 0.2051 - val_loss: 5.0595 - val_accuracy: 0.2177\n",
      "Epoch 19/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 4.7112 - accuracy: 0.2052 - val_loss: 5.0369 - val_accuracy: 0.2196\n",
      "Epoch 20/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 4.6727 - accuracy: 0.2067 - val_loss: 5.0383 - val_accuracy: 0.2199\n",
      "Epoch 21/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 4.6411 - accuracy: 0.2077 - val_loss: 5.0340 - val_accuracy: 0.2219\n",
      "Epoch 22/30\n",
      "875/875 [==============================] - 162s 186ms/step - loss: 4.6080 - accuracy: 0.2086 - val_loss: 5.0408 - val_accuracy: 0.2235\n",
      "Epoch 23/30\n",
      "875/875 [==============================] - 162s 185ms/step - loss: 4.5760 - accuracy: 0.2109 - val_loss: 5.0268 - val_accuracy: 0.2251\n",
      "Epoch 24/30\n",
      "875/875 [==============================] - 161s 183ms/step - loss: 4.5473 - accuracy: 0.2104 - val_loss: 5.0400 - val_accuracy: 0.2262\n",
      "Epoch 25/30\n",
      "875/875 [==============================] - 161s 185ms/step - loss: 4.5156 - accuracy: 0.2125 - val_loss: 5.0508 - val_accuracy: 0.2267\n",
      "Epoch 26/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 4.4944 - accuracy: 0.2130 - val_loss: 5.0556 - val_accuracy: 0.2286\n",
      "Epoch 27/30\n",
      "875/875 [==============================] - 161s 184ms/step - loss: 4.4698 - accuracy: 0.2134 - val_loss: 5.0351 - val_accuracy: 0.2287\n",
      "Epoch 28/30\n",
      "875/875 [==============================] - 160s 183ms/step - loss: 4.4436 - accuracy: 0.2147 - val_loss: 5.0632 - val_accuracy: 0.2297\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "47/47 [==============================] - 4s 73ms/step - loss: 5.0268 - accuracy: 0.2251\n",
      "Cross-entropy: 5.0268\n",
      "Accuracy: 0.2251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f340da3aa60>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_evaluate_model(model_name):\n",
    "    model = load_model(f'{model_dir}{model_name}.h5')\n",
    "    r = model.evaluate(valid_x, valid_y, batch_size=2048, verbose=1)\n",
    "    print(f'Cross-entropy: {r[0]:.4f}')\n",
    "    print(f'Accuracy: {r[1]:.4f}')\n",
    "    return model\n",
    "load_and_evaluate_model('rnn-glove-embeddings')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation.\n",
    "In this step we assess if our model is performing better than random guess. \n",
    "A random guess strategy we consider here is to randomly replace the expected token with most frequently used word.\n",
    "With all tokens taken from most frequently used words, we calculate the accuracy of the validation set and compare it with the accuracy of the model.\n",
    "If the accuracy of the model is higher than random fit, we can conclude our model has learned something and it can perform better than random guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common word: the\n",
      "Accuracy of the model if we replace all words with the most common word: 8.7602%\n",
      "the        Word Count: 36597 \t Predicted 7129 \t Percentage 7.36%\n",
      "a          Word Count: 24878 \t Predicted 4649 \t Percentage 5.0%\n",
      "of         Word Count: 20193 \t Predicted 3856 \t Percentage 4.06%\n",
      ".          Word Count: 16594 \t Predicted 3121 \t Percentage 3.34%\n",
      ",          Word Count: 15410 \t Predicted 2866 \t Percentage 3.1%\n",
      "and        Word Count: 12947 \t Predicted 2448 \t Percentage 2.6%\n",
      "to         Word Count: 12073 \t Predicted 2295 \t Percentage 2.43%\n",
      "network    Word Count: 7731 \t Predicted 1549 \t Percentage 1.55%\n",
      "for        Word Count: 6907 \t Predicted 1429 \t Percentage 1.39%\n",
      "is         Word Count: 7213 \t Predicted 1417 \t Percentage 1.45%\n",
      "Accuracy: 1.53%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "total_words = sum(word_counts.values())\n",
    "frequencies = [word_counts[word]/total_words for word in word2idx.keys()]\n",
    "frequencies.insert(0, 0)\n",
    "print(f'The most common word: ' + idx2word[frequencies.index(max(frequencies))])\n",
    "print(f'Accuracy of the model if we replace all words with the most common word: {round(100 * np.mean(np.argmax(valid_y, axis = 1) == 1), 4)}%')\n",
    "\n",
    "# collect random guesses for every item in validation set\n",
    "# np.random.multinomial(1, frequencies, size=1) returns a one-hot encoded vector of size 1 with a 1 at the index of the randomly chosen word\n",
    "# frequencies is the probability distribution from which the words are chosen\n",
    "random_guesses = [np.argmax(np.random.multinomial(1, frequencies, size=1)) for i in valid_y]\n",
    "\n",
    "# create a counter with the counts of each word\n",
    "c = Counter(random_guesses)\n",
    "# for 10 most common words\n",
    "for i in c.most_common(10):\n",
    "     word = idx2word[i[0]]\n",
    "     word_count = word_counts[word]\n",
    "     print(f'{word:<10} Word Count: {word_count} \\t Predicted {i[1]} \\t Percentage {round(100*word_count/total_words, 2)}%')\n",
    "# accuracy of the model which predicts the most common word\n",
    "accuracy = np.mean(random_guesses == np.argmax(valid_y, axis=1))\n",
    "print(f'Accuracy: {round(100*accuracy, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_output(model, sequences, training_length=50, new_words=50, diversity=1, return_output=False):\n",
    "    \"\"\"Generates new text given a trained model and a seed sequence\"\"\"\n",
    "    \n",
    "    # pick a random sequence    \n",
    "    seq = random.choice(sequences)\n",
    "    \n",
    "    # pick a random starting index\n",
    "    seed_idx = random.randint(0, len(seq)-training_length-10)\n",
    "    \n",
    "    # select end index based on training length and seed\n",
    "    end_idx = seed_idx+training_length\n",
    "    \n",
    "    # seed sequence\n",
    "    seed = seq[seed_idx:end_idx]\n",
    "    \n",
    "    # actual entire sequence\n",
    "    original_sequence_words = [idx2word[i] for i in seed]\n",
    "    \n",
    "    # initializing the generated sequence\n",
    "    generated = seed[:] + ['#']\n",
    "        \n",
    "    # actual entire sequence\n",
    "    actual = generated + seq[end_idx: end_idx+new_words]\n",
    "      \n",
    "    for i in range(new_words):\n",
    "        preds = model.predict(np.array(seed).reshape(1, -1), verbose=0)[0].astype('float64')\n",
    "        preds = np.log(preds)/diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "        \n",
    "        # reweight distribution => softmax\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)[0]\n",
    "        \n",
    "        # find the next word index\n",
    "        next_idx = np.argmax(probas)\n",
    "        \n",
    "        # reseed the seed with the new word\n",
    "        seed = seed[1:] + [next_idx]\n",
    "        \n",
    "        # update generated text\n",
    "        generated.append(next_idx)\n",
    "        \n",
    "    gen_list = []\n",
    "    for i in generated:\n",
    "      gen_list.append(idx2word.get(i, '<--->'))\n",
    "    \n",
    "    a = []\n",
    "    for i in actual:\n",
    "      a.append(idx2word.get(i, '<--->'))\n",
    "    \n",
    "    return original_sequence_words, gen_list, a\n",
    "\n",
    "seed, gen_list, actual = generate_output(model, new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: are determined using occupant feedback provided by individual occupants over at least one of an internet or intranet communications network . according to a first aspect of the invention a setpoint is determined using fuzzy logic . according to a second aspect , historical setpoint data determined using occupant feedback\n",
      "====================================================================================================\n",
      "ACTUAL:are determined using occupant feedback provided by individual occupants over at least one of an internet or intranet communications network . according to a first aspect of the invention a setpoint is determined using fuzzy logic . according to a second aspect , historical setpoint data determined using occupant feedback <---> is used to develop a neural network for predicting setpoint values .\n",
      "====================================================================================================\n",
      "GENERATED:are determined using occupant feedback provided by individual occupants over at least one of an internet or intranet communications network . according to a first aspect of the invention a setpoint is determined using fuzzy logic . according to a second aspect , historical setpoint data determined using occupant feedback <---> at the control machine input desired signals with the actual mapping model at a visual engine , the frame electric algorithms . an learning controller can adjust the high balance states in each character between the system model may be calculated in function . the programming means in one featureless\n"
     ]
    }
   ],
   "source": [
    "print('SEED: ' + ' '.join(seed))\n",
    "print('='*100)\n",
    "print('ACTUAL:' +' '.join(actual))\n",
    "print('='*100)\n",
    "print('GENERATED:' +' '.join(gen_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
