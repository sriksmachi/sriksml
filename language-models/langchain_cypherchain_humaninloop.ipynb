{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install langchain langchain_community\n",
    "# %pip install neo4j\n",
    "# %pip install langchain_openai\n",
    "# %pip install python-dotenv\n",
    "# %pip install chromadb\n",
    "# %pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Human in Loop in Graph RAG\n",
    "\n",
    "Generally LLMs are good at cypher generation, when best practices like sharing examples, schema are followed. However, hallucinations / incorrect queries are common in complex graphs. GraphQACypherChain is commonly used for graph RAG. This example explains how to dissect Graph RAG to ask user for guidance on generating better query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEO4J_URI'] = 'bolt://localhost:7690'\n",
    "os.environ['NEO4J_USERNAME'] = 'neo4j'\n",
    "os.environ['NEO4J_PASSWORD'] = 'Password@123'\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://sriks-openai.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-05-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"] = \"gpt-4o\"\n",
    "os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"] = \"text-embedding-ada-002\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"],\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What is the root cause request id 8ff8696695aa73588ac454809741e2ea\",\n",
    "        \"query\": \"MATCH (n)-[r:DEPENDS_ON]->(m) where n.id <> 'ROOT' OR m.id <> 'ROOT' and r.operationId = 8ff8696695aa73588ac454809741e2ea RETURN n.id, r.duration, r.operation_Name, m.id ORDER BY r.duration DESC LIMIT 3\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is the longest running operation\",\n",
    "        \"query\": \"MATCH (n)-[r:DEPENDS_ON]->(m) where n.id <> 'ROOT' OR m.id <> 'ROOT' RETURN n.id, r.duration, r.operation_Name, m.id ORDER BY r.duration DESC LIMIT 3\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector, MaxMarginalRelevanceExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.graph_qa.cypher import construct_schema\n",
    "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# load human intervention tools\n",
    "tools = load_tools(\n",
    "    [\"human\"], llm\n",
    ")\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "\n",
    "EXAMPLES_PROMPT_TEMPLATE = \"\"\"   \n",
    "                Input: {question},\n",
    "                Output: {query}\n",
    "            \"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"query\"], template=EXAMPLES_PROMPT_TEMPLATE)\n",
    "\n",
    "cypherPromptTemplate = \"\"\"\n",
    "You are an expert Neo4j Developer translating user questions into Cypher to answer questions.\n",
    "Convert the user's question based on the schema.\n",
    "Instructions: Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "\n",
    "Important: In the generated Cypher query, the RETURN statement must explicitly include the property values used in the query's filtering condition, alongside the main information requested from the original question.\n",
    "\n",
    "Question: {question}\n",
    "input:\n",
    "\"\"\"\n",
    "\n",
    "similaritySelector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples, \n",
    "    embeddings=embeddings, \n",
    "    k=1,\n",
    "    vectorstore_cls=Chroma\n",
    ")\n",
    "\n",
    "cypherPrompt = FewShotPromptTemplate(\n",
    "    example_selector=similaritySelector,\n",
    "    example_prompt=example_prompt,\n",
    "    input_variables=[\"question\", \"schema\"], \n",
    "    prefix=cypherPromptTemplate,\n",
    "    suffix=\"the question is:{question}\",\n",
    ")\n",
    "\n",
    "cypherqachain = GraphCypherQAChain.from_llm(\n",
    "    llm = llm,\n",
    "    return_intermediate_steps=True,\n",
    "    validate_cypher=True,\n",
    "    graph=graph, \n",
    "    verbose=True,\n",
    "    k=3,\n",
    "    use_function_response=True,\n",
    "    cypherPrompt=cypherPrompt,\n",
    "    return_direct=True,\n",
    "    output_key=\"input\",\n",
    ") \n",
    "\n",
    "schema  = construct_schema(graph.get_structured_schema, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (crn:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(sql:SQL)\n",
      "RETURN crn.id, COUNT(r) AS azureDependencyCount\n",
      "ORDER BY azureDependencyCount DESC\n",
      "LIMIT 1\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'which nodes has max azure dependencies',\n",
       " 'input': [{'crn.id': 'people-api', 'azureDependencyCount': 23}],\n",
       " 'intermediate_steps': [{'query': 'cypher\\nMATCH (crn:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(sql:SQL)\\nRETURN crn.id, COUNT(r) AS azureDependencyCount\\nORDER BY azureDependencyCount DESC\\nLIMIT 1\\n'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test cypher chain invocation\n",
    "cypherqachain.invoke({\"query\": \"which nodes has max azure dependencies\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code, creates a confidence chain that will be used to ask LLM how confident it is query generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_prompt_template = \"\"\"Given the {input}, {schema} and the question from user question:{question} how confident are you in the query on scale of 1-5, answer precisely in JSON format\n",
    "score: 1, reason: The query is not clear, assumptions: The query is not clear\"\"\"\n",
    "\n",
    "confidence_prompt = PromptTemplate(template=confidence_prompt_template, input_variables=[\"input\", \"schema\"])\n",
    "\n",
    "# ask llm on confidence of the query\n",
    "confidenceChain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=confidence_prompt\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_interaction_prompt = \"\"\"How do you want to proceed?\"\"\"\n",
    "agent = initialize_agent(\n",
    "    tools=tools, \n",
    "    llm=llm, \n",
    "    agent=\"zero-shot-react-description\", \n",
    "    verbose=True, \n",
    "    handle_parsing_errors=True,\n",
    "    prompt=human_interaction_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QA_GENERATION_TEMPLATE = \"\"\"\n",
    "       Task: answer the question you are given based on the context provided.\n",
    "       Instructions:\n",
    "        You are an assistant that helps to form nice and human understandable answers. \n",
    "        Use the context information provided to generate a well organized and comprehensve answer to the user's question. \n",
    "        When the provided information contains multiple elements, structure your answer as a bulleted or numbered list to enhance clarity and readability.\n",
    "        You must use the information to construct your answer. \n",
    "        The provided information is authoritative; do not doubt it or try to use your internal knowledge to correct it. \n",
    "        Make the answer sound like a response to the question without mentioning that you based the result on the given information. \n",
    "        If there is no information provided, say that the knowledge base returned empty results.\n",
    "\n",
    "        Here's the information:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "        \n",
    "        Also explain the assumptions you made while generating the answer given the query used to generate the context and the schema provided.\n",
    "        query: {input}, schema: {schema}\n",
    "            \"\"\"\n",
    "       \n",
    "            \n",
    "qaPrompt = PromptTemplate(input_variables=[\"context\", \"question\", \"input\", \"schema\"], template=QA_GENERATION_TEMPLATE)\n",
    "\n",
    "qaPromptChain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=qaPrompt\n",
    ")   \n",
    "\n",
    "# function is used to extract the query from the message, schema is a global variable\n",
    "def queryExtractor(msg: AIMessage):\n",
    "    # print(msg)\n",
    "    query = msg[\"intermediate_steps\"][0][\"query\"]\n",
    "    question = msg[\"query\"]\n",
    "    msg = {\"input\": query, \"schema\": schema, \"question\": question}\n",
    "    return msg\n",
    "\n",
    "# function is used to execute the cypher query\n",
    "def executeCypher(msg: AIMessage):\n",
    "    # print(msg)\n",
    "    query = msg[\"input\"]\n",
    "    query = query.replace(\"cypher\", \"\")\n",
    "    result = graph.query(query)\n",
    "    # result = pd.DataFrame(result)    \n",
    "    return { \"context\" : result , \"question\": msg[\"question\"], \"input\": msg[\"input\"], \"schema\": msg[\"schema\"]}\n",
    "\n",
    "final_chain = cypherqachain | queryExtractor | confidenceChain | agent | executeCypher | qaPromptChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below execution waits for user to confirm if the cypher query generated is matching user's expectations. User can suggest improvements using natural language to generate a better query, once the user confirms the query is run on the Neo4j instance and final response is generated. \n",
    "\n",
    "[Pending] : Ask for user affirmation only if LLM is not confident. LLM generates a confidence score based on the query, schema shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (c:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(s:SQL)\n",
      "RETURN c.id, COUNT(r) as dependencyCount\n",
      "ORDER BY dependencyCount DESC\n",
      "LIMIT 1\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe question appears to involve a Cypher query for a graph database, specifically querying for a `cloudRoleName` node that has dependencies on `SQL` nodes. The request is to return the ID of the `cloudRoleName` node with the highest number of `AZURE_DEPENDS_ON` relationships, ordered by the count of these relationships in descending order, and limited to one result.\n",
      "\n",
      "The Cypher query provided seems mostly correct, but to ensure clarity and correctness, let's break down the query:\n",
      "\n",
      "1. `MATCH (c:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(s:SQL)`: This matches all `cloudRoleName` nodes that have `AZURE_DEPENDS_ON` relationships to `SQL` nodes.\n",
      "2. `RETURN c.id, COUNT(r) as dependencyCount`: This returns the ID of the `cloudRoleName` node and the count of `AZURE_DEPENDS_ON` relationships.\n",
      "3. `ORDER BY dependencyCount DESC`: This orders the results by the count of dependencies in descending order.\n",
      "4. `LIMIT 1`: This limits the result to one entry.\n",
      "\n",
      "The query appears to be correct. However, since I am tasked with answering as best as I can and if there is any uncertainty or additional context needed, I would ask for clarification.\n",
      "\n",
      "Action: human\n",
      "Action Input: Is the provided Cypher query correct for finding the `cloudRoleName` node with the most `AZURE_DEPENDS_ON` relationships to `SQL` nodes?\u001b[0m\n",
      "\n",
      "Is the provided Cypher query correct for finding the `cloudRoleName` node with the most `AZURE_DEPENDS_ON` relationships to `SQL` nodes?\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mdo not limit by 1 get all matches\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe observation suggests that we should not limit the results to just one match but rather get all matches ordered by the dependency count. Therefore, we should remove the `LIMIT 1` clause from the query.\n",
      "\n",
      "Here's the revised Cypher query:\n",
      "\n",
      "```cypher\n",
      "MATCH (c:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(s:SQL)\n",
      "RETURN c.id, COUNT(r) as dependencyCount\n",
      "ORDER BY dependencyCount DESC\n",
      "```\n",
      "\n",
      "This query will return all `cloudRoleName` nodes ordered by the number of `AZURE_DEPENDS_ON` relationships to `SQL` nodes, in descending order of the count.\n",
      "\n",
      "Final Answer: The revised Cypher query is:\n",
      "```cypher\n",
      "MATCH (c:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(s:SQL)\n",
      "RETURN c.id, COUNT(r) as dependencyCount\n",
      "ORDER BY dependencyCount DESC\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'cypher\\nMATCH (c:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(s:SQL)\\nRETURN c.id, COUNT(r) as dependencyCount\\nORDER BY dependencyCount DESC\\nLIMIT 1\\n', 'schema': 'Node properties are the following:\\ncloudRoleName {id: STRING},SQL {id: STRING}\\nRelationship properties are the following:\\nDEPENDS_ON {counter: INTEGER, operation_Name: STRING, duration: FLOAT, itemType: STRING, operationId: STRING, type: STRING, id: STRING},INPROC_TT {operation_Name: STRING, duration: FLOAT, itemType: STRING, id: STRING, counter: INTEGER, operationId: STRING, type: STRING},AZURE_DEPENDS_ON {itemType: STRING, operation_Name: STRING, id: STRING, operationId: STRING, type: STRING, duration: FLOAT, counter: INTEGER}\\nThe relationships are the following:\\n(:cloudRoleName)-[:DEPENDS_ON]->(:cloudRoleName),(:cloudRoleName)-[:INPROC_TT]->(:cloudRoleName),(:cloudRoleName)-[:AZURE_DEPENDS_ON]->(:SQL)', 'question': 'which nodes has max sql dependencies', 'text': '```json\\n{\\n  \"score\": 1,\\n  \"reason\": \"The query is not correctly formed. It is not clear what is meant by \\'max sql dependencies\\'. The relationships and properties provided do not fully match the query structure, and the query seems to assume a specific structure that is not clearly defined in the provided context. The provided node and relationship properties are also not fully utilized in the query, leading to potential inaccuracies. Adjustments or further clarification is needed for a more accurate query.\"\\n}\\n```', 'output': 'The revised Cypher query is:\\n```cypher\\nMATCH (c:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(s:SQL)\\nRETURN c.id, COUNT(r) as dependencyCount\\nORDER BY dependencyCount DESC\\n```'}\n",
      "############ Final Answer ############\n",
      "The node with the maximum SQL dependencies is:\n",
      "\n",
      "- **people-api** with **23 dependencies**.\n",
      "\n",
      "### Assumptions\n",
      "1. The query provided was used to find the node with the maximum dependencies on SQL. The query specifically matches nodes of type `cloudRoleName` that have a relationship of type `AZURE_DEPENDS_ON` with nodes of type `SQL`.\n",
      "2. The `COUNT(r)` function counts the number of `AZURE_DEPENDS_ON` relationships each `cloudRoleName` node has with `SQL` nodes, and the results are ordered in descending order of dependency count.\n",
      "3. The schema indicates that the `cloudRoleName` and `SQL` nodes are identified by the `id` property, and the relationships have various properties, including a `counter` which likely tracks the number of dependencies.\n",
      "\n",
      "Given these assumptions and the query results, the node `people-api` has the highest number of SQL dependencies, totaling 23.\n"
     ]
    }
   ],
   "source": [
    "result = final_chain.invoke({\"query\": \"which nodes has max sql dependencies\"})\n",
    "print(\"############ Final Answer ############\")\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the conversation `Observation: do not limit by 1 get all matches` and its influence on final query generation. <br/>\n",
    "The final answer begins from ########### Final Answer ###########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
