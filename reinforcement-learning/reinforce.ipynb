{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriksmachi/sriksml/blob/main/reinforcement-learning/reinforce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_WZsNmg79Xw"
      },
      "source": [
        "# Reinforce Algorithm\n",
        "\n",
        "## Definition\n",
        "\n",
        "Reinforcement learning problems can be broadly classified into policy based and value based methods. In value based methods, the agent learns value of an state or state-action pair at every point. The agent then uses those values to maximize the expected return. In a way, this is deterministic approach because after the values are learnt, there is no randomness exhibited by the agent to take actions.\n",
        "In contrast, in policy based methods the agent learns the probability distribution of actions that can be taken in a given time step, it then can stochasticaly choose action from the given distribution.\n",
        "\n",
        "Reinforce is a easy and simple to understand RL algorithm that belongs to policy gradient class of algorithms. Policy gradient methods attempt to learn the optimal policy, that is the probability distribution of action that can be taken in every state. Compared to deterministic algorithms like DQL, stochastic methods like reinforce have an advantage of avoiding abrupt jumps in actions taken for small gradient updates. Since the model is dealing with probabilities of the actions, for smaller gradients the impact on probabilities is also small.\n",
        "\n",
        "## Intuition\n",
        "\n",
        "The fundamental idea in reinforce algorithm is to optimize the expected reward,  if we gather enough trajectories and compute the expected reward for each trajectory. We can train a model such that reinforces the good actions so that good actions lead to higher returns. Similarly, it tends to push down the probabilities of bad actions. Eventually the model learns only the good actions leading to learning the optimal policy.\n",
        "\n",
        "## Training process\n",
        "The agent goes through following steps\n",
        "- The model is initialized with initial random weights that acts as an initial policy.\n",
        "- The agent acts on the environment and collects - next state, reward, action.\n",
        "- The agents continuous to take action on the environment until it reaches the end state.\n",
        "- The next state, reward and the action taken are used to calculate the rewards obtained per episode/trajectory.\n",
        "- The model weights are updated using gradient ascent.\n",
        "- These steps are repeated until the model converges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ZDLOuA7mzm"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EByxNSC-9pq"
      },
      "outputs": [],
      "source": [
        "# hide output\n",
        "!pip install renderlab > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIY_tX9J-t5A"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import renderlab as rl\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = rl.RenderFrame(env, \"./output\")\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5gtMwpZ79X4",
        "outputId": "3fb11c0b-86f4-46cc-d445-24cbef665820"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow_probability as tfp\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t5fPD6eNlGk",
        "outputId": "13730d83-7e6c-4db3-cc92-94ea7f6ebe9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "terminal = False\n",
        "rewards = []\n",
        "while not terminal:\n",
        "  action = env.action_space.sample()\n",
        "  observation, reward, terminal, info = env.step(action)\n",
        "  rewards.append(reward)\n",
        "print(len(rewards))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiQ_vWcN79X7",
        "outputId": "579dfbb5-4905-412c-f3cc-237699ceff24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of action spaces:  2\n"
          ]
        }
      ],
      "source": [
        "low = env.observation_space.low\n",
        "high = env.observation_space.high\n",
        "print(f'Number of action spaces:  {env.action_space.n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm2bKelJ79X8"
      },
      "outputs": [],
      "source": [
        "class model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(64,activation='relu')\n",
        "    self.d2 = tf.keras.layers.Dense(64,activation='relu')\n",
        "    self.out = tf.keras.layers.Dense(env.action_space.n,activation='softmax')\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = tf.convert_to_tensor(input_data)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    x = self.out(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWNyJQkn79X9"
      },
      "outputs": [],
      "source": [
        "class agent():\n",
        "  def __init__(self, learning_rate=0.001, gamma=0.99):\n",
        "    self.model = model()\n",
        "    self.opt = tf.keras.optimizers.Adam(learning_rate)\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def act(self,state):\n",
        "    prob = self.model(np.array([state]))\n",
        "    dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "    action = dist.sample()\n",
        "    return int(action.numpy()[0])\n",
        "\n",
        "  def a_loss(self,prob, action, reward):\n",
        "    dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "    log_prob = dist.log_prob(action)\n",
        "    loss = -log_prob*reward\n",
        "    return loss\n",
        "\n",
        "  def train(self, states, rewards, actions):\n",
        "    sum_reward = 0\n",
        "    discnt_rewards = []\n",
        "    rewards.reverse()\n",
        "    for r in rewards:\n",
        "      sum_reward = r + self.gamma * sum_reward\n",
        "      discnt_rewards.append(sum_reward)\n",
        "    discnt_rewards.reverse()\n",
        "\n",
        "    for state, reward, action in zip(states, discnt_rewards, actions):\n",
        "      with tf.GradientTape() as tape:\n",
        "        # forward pass\n",
        "        p = self.model(np.array([state]), training=True)\n",
        "        # compute loss\n",
        "        loss = self.a_loss(p, action, reward)\n",
        "      # compute gradients\n",
        "      grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "      # update weights\n",
        "      self.opt.apply_gradients(zip(grads, self.model.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VseEnGjo79X-",
        "outputId": "523767d5-3ef3-4447-de72-680d6fa732e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 101/2000 [02:51<20:36,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory 100\tAverage Score: 15.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 201/2000 [07:38<1:42:44,  3.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory 200\tAverage Score: 189.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 301/2000 [12:51<1:32:56,  3.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory 300\tAverage Score: 177.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 401/2000 [16:28<50:36,  1.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory 400\tAverage Score: 154.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 501/2000 [20:21<1:23:05,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory 500\tAverage Score: 196.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 601/2000 [24:15<28:57,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory 600\tAverage Score: 10.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 701/2000 [26:30<24:40,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory 700\tAverage Score: 17.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 794/2000 [29:59<43:09,  2.15s/it]"
          ]
        }
      ],
      "source": [
        "# params\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "\n",
        "cartpole = agent(learning_rate, gamma)\n",
        "max_trajectories = 2000\n",
        "rewards_tracked = []\n",
        "for trajectory in tqdm(range(max_trajectories)):\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "  rewards = []\n",
        "  states = []\n",
        "  actions = []\n",
        "  while not done:\n",
        "    action = cartpole.act(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "      cartpole.train(states, rewards, actions)\n",
        "      if trajectory % 100 == 0 and trajectory > 0:\n",
        "        print('Trajectory {}\\tAverage Score: {:.2f}'.format(trajectory, total_reward))\n",
        "      rewards_tracked.append(total_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcXM8FiR79X_"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYsEjtXh79YA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-KdWIMI79YB"
      },
      "outputs": [],
      "source": [
        "x_axis = np.asarray(range(0, max_trajectories))\n",
        "y_axis = np.asarray(rewards_tracked)\n",
        "plt.figure(0, figsize=(16,4))\n",
        "plt.title('Rewards per episode')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Rewards')\n",
        "plt.plot(x_axis,y_axis,'green')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwiJuiomTEhQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
