{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.5.0-cp311-cp311-win_amd64.whl (10.5 MB)\n",
      "                                              0.0/10.5 MB ? eta -:--:--\n",
      "                                              0.1/10.5 MB 1.1 MB/s eta 0:00:10\n",
      "                                              0.2/10.5 MB 1.8 MB/s eta 0:00:06\n",
      "     -                                        0.4/10.5 MB 2.8 MB/s eta 0:00:04\n",
      "     ---                                      0.9/10.5 MB 4.5 MB/s eta 0:00:03\n",
      "     ------                                   1.8/10.5 MB 7.5 MB/s eta 0:00:02\n",
      "     -------                                  2.1/10.5 MB 8.3 MB/s eta 0:00:02\n",
      "     ---------                                2.5/10.5 MB 7.5 MB/s eta 0:00:02\n",
      "     -----------------                        4.6/10.5 MB 11.4 MB/s eta 0:00:01\n",
      "     -----------------------                  6.3/10.5 MB 13.8 MB/s eta 0:00:01\n",
      "     ------------------------------           8.1/10.5 MB 16.2 MB/s eta 0:00:01\n",
      "     -------------------------------------   10.0/10.5 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.5/10.5 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.5/10.5 MB 21.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install gym==0.22\n",
    "# %pip install tensorflow_probability\n",
    "# %pip install tensorflow\n",
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.losses as kls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make(\"CartPole-v1\")\n",
    "low = env.observation_space.low\n",
    "high = env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    self.v = tf.keras.layers.Dense(1, activation = None)\n",
    "\n",
    "  def call(self, input_data):\n",
    "    x = self.d1(input_data)\n",
    "    v = self.v(x)\n",
    "    return v\n",
    "  \n",
    "class actor(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    self.a = tf.keras.layers.Dense(2,activation='softmax')\n",
    "\n",
    "  def call(self, input_data):\n",
    "    x = self.d1(input_data)\n",
    "    a = self.a(x)\n",
    "    return a\n",
    "  \n",
    "class agent():\n",
    "    def __init__(self):\n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
    "        self.c_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
    "        self.actor = actor()\n",
    "        self.critic = critic()\n",
    "        self.clip_pram = 0.2\n",
    "\n",
    "    # agent's action function\n",
    "    def act(self,state):\n",
    "        # convert state into tensor\n",
    "        prob = self.actor(np.array([state]))\n",
    "        prob = prob.numpy()\n",
    "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "        action = dist.sample()\n",
    "        return int(action.numpy()[0])\n",
    "    \n",
    "    def actor_loss(self, probs, actions, adv, old_probs, closs):\n",
    "        probability = probs      \n",
    "        entropy = tf.reduce_mean(tf.math.negative(tf.math.multiply(probability,tf.math.log(probability))))\n",
    "        #print(probability)\n",
    "        #print(entropy)\n",
    "        sur1 = []\n",
    "        sur2 = []\n",
    "        for pb, t, op,a  in zip(probability, adv, old_probs, actions):\n",
    "                        t =  tf.constant(t)\n",
    "                        #op =  tf.constant(op)\n",
    "                        #print(f\"t{t}\")\n",
    "                        #ratio = tf.math.exp(tf.math.log(pb + 1e-10) - tf.math.log(op + 1e-10))\n",
    "                        ratio = tf.math.divide(pb[a],op[a])\n",
    "                        #print(f\"ratio{ratio}\")\n",
    "                        s1 = tf.math.multiply(ratio,t)\n",
    "                        #print(f\"s1{s1}\")\n",
    "                        s2 =  tf.math.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram),t)\n",
    "                        #print(f\"s2{s2}\")\n",
    "                        sur1.append(s1)\n",
    "                        sur2.append(s2)\n",
    "\n",
    "        sr1 = tf.stack(sur1)\n",
    "        sr2 = tf.stack(sur2)\n",
    "        #closs = tf.reduce_mean(tf.math.square(td))\n",
    "        loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - closs + 0.001 * entropy)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "      \n",
    "    def learn(self, states, actions,  adv , old_probs, discnt_rewards):\n",
    "        discnt_rewards = tf.reshape(discnt_rewards, (len(discnt_rewards),))\n",
    "        adv = tf.reshape(adv, (len(adv),))\n",
    "        old_p = old_probs\n",
    "        old_p = tf.reshape(old_p, (len(old_p),2))\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            p = self.actor(states, training=True)\n",
    "            v =  self.critic(states,training=True)\n",
    "            v = tf.reshape(v, (len(v),))\n",
    "            td = tf.math.subtract(discnt_rewards, v)\n",
    "            c_loss = 0.5 * kls.mean_squared_error(discnt_rewards, v)\n",
    "            a_loss = self.actor_loss(p, actions, adv, old_probs, c_loss)\n",
    "            \n",
    "        grads1 = tape1.gradient(a_loss, self.actor.trainable_variables)\n",
    "        grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n",
    "        self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
    "        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
    "        return a_loss, c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward(env):\n",
    "  total_reward = 0\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  while not done:\n",
    "    action = np.argmax(agentoo7.actor(np.array([state])).numpy())\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "\n",
    "  return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new episode\n",
      "total test reward is 27.8\n",
      "best reward=27.8\n",
      "INFO:tensorflow:Assets written to: model_actor_0_27.8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_actor_0_27.8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_critic_0_27.8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_critic_0_27.8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new episode\n",
      "total test reward is 26.4\n",
      "new episode\n",
      "total test reward is 180.6\n",
      "best reward=180.6\n",
      "INFO:tensorflow:Assets written to: model_actor_2_180.6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_actor_2_180.6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_critic_2_180.6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_critic_2_180.6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new episode\n",
      "total test reward is 345.8\n",
      "best reward=345.8\n",
      "INFO:tensorflow:Assets written to: model_actor_3_345.8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_actor_3_345.8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_critic_3_345.8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_critic_3_345.8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new episode\n",
      "total test reward is 500.0\n",
      "best reward=500.0\n",
      "INFO:tensorflow:Assets written to: model_actor_4_500.0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_actor_4_500.0\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_critic_4_500.0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_critic_4_500.0\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new episode\n",
      "total test reward is 62.2\n",
      "new episode\n",
      "total test reward is 45.0\n",
      "new episode\n",
      "total test reward is 45.2\n",
      "new episode\n",
      "total test reward is 210.2\n",
      "new episode\n",
      "total test reward is 194.8\n",
      "new episode\n",
      "total test reward is 155.6\n",
      "new episode\n",
      "total test reward is 157.4\n",
      "new episode\n",
      "total test reward is 283.4\n",
      "new episode\n",
      "total test reward is 500.0\n",
      "new episode\n",
      "total test reward is 240.6\n",
      "new episode\n",
      "total test reward is 196.0\n",
      "new episode\n",
      "total test reward is 113.8\n",
      "new episode\n",
      "total test reward is 123.0\n",
      "new episode\n",
      "total test reward is 129.8\n",
      "new episode\n",
      "total test reward is 97.6\n",
      "new episode\n",
      "total test reward is 108.0\n",
      "new episode\n",
      "total test reward is 83.6\n",
      "new episode\n",
      "total test reward is 125.2\n",
      "new episode\n",
      "total test reward is 107.0\n",
      "new episode\n",
      "total test reward is 112.6\n",
      "new episode\n",
      "total test reward is 119.8\n",
      "new episode\n",
      "total test reward is 158.8\n",
      "new episode\n",
      "total test reward is 214.6\n",
      "new episode\n",
      "total test reward is 248.6\n",
      "new episode\n",
      "total test reward is 241.4\n",
      "new episode\n",
      "total test reward is 210.2\n",
      "new episode\n",
      "total test reward is 500.0\n",
      "new episode\n",
      "total test reward is 500.0\n",
      "new episode\n",
      "total test reward is 492.0\n",
      "new episode\n",
      "total test reward is 436.2\n",
      "new episode\n",
      "total test reward is 500.0\n",
      "new episode\n",
      "total test reward is 470.0\n",
      "new episode\n",
      "total test reward is 217.0\n",
      "new episode\n",
      "total test reward is 134.0\n",
      "new episode\n",
      "total test reward is 106.2\n",
      "new episode\n",
      "total test reward is 93.4\n",
      "new episode\n",
      "total test reward is 500.0\n",
      "new episode\n",
      "total test reward is 9.4\n",
      "new episode\n",
      "total test reward is 9.4\n",
      "new episode\n",
      "total test reward is 9.0\n",
      "new episode\n",
      "total test reward is 9.2\n",
      "new episode\n",
      "total test reward is 118.6\n",
      "new episode\n",
      "total test reward is 154.8\n",
      "new episode\n",
      "total test reward is 233.0\n",
      "new episode\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def preprocess1(states, actions, rewards, done, values, gamma):\n",
    "    g = 0\n",
    "    lmbda = 0.95\n",
    "    returns = []\n",
    "    for i in reversed(range(len(rewards))):\n",
    "       delta = rewards[i] + gamma * values[i + 1] * done[i] - values[i]\n",
    "       g = delta + gamma * lmbda * dones[i] * g\n",
    "       returns.append(g + values[i])\n",
    "    returns.reverse()\n",
    "    adv = np.array(returns, dtype=np.float32) - values[:-1]\n",
    "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    returns = np.array(returns, dtype=np.float32)\n",
    "    return states, actions, returns, adv    \n",
    "\n",
    "\n",
    "tf.random.set_seed(336699)\n",
    "agentoo7 = agent()\n",
    "steps = 5000\n",
    "ep_reward = []\n",
    "total_avgr = []\n",
    "target = False \n",
    "best_reward = 0\n",
    "avg_rewards_list = []\n",
    "\n",
    "# stepping through\n",
    "for s in range(steps):\n",
    "  if target == True:\n",
    "      break\n",
    "  \n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  all_aloss = []\n",
    "  all_closs = []\n",
    "  rewards = []\n",
    "  states = []\n",
    "  actions = []\n",
    "  probs = []\n",
    "  dones = []\n",
    "  values = []\n",
    "  print(\"new episode\")\n",
    "\n",
    "  for e in range(128):\n",
    "    action = agentoo7.act(state)\n",
    "    value = agentoo7.critic(np.array([state])).numpy()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    dones.append(1-done)\n",
    "    rewards.append(reward)\n",
    "    states.append(state)\n",
    "    #actions.append(tf.one_hot(action, 2, dtype=tf.int32).numpy().tolist())\n",
    "    actions.append(action)\n",
    "    prob = agentoo7.actor(np.array([state]))\n",
    "    probs.append(prob[0])\n",
    "    values.append(value[0][0])\n",
    "    state = next_state\n",
    "    if done:\n",
    "      env.reset()\n",
    "  \n",
    "  value = agentoo7.critic(np.array([state])).numpy()\n",
    "  values.append(value[0][0])\n",
    "  np.reshape(probs, (len(probs),2))\n",
    "  probs = np.stack(probs, axis=0)\n",
    "  states, actions,returns, adv  = preprocess1(states, actions, rewards, dones, values, 1)\n",
    "  for epocs in range(10):\n",
    "      al,cl = agentoo7.learn(states, actions, adv, probs, returns)\n",
    "      # print(f\"al{al}\") \n",
    "      # print(f\"cl{cl}\")   \n",
    "\n",
    "  avg_reward = np.mean([test_reward(env) for _ in range(5)])\n",
    "  print(f\"total test reward is {avg_reward}\")\n",
    "  avg_rewards_list.append(avg_reward)\n",
    "  if avg_reward > best_reward:\n",
    "        print('best reward=' + str(avg_reward))\n",
    "        agentoo7.actor.save('model_actor_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "        agentoo7.critic.save('model_critic_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "        best_reward = avg_reward\n",
    "  if best_reward == 200:\n",
    "        target = True\n",
    "  env.reset()\n",
    "\n",
    "env.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
