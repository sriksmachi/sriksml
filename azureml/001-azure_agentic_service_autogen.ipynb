{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent orchestrator using Azure AI Agentic Service\n",
    "\n",
    "**Problem Statement**: AI agents are autonomous software entities designed to perform tasks, make decisions, and interact with environments using artificial intelligence, machine learning, natural language processing, and reinforcement learning. However, every custom AI agent written today needs lifecycle management, that includes packaging as containers, deployment, scaling, allocation of right resources etc. As the number of agents grow in your ecosystem it becomes tedious to manage the environment.\n",
    "\n",
    "**Azure AI Agent Service** is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, and extensible AI agents without needing to manage the underlying compute and storage resources. We can use Azure AI Agent Service to create and run an agent in just a few lines of code. You can also manage complex workflows with **AutoGen** and **Semantic Kernel**.\n",
    "\n",
    "![AzureAIAgentService](../images/azure-agentic-service.png)\n",
    "\n",
    "In this notebook, I will show the capabilities of **Azure AI Agent Service** along with add-ons like Bing Service, Tools. At the end of this tutorial we will summarize a research paper into LinkedIn post using Azure Agentic Service, Bing & LLMs.\n",
    "\n",
    "**Pre-requisites**\n",
    "- Azure Subscription\n",
    "- Azure Open AI\n",
    "- Azure AI Foundry Project ([Link](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/create-projects?tabs=ai-studio))\n",
    "\n",
    "**ENV Config**\n",
    "- `PROJECT_CONNECTION_STRING`: Connection string for the Azure AI project.\n",
    "- `SEARCH_KEY`: Key for the Azure AI search service.\n",
    "- `SEARCH_ENDPOINT`: Endpoint for the Azure AI search service.\n",
    "- `AOAI_ENDPOINT`: Endpoint for the Azure OpenAI service.\n",
    "- `AOAI_KEY`: Key for the Azure OpenAI service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Project PIP requirements\n",
    "# %pip install -q azure-ai-projects azure-identity azure-ai-ml azure-search-documents tika \"autogen-agentchat\" \"autogen-ext[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import CodeInterpreterTool\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Research paper path\n",
    "research_paper_path =  \"https://arxiv.org/pdf/2503.05142\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding make sure you have created an Azure AI Agentic Service project. Copy the connection string to the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(), conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"]\n",
    ")\n",
    "\n",
    "conn_list = project_client.connections._list_connections()[\"value\"]\n",
    "conn_id = \"\"\n",
    "\n",
    "# Search in the metadata field of each connection in the list for the azure_ai_search type and get the id value to establish the variable\n",
    "for conn in conn_list:\n",
    "    metadata = conn[\"properties\"].get(\"metadata\", {})\n",
    "    if metadata.get(\"type\", \"\").upper() == \"AZURE_AI_SEARCH\":\n",
    "        conn_id = conn[\"id\"]\n",
    "        break\n",
    "    \n",
    "print(f\"Connection ID: {conn_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below model will be used for orchestration, you may choose any Open AI model from Azure Open AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    azure_endpoint=os.environ[\"AOAI_ENDPOINT\"], # Azure OpenAI endpoint.\n",
    "    api_key=os.environ[\"AOAI_KEY\"], # For key-based authentication.\n",
    ")\n",
    "\n",
    "# Test the Azure OpenAI model client\n",
    "result = await az_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Create Azure AI Search and configure as connector to the agent. [Link](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/azure-ai-search?tabs=pythonsdk%2Cpython&pivots=code-examples#setup-create-an-agent-that-can-use-an-existing-azure-ai-search-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and ingest the file to vector store\n",
    "\n",
    "In the below step we are downloading the research paper and extracting the content out of it. Note, that I'm only extracting text content, in a real world scenario you may want to extract images, tables using semantic parsing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import uuid\n",
    "from tika import parser \n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "file_path = \"./data/research_paper.pdf\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading research paper...\")\n",
    "    response = requests.get(research_paper_path)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "       f.write(response.content)\n",
    "else:\n",
    "    print(\"Research paper already downloaded.\")\n",
    "raw = parser.from_file(file_path)\n",
    "content = raw['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grounding with Bing Search Agent\n",
    "\n",
    "Grounding helps AI systems understand and reason about the physical world by linking abstract symbols (words, data points) to their real-world meanings and contexts. \n",
    "Bing search can be used to get latest information for words, phrases generated by LLMs from internet, this allows you to ground the text generated to facts, links from sources in internet. Before proceeding, complete the following steps\n",
    "\n",
    "1. Create a Bing Service in Azure, copy the value into the ENV Key/Value pair shown below\n",
    "2. [Connect](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/bing-grounding?tabs=python&pivots=overview) your AI project to the Bing service \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment variable, change the value to your connection name\n",
    "os.environ[\"BING_CONNECTION_NAME \"] = \"bing-grounding\"\n",
    "\n",
    "bing_connection = project_client.connections.get(\n",
    "    connection_name=\"binggrounding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import BingGroundingTool\n",
    "\n",
    "\n",
    "async def web_ai_agent(query: str) -> str:\n",
    "    \"\"\"AIProjectClient agent that uses the Bing search tool to answer questions.\"\"\"\n",
    "    # Create a BingGroundingTool instance\n",
    "    \n",
    "    conn_id = bing_connection.id\n",
    "    \n",
    "    bing = BingGroundingTool(connection_id=conn_id)\n",
    "    \n",
    "    project_client = AIProjectClient.from_connection_string(credential=DefaultAzureCredential(), conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"])\n",
    "    \n",
    "    \n",
    "    with project_client:\n",
    "        \n",
    "        agent = project_client.agents.create_agent(\n",
    "            model=\"gpt-4o\",\n",
    "            name=\"bing-search-assistant\",\n",
    "            instructions=\"\"\"        \n",
    "                You are a web search agent.\n",
    "                Your only tool is search_tool - use it to find information.\n",
    "                You make only one search call at a time.\n",
    "                Once you have the results, you never do calculations based on them.\n",
    "            \"\"\",\n",
    "            tools=bing.definitions,\n",
    "            headers={\"x-ms-enable-preview\": \"true\"}\n",
    "        )\n",
    "        \n",
    "        print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "        # Create thread for communication\n",
    "        thread = project_client.agents.create_thread()\n",
    "        print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "        # Create message to thread\n",
    "        message = project_client.agents.create_message(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=query,\n",
    "        )\n",
    "        \n",
    "        print(f\"SMS: {message}\")\n",
    "        # Create and process agent run in thread with tools\n",
    "        run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "        print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "        if run.status == \"failed\":\n",
    "            print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "        # Delete the assistant when done\n",
    "        project_client.agents.delete_agent(agent.id)\n",
    "        print(\"Deleted agent\")\n",
    "\n",
    "        # Fetch and log all messages\n",
    "        messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "        print(\"Messages:\"+ messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"])\n",
    "    return messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bing_search_agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[web_ai_agent],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    ")\n",
    "\n",
    "print(\"Bing Search Agent created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below code shows how to use tools in the agent service\n",
    "# you can also use to call custom function that can get real-time data, perform calculations, and call APIs\n",
    "async def save_blog_agent(blog_content: str) -> str:\n",
    "    \"\"\"AIProjectClient agent that uses the Code Interpreter tool to save blog content.\"\"\"\n",
    "    \n",
    "    print(\"This is Code Interpreter for Azure AI Agent Service .......\")\n",
    "    \n",
    "    project_client = AIProjectClient.from_connection_string(credential=DefaultAzureCredential(), conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"])\n",
    "    \n",
    "    code_interpreter = CodeInterpreterTool()\n",
    "    \n",
    "    agent = project_client.agents.create_agent(\n",
    "            model=\"gpt-4o\",\n",
    "            name=\"blog-save-agent\",\n",
    "            instructions=\"You are helpful agent\",\n",
    "            tools=code_interpreter.definitions,\n",
    "            # tool_resources=code_interpreter.resources,\n",
    "    )\n",
    "\n",
    "    thread = project_client.agents.create_thread()\n",
    "\n",
    "    message = project_client.agents.create_message(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=\"\"\"\n",
    "        \n",
    "                    You are my Python programming assistant. Generate code,save \"\"\"+ blog_content +\n",
    "                    \n",
    "                \"\"\"    \n",
    "                    and execute it according to the following requirements\n",
    "\n",
    "                    1. Save blog content to blog-{YYMMDDHHMMSS}.md\n",
    "\n",
    "                    2. give me the download this file link\n",
    "                \"\"\",\n",
    "    )\n",
    "    \n",
    "    # create and execute a run\n",
    "    \n",
    "    run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "    \n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    if run.status == \"failed\":\n",
    "        # Check if you got \"Rate limit is exceeded.\", then you want to get more quota\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "        # print the messages from the agent\n",
    "\n",
    "    messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "\n",
    "    print(f\"Messages: {messages}\")\n",
    "\n",
    "    # get the most recent message from the assistant\n",
    "    last_msg = messages.get_last_text_message_by_role(\"assistant\")\n",
    "    \n",
    "    if last_msg:\n",
    "        print(f\"Last Message: {last_msg.text.value}\")\n",
    "\n",
    "    for file_path_annotation in messages.file_path_annotations:\n",
    "\n",
    "        file_name = os.path.basename(file_path_annotation.text)\n",
    "\n",
    "        project_client.agents.save_file(file_id=file_path_annotation.file_path.file_id, file_name=file_name,target_dir=\"./blog\")\n",
    "        \n",
    "        \n",
    "    project_client.agents.delete_agent(agent.id)\n",
    "    \n",
    "    print(\"Deleted agent\")\n",
    "\n",
    "    return \"Saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_blog_content_agent = AssistantAgent(\n",
    "    name=\"save_post_content_agent\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[save_blog_agent],\n",
    "    system_message=\"\"\"\n",
    "        Save post content. Respond with 'Saved' to when your post are saved.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_agent = AssistantAgent(\n",
    "    name=\"write_agent\",\n",
    "    model_client=az_model_client,\n",
    "    system_message=\"\"\"\n",
    "        You are a linked in post writer, please help me write a linked post based on research paper and bing search content.\"\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Orchestration\n",
    "\n",
    "The below code uses AutoGen to setup a roundrobin group chat. RoundRobinGroupChat is a group chat that invokes agents in a round-robin order. It's useful when you want to call multiple agents in a fixed sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_termination = TextMentionTermination(\"Saved\")\n",
    "# Define a termination condition that stops the task after 5 messages.\n",
    "max_message_termination = MaxMessageTermination(max_messages=5)\n",
    "# Combine the termination conditions using the `|`` operator so that the\n",
    "# task stops when either condition is met.\n",
    "termination = text_termination | max_message_termination\n",
    "reflection_team = RoundRobinGroupChat([bing_search_agent, write_agent,save_blog_content_agent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await Console(\n",
    "    reflection_team.run_stream(task=f\"\"\"\n",
    "                    I'm writing a linked in post about a research paper. The content of the research paper is {content}.\n",
    "                    \n",
    "                    Extract the following information from the research paper:\n",
    "                    \n",
    "                    - Problem/Research Question: What is the research trying to address or investigate? \n",
    "                    - Methods/Approach: How did the researchers conduct their study? \n",
    "                    - Results/Findings: What did the research uncover? \n",
    "                    - Conclusions/Implications: What are the main takeaways and significance of the research? \n",
    "                    - Limitations and Future Directions: What\n",
    "                    \n",
    "                    Generate a high-engagement Linked In post about the challenges and solutions in the research paper.\n",
    "                    \n",
    "                    Extract keywords and use bing search to explain the keywords in the research paper.                    \n",
    "                    The tone should be authoritative yet engaging. \n",
    "                    Follow the Hook â†’ Context â†’ Insights â†’ Engagement Trigger structure. End with a question to spark discussion. Add 3-5 relevant hashtags.\n",
    "    \"\"\")\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output below\n",
    "\n",
    "\n",
    "ðŸš€ **Revolutionizing LLM Evaluation: The Rise of RocketEval** ðŸš€\n",
    "\n",
    "In an era where large language models (LLMs) are rapidly evolving, the efficacy of their evaluation has become an urgent necessity. Traditional methods often involve costly human evaluations that can compromise privacy and reproducibility. **Enter \"RocketEval.\"**\n",
    "\n",
    "**Context:** This groundbreaking research presented at ICLR 2025 explores how lightweight LLMs can serve as highly-efficient evaluators through a structured checklist approach. By transitioning the evaluation process from subjective human judgments to a systematic, automated framework, researchers have opened new avenues for cost-effective, scalable assessments that align closely with human preferences.\n",
    "\n",
    "**Insights:** \n",
    "- **Key Findings**: The RocketEval framework demonstrates a remarkable correlation of **0.965** with human evaluations using a lightweight model (Gemma-2-2B), achieving results comparable to powerful models like GPT-4, but at **over a 50-fold** cost reduction! \n",
    "- **Innovative Approach**: By reframing evaluations into multi-faceted checklists, the study addresses common pitfalls in LLM assessments: high uncertainty and positional bias. This systematic approach ensures that critical aspects of a model's response are thoroughly evaluated, leading to more reliable outputs.\n",
    "\n",
    "**Engagement Trigger:** As the landscape of AI continues to shift, how can we ensure that our evaluation methods keep pace with the technology? Do you believe automated systems can fully replace human evaluators, or is there still value in the human touch in this process?\n",
    "\n",
    "ðŸ’¬ *Let's discuss the future of LLM evaluations!*\n",
    "\n",
    "#LLM #Automation #AIResearch #DataScience #MachineLearning \n",
    "\n",
    "---\n",
    "\n",
    "**Keywords Explained**:\n",
    "\n",
    "1. **Large Language Models (LLMs)**: These are complex AI models designed to understand and generate human language. They are foundational in various applications, from chatbots to content generation.\n",
    "\n",
    "2. **Automated Evaluation**: This refers to the use of technology, especially AI, to assess the performance of models. It aims to provide faster, scalable, and cost-effective assessments compared to traditional methods.\n",
    "\n",
    "3. **Checklist Grading**: A systematic approach where specific criteria are outlined (checklists) to guide the evaluation process, ensuring comprehensive and consistent assessments.\n",
    "\n",
    "4. **High Correlation with Human Preferences**: A metric indicating how closely the automated evaluation results match human evaluations, highlighting the effectiveness of the evaluation method.\n",
    "\n",
    "5. **Positional Bias**: A cognitive bias that affects decision-making based on the order of presented information. Addressing positional bias is crucial in evaluations to enhance fairness and accuracy. \n",
    "\n",
    "Feel free to ask if you have more specific questions or need additional information!    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
