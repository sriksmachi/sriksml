{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent orchestrator using Azure AI Agentic Service\n",
    "\n",
    "**Problem Statment**: AI agents are autonomous software entities designed to perform tasks, make decisions, and interact with environments using artificial intelligence, machine learning, natural language processing, and reinforcement learning. However, every custom AI agent written today needs lifecycle management, that includes packaging as containers, deployment, scaling, allocation of right resources etc.\n",
    "As the number of agents grow in your ecosystem it becomes tedious to manage the environment. \n",
    "\n",
    "**Azure AI Agent Service** is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, and extensible AI agents without needing to manage the underlying compute and storage resources. We can use Azure AI Agent Service to create and run an agent in just a few lines of code. You can also manage complex workflows with **AutoGen** and **Semantic Kernel**.\n",
    "\n",
    "\n",
    "In this example we will summarize a research paper into LinkedIn post using Azure Agentic Service, Bing & LLMs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Project PIP requirements\n",
    "# %pip install -q azure-ai-projects azure-identity azure-ai-ml azure-search-documents tika \"autogen-agentchat\" \"autogen-ext[openai]\"\n",
    "# %pip install -q -U \"autogen-agentchat\" \"autogen-ext[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import CodeInterpreterTool\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Research paper path\n",
    "research_paper_path =  \"https://arxiv.org/pdf/2503.05142\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding make sure you have created an Azure AI Agentic Service project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection ID: /subscriptions/6a01260f-39d6-415f-a6c9-cf4fd479cbec/resourceGroups/rg-vism-7896_ai/providers/Microsoft.MachineLearningServices/workspaces/agentic-service/connections/vectorsearchagentic\n"
     ]
    }
   ],
   "source": [
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(), conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"]\n",
    ")\n",
    "\n",
    "conn_list = project_client.connections._list_connections()[\"value\"]\n",
    "conn_id = \"\"\n",
    "\n",
    "# Search in the metadata field of each connection in the list for the azure_ai_search type and get the id value to establish the variable\n",
    "for conn in conn_list:\n",
    "    metadata = conn[\"properties\"].get(\"metadata\", {})\n",
    "    if metadata.get(\"type\", \"\").upper() == \"AZURE_AI_SEARCH\":\n",
    "        conn_id = conn[\"id\"]\n",
    "        break\n",
    "    \n",
    "print(f\"Connection ID: {conn_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=15, completion_tokens=8) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    azure_endpoint=os.environ[\"AOAI_ENDPOINT\"], # Azure OpenAI endpoint.\n",
    "    api_key=os.environ[\"AOAI_KEY\"], # For key-based authentication.\n",
    ")\n",
    "\n",
    "# Test the Azure OpenAI model client\n",
    "result = await az_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-requisites**\n",
    "1. Create Azure AI Search and configure as connector to the agent. [Link](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/azure-ai-search?tabs=pythonsdk%2Cpython&pivots=code-examples#setup-create-an-agent-that-can-use-an-existing-azure-ai-search-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and ingest the file to vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research paper already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import uuid\n",
    "from tika import parser \n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "file_path = \"./data/research_paper.pdf\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading research paper...\")\n",
    "    response = requests.get(research_paper_path)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "       f.write(response.content)\n",
    "else:\n",
    "    print(\"Research paper already downloaded.\")\n",
    "raw = parser.from_file(file_path)\n",
    "content = raw['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bing Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment variable\n",
    "os.environ[\"BING_CONNECTION_NAME \"] = \"bing-grounding\"\n",
    "\n",
    "bing_connection = project_client.connections.get(\n",
    "    connection_name=\"binggrounding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import BingGroundingTool\n",
    "\n",
    "async def web_ai_agent(query: str) -> str:\n",
    "    \n",
    "    conn_id = bing_connection.id\n",
    "    \n",
    "    bing = BingGroundingTool(connection_id=conn_id)\n",
    "    \n",
    "    project_client = AIProjectClient.from_connection_string(credential=DefaultAzureCredential(), conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"])\n",
    "    \n",
    "    \n",
    "    with project_client:\n",
    "        \n",
    "        agent = project_client.agents.create_agent(\n",
    "            model=\"gpt-4o\",\n",
    "            name=\"bing-search-assistant\",\n",
    "            instructions=\"\"\"        \n",
    "                You are a web search agent.\n",
    "                Your only tool is search_tool - use it to find information.\n",
    "                You make only one search call at a time.\n",
    "                Once you have the results, you never do calculations based on them.\n",
    "            \"\"\",\n",
    "            tools=bing.definitions,\n",
    "            headers={\"x-ms-enable-preview\": \"true\"}\n",
    "        )\n",
    "        \n",
    "        print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "        # Create thread for communication\n",
    "        thread = project_client.agents.create_thread()\n",
    "        print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "        # Create message to thread\n",
    "        message = project_client.agents.create_message(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=query,\n",
    "        )\n",
    "        print(f\"SMS: {message}\")\n",
    "        # Create and process agent run in thread with tools\n",
    "        run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "        print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "        if run.status == \"failed\":\n",
    "            print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "        # Delete the assistant when done\n",
    "        project_client.agents.delete_agent(agent.id)\n",
    "        print(\"Deleted agent\")\n",
    "\n",
    "        # Fetch and log all messages\n",
    "        messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "        print(\"Messages:\"+ messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"])\n",
    "    return messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bing Search Agent created\n"
     ]
    }
   ],
   "source": [
    "bing_search_agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[web_ai_agent],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    ")\n",
    "\n",
    "print(\"Bing Search Agent created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save_blog_agent(blog_content: str) -> str:\n",
    "    \n",
    "    print(\"This is Code Interpreter for Azure AI Agent Service .......\")\n",
    "    \n",
    "    project_client = AIProjectClient.from_connection_string(credential=DefaultAzureCredential(), conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"])\n",
    "    \n",
    "    code_interpreter = CodeInterpreterTool()\n",
    "    \n",
    "    agent = project_client.agents.create_agent(\n",
    "            model=\"gpt-4o\",\n",
    "            name=\"blog-save-agent\",\n",
    "            instructions=\"You are helpful agent\",\n",
    "            tools=code_interpreter.definitions,\n",
    "            # tool_resources=code_interpreter.resources,\n",
    "    )\n",
    "\n",
    "    thread = project_client.agents.create_thread()\n",
    "\n",
    "    message = project_client.agents.create_message(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=\"\"\"\n",
    "        \n",
    "                    You are my Python programming assistant. Generate code,save \"\"\"+ blog_content +\n",
    "                    \n",
    "                \"\"\"    \n",
    "                    and execute it according to the following requirements\n",
    "\n",
    "                    1. Save blog content to blog-{YYMMDDHHMMSS}.md\n",
    "\n",
    "                    2. give me the download this file link\n",
    "                \"\"\",\n",
    "    )\n",
    "    \n",
    "    # create and execute a run\n",
    "    \n",
    "    run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "    \n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    if run.status == \"failed\":\n",
    "        # Check if you got \"Rate limit is exceeded.\", then you want to get more quota\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "        # print the messages from the agent\n",
    "\n",
    "    messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "\n",
    "    print(f\"Messages: {messages}\")\n",
    "\n",
    "    # get the most recent message from the assistant\n",
    "    last_msg = messages.get_last_text_message_by_role(\"assistant\")\n",
    "    \n",
    "    if last_msg:\n",
    "        print(f\"Last Message: {last_msg.text.value}\")\n",
    "\n",
    "    for file_path_annotation in messages.file_path_annotations:\n",
    "\n",
    "        file_name = os.path.basename(file_path_annotation.text)\n",
    "\n",
    "        project_client.agents.save_file(file_id=file_path_annotation.file_path.file_id, file_name=file_name,target_dir=\"./blog\")\n",
    "        \n",
    "        \n",
    "    project_client.agents.delete_agent(agent.id)\n",
    "    \n",
    "    print(\"Deleted agent\")\n",
    "\n",
    "    return \"Saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_blog_content_agent = AssistantAgent(\n",
    "    name=\"save_post_content_agent\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[save_blog_agent],\n",
    "    system_message=\"\"\"\n",
    "        Save post content. Respond with 'Saved' to when your post are saved.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_agent = AssistantAgent(\n",
    "    name=\"write_agent\",\n",
    "    model_client=az_model_client,\n",
    "    system_message=\"\"\"\n",
    "        You are a linked in post writer, please help me write a linked post based on research paper and bing search content.\"\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_termination = TextMentionTermination(\"Saved\")\n",
    "# Define a termination condition that stops the task after 5 messages.\n",
    "max_message_termination = MaxMessageTermination(max_messages=5)\n",
    "# Combine the termination conditions using the `|`` operator so that the\n",
    "# task stops when either condition is met.\n",
    "termination = text_termination | max_message_termination\n",
    "reflection_team = RoundRobinGroupChat([bing_search_agent, write_agent,save_blog_content_agent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "\n",
      "                    I'm writing a linked in post about a research paper. The content of the research paper is \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "ROCKETEVAL: EFFICIENT AUTOMATED LLM\n",
      "EVALUATION VIA GRADING CHECKLIST\n",
      "\n",
      "Tianjun Wei∗† 12, Wei Wen∗ 2, Ruizhi Qiao‡2, Xing Sun 2, Jianghong Ma‡ 3\n",
      "\n",
      "1 City University of Hong Kong, 2 Tencent Youtu Lab, 3 Harbin Institute of Technology Shenzhen.\n",
      "tjwei2-c@my.cityu.edu.hk {jawnrwen,ruizhiqiao,winfredsun}@tencent.com\n",
      "majianghong@hit.edu.cn\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Evaluating large language models (LLMs) in diverse and challenging scenarios\n",
      "is essential to align them with human preferences. To mitigate the prohibitive\n",
      "costs associated with human evaluations, utilizing a powerful LLM as a judge\n",
      "has emerged as a favored approach. Nevertheless, this methodology encounters\n",
      "several challenges, including substantial expenses, concerns regarding privacy\n",
      "and security, and reproducibility. In this paper, we propose a straightforward,\n",
      "replicable, and accurate automated evaluation method by leveraging a lightweight\n",
      "LLM as the judge, named RocketEval. Initially, we identify that the performance\n",
      "disparity between lightweight and powerful LLMs in evaluation tasks primarily\n",
      "stems from their ability to conduct comprehensive analyses, which is not easily\n",
      "enhanced through techniques such as chain-of-thought reasoning. By reframing\n",
      "the evaluation task as a multi-faceted Q&A using an instance-specific checklist,\n",
      "we demonstrate that the limited judgment accuracy of lightweight LLMs is largely\n",
      "attributes to high uncertainty and positional bias. To address these challenges, we\n",
      "introduce an automated evaluation process grounded in checklist grading, which\n",
      "is designed to accommodate a variety of scenarios and questions. This process en-\n",
      "compasses the creation of checklists, the grading of these checklists by lightweight\n",
      "LLMs, and the reweighting of checklist items to align with the supervised annota-\n",
      "tions. Our experiments carried out on the automated evaluation benchmarks, MT-\n",
      "BENCH and WILDBENCH datasets, reveal that RocketEval, when using Gemma-\n",
      "2-2B as the judge, achieves a high correlation (0.965) with human preferences,\n",
      "which is comparable to GPT-4o. Moreover, RocketEval provides a cost reduction\n",
      "exceeding 50-fold for large-scale evaluation and comparison scenarios. Our code\n",
      "is available at https://github.com/Joinn99/RocketEval-ICLR.\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Why is automated LLM evaluation necessary? In recent years, the progress in large language\n",
      "models (LLMs) has been remarkable (Jiang et al., 2024a; Team et al., 2024; Yang et al., 2024), driven\n",
      "by continuous technological advancements. The rapid emergence of new models and techniques has\n",
      "broadened their applications, encompassing both general-purpose textual and visual LLMs, as well\n",
      "as those fine-tuned for specific tasks in various domains. These LLMs exhibit a range of capabili-\n",
      "ties and performances across different application scenarios. Therefore, evaluating their capabilities\n",
      "effectively has become crucial for guiding their development. Since most tasks performed by LLMs\n",
      "involve human interaction, human preferences are often considered the gold standard for LLM eval-\n",
      "uation (Zheng et al., 2023). Currently, crowd-sourcing platforms like CHATBOT ARENA (Chiang\n",
      "et al., 2024) collect a significant number of human votes to evaluate LLMs. However, this approach\n",
      "relies on extensive and long-term human annotation, which is costly and challenging to reproduce\n",
      "and interpret (Ni et al., 2024). Considering the typical applications of LLM evaluation today, which\n",
      "\n",
      "∗Equal Contribution.\n",
      "†Work done during internship at Tencent.\n",
      "‡Corresponding Authors: Ruizhi Qiao and Jianghong Ma.\n",
      "1Unless otherwise stated, all names of LLM in this paper refer to their \"Instruct\" or \"Chat\" versions.\n",
      "\n",
      "1\n",
      "\n",
      "ar\n",
      "X\n",
      "\n",
      "iv\n",
      ":2\n",
      "\n",
      "50\n",
      "3.\n",
      "\n",
      "05\n",
      "14\n",
      "\n",
      "2v\n",
      "1 \n",
      "\n",
      " [\n",
      "cs\n",
      "\n",
      ".C\n",
      "L\n",
      "\n",
      "] \n",
      " 7\n",
      "\n",
      " M\n",
      "ar\n",
      "\n",
      " 2\n",
      "02\n",
      "\n",
      "5\n",
      "\n",
      "https://github.com/Joinn99/RocketEval-ICLR\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "5 10 15 20 25 30 35 40 45\n",
      "OpenLLM LeaderBoard 2 Average\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "A\n",
      "gr\n",
      "\n",
      "ee\n",
      "m\n",
      "\n",
      "en\n",
      "t \n",
      "\n",
      "w\n",
      "it\n",
      "\n",
      "h \n",
      "hu\n",
      "\n",
      "m\n",
      "an\n",
      "\n",
      " %\n",
      "\n",
      "Qwen2-72B\n",
      "\n",
      "Llama-3-70B\n",
      "\n",
      "Yi-1.5-34B\n",
      "\n",
      "Mistral-Nemo\n",
      "\n",
      "Gemma-2-9B\n",
      "\n",
      "Yi-1.5-9B\n",
      "\n",
      "Qwen2-7B\n",
      "Mistral-7B-v0.3\n",
      "\n",
      "Qwen1.5-4B\n",
      "\n",
      "Phi-3-mini-4k\n",
      "Gemma-2-2B\n",
      "\n",
      "Llama-3.2-3B\n",
      "\n",
      "InternLM2.5-1.8B\n",
      "\n",
      "Qwen2-1.5B\n",
      "Llama-3.2-1B\n",
      "\n",
      "Yi-1.5-6B\n",
      "\n",
      "Llama-3-8B\n",
      "\n",
      "Qwen1.5-14B\n",
      "\n",
      "Phi-3.5-mini\n",
      "\n",
      "Phi-3-small-8k\n",
      "\n",
      "Phi-3-medium-4k\n",
      "GLM-4-9B\n",
      "\n",
      "Qwen2.5-3B\n",
      "\n",
      "Qwen2.5-1.5B\n",
      "\n",
      "Qwen2.5-0.5B\n",
      "\n",
      "InternLM2.5-20B\n",
      "InternLM2.5-7B\n",
      "\n",
      "Llama-3.1-8B\n",
      "\n",
      "Agreement (CoT)\n",
      "Agreement (Ours)\n",
      "Human (64.8%)\n",
      "GPT-4o (66.7%)\n",
      "Random (33.3%)\n",
      "\n",
      "Figure 1: Agreements with MT-BENCH HUMAN JUDGMENTS with different LLM 1judges. \"CoT\"\n",
      "indicates the judgments derived using the original Chain-of-thought (CoT) prompting, and \"Ours\"\n",
      "indicates the judgments derived using our proposed RocketEval framework.\n",
      "\n",
      "include validating the effectiveness of LLM development and assisting users in selecting the best-\n",
      "performing models for specific tasks, there is a growing demand for more efficient, reproducible,\n",
      "and interpretable evaluations of LLMs. This demand has led to the proposal of various methods\n",
      "(Wang et al., 2024b; Kim et al., 2024a) and benchmarks (Lin et al., 2025; Zheng et al., 2023) aimed\n",
      "at achieving reliable and efficient automated LLM evaluation.\n",
      "\n",
      "Pros and cons of the existing automated LLM evaluation paths. Automated LLM evaluation\n",
      "methods can generally be categorized into three primary types:\n",
      "\n",
      "Multiple-Choice Questions (MCQ) and Keyword Matching Benchmarks. These methods evaluate\n",
      "the accuracy of a model’s responses by designing a series of closed-ended questions and comparing\n",
      "the model’s answers to a predefined standard ground truth. Benchmarks constructed using this\n",
      "approach have proven effective for quickly assessing a model’s capabilities across several tasks,\n",
      "such as reasoning (Zellers et al., 2019; Qiu et al., 2020), comprehension (Mihaylov et al., 2018; Liu\n",
      "et al., 2024), and knowledge retention (Hendrycks et al., 2021; Liang et al., 2023). However, the\n",
      "requirement for specific response formats limits the comprehensiveness of this evaluation method.\n",
      "In practice, most LLM applications involve response styles that differ significantly from simple\n",
      "choices and keywords. Notably, many open-ended tasks cannot be judged by a fixed ground truth,\n",
      "which strictly limits the applicability of this approach.\n",
      "\n",
      "LLM-as-a-Judge. Early studies have attempted to adopt language models in automated evaluation\n",
      "(Zhang* et al., 2020; Yuan et al., 2021). Given the robust generalization capabilities of LLMs,\n",
      "employing a powerful LLM as an evaluator has emerged as a viable solution. This method typically\n",
      "involves prompting an LLM to serve as a judge, evaluating the responses from different LLMs to\n",
      "a set of well-designed queries. A crucial prerequisite for LLM-as-a-Judge is that the LLM must\n",
      "possess sufficient capability to fully comprehend the queries and discern the quality of different\n",
      "responses. Consequently, many existing benchmarks (Zheng et al., 2023; Dubois et al., 2023; Li\n",
      "et al., 2024a; Liu et al., 2023) tend to employ the most powerful proprietary LLMs, such as GPT-4o,\n",
      "as the judge. However, the use of these models for evaluation not only incurs high costs but also\n",
      "raises other issues such as reproducibility and data privacy.\n",
      "\n",
      "Fine-Tuned Judge Models. These models are designed to address the limitations associated with\n",
      "powerful LLM-as-a-Judge approaches. Fine-tuned judge models, derived from lightweight base\n",
      "models, are trained with high-quality evaluation data to more closely align with human preferences\n",
      "(Jiang et al., 2024b; Zhu et al., 2025; Wang et al., 2024b; Kim et al., 2024a). Compared to pro-\n",
      "prietary LLMs, such fine-tuned judge models exhibit competitive evaluation capabilities in a more\n",
      "transparent and cost-effective manner. However, simply fine-tuning the model on evaluation task\n",
      "data may degrade other capabilities of the lightweight model, which are already weaker compared\n",
      "to those of powerful proprietary LLMs. This degradation can result in the model failing to correctly\n",
      "understand complex instructions in the queries, thereby deteriorating subsequent evaluation perfor-\n",
      "mance (Huang et al., 2024). Additionally, as the capabilities of the base models rapidly evolve and\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "   \"Yes\": -0.25\n",
      "\"No\":  -1.27\n",
      "\n",
      "Normalized      \n",
      "Probability %   \n",
      "\n",
      "���\n",
      "\n",
      "���\n",
      "\n",
      "   What is the date of \n",
      "\n",
      "   the 15th day after\n",
      "   Christmas in 2024?\n",
      "   Think step by step.\n",
      "\n",
      "   Answer me with a \n",
      "   JSON containing\n",
      "   \"Thought\" and\n",
      "\n",
      "   \"Answer\".\n",
      "\n",
      "   A {\"Thought\"���\"Answer\": \"Jan 9 2025\"}\n",
      "\n",
      "   B Thought: The 15th ��� is 2025-1-9.\n",
      "\n",
      "   C {\"Thought\"���\"Answer\": \"2024-12-30\"}\n",
      "\n",
      "   Does the response provide\n",
      "\n",
      "   detailed analysis?\n",
      "\n",
      "   Is the response correctly \n",
      "\n",
      "   formatted as JSON?\n",
      "\n",
      "   Does the response include  \n",
      "\n",
      "   answer \"Jan 9 2025\"?\n",
      "\n",
      "🤖 X 🤖 X    X (Response)\n",
      "\n",
      "0\n",
      "82\n",
      "\n",
      "55\n",
      "\n",
      "75 3\n",
      "62\n",
      "\n",
      "99 80 72\n",
      "\n",
      "0\n",
      "82\n",
      "\n",
      "550 82\n",
      "55\n",
      "\n",
      "0 82\n",
      "555 97\n",
      "30\n",
      "\n",
      "   9/10📝\n",
      "👤\n",
      "\n",
      "9.7\n",
      "\n",
      "6.1\n",
      "\n",
      "2.3\n",
      "\n",
      "74\n",
      "\n",
      "Checklist Generation\n",
      "\n",
      "Query with History\n",
      "\n",
      "AnnotationsResponses\n",
      "\n",
      "Checklist\n",
      "\n",
      "🤖\n",
      "\n",
      "Judgment\n",
      "\n",
      "Alignment\n",
      "\n",
      "Score\n",
      "\n",
      "🤖\n",
      "\n",
      "👤\n",
      "\n",
      "👤\n",
      "\n",
      "🤖\n",
      "\n",
      "�\n",
      "\n",
      "�\n",
      "\n",
      "�\n",
      "\n",
      "🤖\n",
      "\n",
      "🤖\n",
      "\n",
      "🤖\n",
      "\n",
      "🤖\n",
      "\n",
      "📊\n",
      "\n",
      "Figure 2: Illustration of RocketEval framework for automated LLM evaluation. The framework\n",
      "consists of three components: Checklist Creation, Checklist Grading and Score Prediction.\n",
      "\n",
      "new data continuously emerges, these judge models may need iterative updates, leading to signifi-\n",
      "cant cost escalations and reproducibility issues.\n",
      "\n",
      "RocketEval: Towards efficient automated LLM evaluation. Building on the aforementioned\n",
      "concepts, we find a pathway that synergizes the language modeling and human preference align-\n",
      "ment capabilities of the most powerful LLMs with the evaluation efficiency of lightweight models.\n",
      "Specifically, we conduct a thorough analysis of the evaluation capabilities of various large models\n",
      "and observe the following:\n",
      "\n",
      "1. The agreement between LLM judges and humans is significantly influenced by the inherent\n",
      "capabilities of the LLMs. More powerful LLMs tend to generate evaluation results that\n",
      "more accurately reflect human preferences.\n",
      "\n",
      "2. Prompt engineering techniques, such as Chain-of-Thought (CoT), exert minimal impact on\n",
      "the evaluation capabilities of the model, particularly when a lightweight LLM is employed\n",
      "as the judge. High uncertainty and positional bias during the decoding process are potential\n",
      "contributing factors to this phenomenon.\n",
      "\n",
      "Inspired by these observations, we introduce a novel evaluation framework named RocketEval, de-\n",
      "signed to meet the demands of evaluation scenarios that require high efficiency, low cost, alignment\n",
      "with human preferences, reproducibility, and interpretability. As illustrated in Figure 2, RocketE-\n",
      "val operates through a three-stage framework to generate evaluations. Initially, an instance-level\n",
      "checklist is created, providing essential knowledge and critical focus areas to overcome the limi-\n",
      "tations of lightweight LLMs in constructing analyses. Subsequently, lightweight LLMs assess the\n",
      "quality of responses for each checklist item independently, resulting in a multifaceted and unbi-\n",
      "ased judgment. The normalized score of each judgment is then collected and aggregated to predict\n",
      "the final score, aiming to mitigate the uncertainty associated with lightweight LLMs. Considering\n",
      "the widespread availability of human annotations across various evaluation scenarios, we further\n",
      "introduce a supervised prediction process to align the scores from lightweight LLMs with these\n",
      "annotations. Experimental results demonstrate that RocketEval significantly enhances agreement\n",
      "with human judgments, achieving a remarkable Spearman correlation of 0.965 when utilizing the\n",
      "Gemma-2-2B model as the judge to rank test models. This offers a comparable solution to GPT-\n",
      "4o at only 2% of the evaluation cost in large-scale evaluation scenarios, rendering it suitable for\n",
      "performing efficient, reliable, and reproducible LLM evaluations.\n",
      "\n",
      "2 HOW LIGHTWEIGHT LLMS PERFORM AS A JUDGE?\n",
      "\n",
      "In this section, we verify the capability of lightweight LLMs in automated evaluation by conducting\n",
      "a series of experiments employing various LLM judges.\n",
      "\n",
      "2.1 SETUP\n",
      "\n",
      "We selected two benchmark datasets for our experiments:\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "• MT-BENCH (Zheng et al., 2023) is a classic benchmark that includes 80 multi-round\n",
      "queries from multiple domains and uses GPT-4 as the judge for multi-round evaluations.\n",
      "\n",
      "• WILDBENCH (Lin et al., 2025) is a newly released benchmark containing 1,024 real-world\n",
      "user queries. WildBench first introduces a manually revised checklist as contextual infor-\n",
      "mation to guide the evaluation and uses GPT-4o as the judge.\n",
      "\n",
      "Both benchmarks include pairwise comparison and point-wise scoring evaluation methods. Since\n",
      "the results of pairwise comparisons can be derived from comparing the scores derived from point-\n",
      "wise scoring, we focus on the point-wise scoring method. In point-wise setting, both benchmarks\n",
      "prompt the judge to first generate an analysis of the response, followed by a score in 1-10 as the\n",
      "final judgment. This can be seen as a chain-of-thought (CoT) (Wei et al., 2022) process, aimed at\n",
      "enhancing the ability of the LLM on the evaluation task that involves a reasoning process.\n",
      "\n",
      "2.2 HOW DO LIGHTWEIGHT LLM JUDGES PERFORM?\n",
      "\n",
      "First, we aim to understand the capabilities of different models when they serve as judges. A key\n",
      "metric for evaluating LLMs’ evaluation capability in aligning user preferences is their agreement\n",
      "with humans. Here, we measure this agreement using MT-BENCH HUMAN JUDGMENTS (Zheng\n",
      "et al., 2023), which provides human-annotated results for MT-BENCH across six test models. Each\n",
      "sample includes a target query, responses from two LLMs, and human-annotated match results (in-\n",
      "cluding ties). We structure the scores obtained in a point-wise manner into the same format by\n",
      "comparing the scores of each pair of responses. Unlike the previous study (Kim et al., 2024a) that\n",
      "uses sampling decoding to derive results without ties, we follow the setting of Lin et al. (2025) and\n",
      "retain cases with scores difference smaller than 0.1 as ties. We then compared the agreement of\n",
      "different LLM judges with human annotations. The human-human agreement reported by Zheng\n",
      "et al. (2023) and the agreement between GPT-4o and human are listed as the baseline. As illus-\n",
      "trated in Figure 1, agreement between LLMs and humans typically ranges from human-to-human\n",
      "agreement (64.8%) to lower than random outcomes (33.3%). This suggests that the existing evalua-\n",
      "tion method using CoT scoring imposes significant demands on the judges’ abilities. The ability of\n",
      "LLMs, reflected by OPENLLM 2 (Fourrier et al., 2024) scores, significantly impacts its alignment\n",
      "with human preferences during evaluation, thereby complicating the process of performing efficient\n",
      "and reliable evaluations with lightweight LLMs.\n",
      "\n",
      "2.3 WHERE DO LIGHTWEIGHT LLMS UNDERPERFORM AS A JUDGE?\n",
      "\n",
      "In this part, our objective is to delve deeper and identify the key components that affect the judg-\n",
      "ment of lightweight LLMs when they serve as judges. We start by conducting an analysis of the two\n",
      "processes involved in evaluation: analysis generation and scoring. To determine whether a compre-\n",
      "hensive analysis can boost or degrade the performance of judgment, we instruct the LLM judges to\n",
      "score the responses under three different settings:\n",
      "\n",
      "• CoT: The original Chain-of-Thought style, generating the analysis and score step-by-step.\n",
      "\n",
      "• Direct: The judge is prompted to skip the analysis step and score the response directly.\n",
      "\n",
      "• CoTGPT-4o: We extract the analysis generated by GPT-4o to replace the part in judge\n",
      "model’s output, then prompt the judge model to score the response accordingly.\n",
      "\n",
      "Then, the scores of all responses are averaged to derive the score of the tested model for ranking.\n",
      "For baselines, we derive the score predicted by GPT-4o and Claude-3.5-Sonnet as judges equipped\n",
      "with CoT. We use the CHATBOT ARENA ELO RATINGS (Hard-prompt English) (Chiang et al., 2024)\n",
      "as the ground-truth rankings of human preferences in these models. Figure 3 shows the scores of\n",
      "12 test models and the Spearman correlation coefficient of ranking lists with GPT-4o when different\n",
      "LLMs served as judge. Also, we convert the scores into pairwise comparison results and com-\n",
      "pare the agreement of different LLM judges with strong baselines. The result is shown in Table\n",
      "1. Compared to direct scoring, the process of lightweight models conducting their own analysis\n",
      "did not yield significant gains. However, utilizing analysis from powerful LLM as the context sig-\n",
      "nificantly improves the evaluation performance in terms of instance-level agreement and list-level\n",
      "correlations. This indicates that lightweight judges are capable of calibrating their judgments with\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "1 3 5 7 9 11\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "Sc\n",
      "\n",
      "or\n",
      "e\n",
      "\n",
      "0.035\n",
      "0.909\n",
      "0.958\n",
      "\n",
      "0.923\n",
      "0.979\n",
      "\n",
      "0.979\n",
      "0.986\n",
      "\n",
      "Analysis + Score\n",
      "\n",
      "1 3 5 7 9 11\n",
      "Model Ranking in Arena Elo (Hard-EN)\n",
      "\n",
      "0.420\n",
      "0.727\n",
      "0.958\n",
      "\n",
      "0.944\n",
      "0.923\n",
      "\n",
      "Only Score\n",
      "\n",
      "1 3 5 7 9 11\n",
      "\n",
      "0.958\n",
      "0.979\n",
      "0.979\n",
      "\n",
      "0.979\n",
      "0.979\n",
      "\n",
      "GPT-4o Analysis + Score\n",
      "\n",
      "Qwen2-1.5B\n",
      "Qwen2-7B\n",
      "Qwen2-72B\n",
      "\n",
      "Llama-3-8B\n",
      "Llama-3-70B\n",
      "\n",
      "GPT-4o\n",
      "Claude-3.5-Sonnet\n",
      "\n",
      "Figure 3: WILDBENCH scores predicted by different\n",
      "LLM judges and the ranking correlation with GPT-4o.\n",
      "\n",
      "Table 1: Agreements of different judges\n",
      "on WILDBENCH.\n",
      "\n",
      "Agreement with GPT-4o\n",
      "\n",
      "Direct CoT CoTGPT-4o\n",
      "\n",
      "Claude-3.5-Sonnet 60.7%\n",
      "\n",
      "Qwen2-1.5B 40.1% 36.4% 70.3%\n",
      "Qwen2-7B 43.1% 45.0% 76.6%\n",
      "Qwen2-72B 61.0% 61.4% 74.0%\n",
      "Llama-3-8B 46.7% 48.4% 74.0%\n",
      "Llama-3-70B 57.2% 59.3% 74.9%\n",
      "\n",
      "Agreement with Claude-3.5-Sonnet\n",
      "\n",
      "GPT-4o 60.7%\n",
      "\n",
      "Qwen2-1.5B 40.6% 36.4% 58.2%\n",
      "Qwen2-7B 42.8% 45.3% 60.3%\n",
      "Qwen2-72B 61.5% 62.3% 62.9%\n",
      "Llama-3-8B 48.2% 49.2% 61.0%\n",
      "Llama-3-70B 58.3% 62.0% 63.4%\n",
      "\n",
      "powerful LLM when high-quality and comprehensive analysis is provided. In other words, the abil-\n",
      "ity of such lightweight LLM judges is limited mostly due to the poor comprehension and analysis\n",
      "process when they dealing with hard queries and complicated responses.\n",
      "\n",
      "2.4 WHY DO LIGHTWEIGHT LLMS NOT GOOD AT JUDGING?\n",
      "\n",
      "In the evaluation scenario, conducting analysis can be viewed as a series of judgments on the target\n",
      "response. In order to explore the reasons why the lightweight model are struggling in the conducting\n",
      "analysis, we further conduct a fine-grained level experiments. Specifically, we transform the process\n",
      "of analyzing into a process of judging on a series of checklist questions. We treat each item in the\n",
      "checklist provided in WILDBENCH as a independent question, and prompt the LLM judge to make\n",
      "decisions. Since these questions can be viewed as binary choice questions (for example, \"Does the\n",
      "response correctly identify...\"), the judges are asked to simply output \"Yes\" or \"No\" for each question\n",
      "based on the content of the query and response. Then, we obtain judgments of different LLM judges\n",
      "on the checklist questions through repeated sampling, and calculate the ratio of disagreement in dif-\n",
      "ferent sampling results among all checklist questions. Figure 4 shows that the ratio of disagreement\n",
      "varies significantly across different model sizes. Lightweight LLM such as Qwen-2-1.5B, shows an\n",
      "disagreement ratio exceeding 50% across 3 sampling results. This indicates a high uncertainty in\n",
      "making decisions on these checklist questions.\n",
      "\n",
      "1 2 3 4 5 6 7\n",
      "Number of sampling times\n",
      "\n",
      "0.0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "Ra\n",
      "tio\n",
      "\n",
      " o\n",
      "f d\n",
      "\n",
      "is\n",
      "ag\n",
      "\n",
      "re\n",
      "em\n",
      "\n",
      "en\n",
      "t\n",
      "\n",
      "Llama-3-70B\n",
      "Llama-3-8B\n",
      "Qwen2-72B\n",
      "Qwen2-7B\n",
      "Qwen2-1.5B\n",
      "\n",
      "Figure 4: Ratio of disagreement on WILDBENCH\n",
      "with different number of sampling times.\n",
      "\n",
      "1 2 3 4 5 6 7\n",
      "Position of the checklist item\n",
      "\n",
      "0.0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "Ra\n",
      "tio\n",
      "\n",
      " o\n",
      "f d\n",
      "\n",
      "is\n",
      "ag\n",
      "\n",
      "re\n",
      "em\n",
      "\n",
      "en\n",
      "t\n",
      "\n",
      "Llama3-70B\n",
      "Llama-3-8B\n",
      "Qwen2-72B\n",
      "Qwen2-7B\n",
      "Qwen2-1.5B\n",
      "\n",
      "Figure 5: Ratio of disagreement on WILDBENCH\n",
      "with checklist items in different positions.\n",
      "\n",
      "When lightweight LLM judges conduct the CoT style analysis, this high uncertainty may lead to\n",
      "greater deviations in the final scoring results and degrade the performance. Although previous stud-\n",
      "ies (Li et al., 2024b; Wang et al., 2024a) has demonstrated bias of response order in pairwise compar-\n",
      "isons, bias in the analytical process remains unexplored. Therefore, we want to understand whether\n",
      "this form can affect the models’ judgment results. We transform the item-level questions from the\n",
      "previous step into a multi-turn dialogue format, with the sequence consistent with the original check-\n",
      "list order. To show the impact of different judgment results, we set all previous judgments to \"Yes\"\n",
      "or \"No\" and then compare the disagreements in the current item’s judgment results under the two\n",
      "settings. As shown in Figure 5, with an increase in the number of previous questions, the proportion\n",
      "of inconsistencies in all models shows a growing trend. Interestingly, the Llama-3 series show an\n",
      "overall higher consistency compared to the Qwen-2 series. Additionally, we notice a correlation\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "between model size and consistency, with smaller models tending to produce higher inconsistency.\n",
      "This suggests that the uncertainty of lightweight LLMs is more likely to be amplified in the process\n",
      "of setting up sequential analyses with CoT, thus reducing confidence in automated evaluation.\n",
      "\n",
      "From the above analysis, we demonstrate that lightweight LLM judges exhibit high uncertainty and\n",
      "position bias, which can lead to difficulties in making reliable judgments. A feasible and efficient\n",
      "evaluation method should avoid the above defects, which inspired us to propose the RocketEval.\n",
      "\n",
      "3 METHODOLOGY\n",
      "\n",
      "In this section, based on the analysis on existing evaluation benchmarks, we introduce a new auto-\n",
      "mated LLM evaluation framework named RocketEval. As shown in Figure 2, the entire RocketEval\n",
      "framework can be divided into three stages. First, we employ a powerful LLM such as GPT-4o to\n",
      "conduct meta-analysis and create a checklist for assessing user query. Subsequently, lightweight\n",
      "LLMs are employed to evaluate the checklist items for responses from each test model. Finally, the\n",
      "evaluations for each item are collected to derive the final score, which can be predicted using either\n",
      "an unsupervised arithmetic mean or a supervised predictor learned from annotations.\n",
      "\n",
      "3.1 CHECKLIST CREATION\n",
      "\n",
      "Evaluating LLM responses is challenging for both human evaluators and LLM judges. Human\n",
      "evaluations can be subjective, while LLM judges may struggle with query understanding, detailed\n",
      "analysis, and interpretation, especially when the lightweight LLMs are employed as the judge. Ex-\n",
      "isting methods improve the accuracy of LLM judgments by providing additional information to the\n",
      "LLM Judge’s analytical process, such as reference answers (Zheng et al., 2023; Dubois et al., 2023)\n",
      "and hand-crafted rubrics (Kim et al., 2024a). However, reference answers are of limited use in open-\n",
      "ended queries, while hand-crafted rubrics face a trade-off between labor cost, generalizability, and\n",
      "accuracy. Therefore, here we follow WILDBENCH (Lin et al., 2025) to create a instance-specific\n",
      "checklist, guiding judge LLMs in evaluation. The checklist items are expected to have the follow-\n",
      "ing characteristics: 1) Relevance to the topic of the query. 2) Capability to effectively distinguish\n",
      "between different responses. 3) Independence from each other, as independent questions can ensure\n",
      "a complete evaluation of the response’s quality. Essentially, checklist questions can be considered\n",
      "as a distillation of knowledge from powerful LLMs to prompt the lightweight judges. A question\n",
      "like \"Does the response include the correct reasoning steps/final answer as X?\" can be helpful when\n",
      "lightweight LLMs are struggling to identify all key factors or solve the problem by itself. We em-\n",
      "ploy GPT-4o as the checklist creator, with 5-10 questions created for each instance. This process\n",
      "only needs to be executed once, while the checklist can be leveraged by lightweight LLM judge to\n",
      "evaluate any number of responses. The prompts used are shown in Appendix A.1.\n",
      "\n",
      "3.2 CHECKLIST GRADING\n",
      "\n",
      "In Section 2.3, we examine the limitations of employing lightweight LLMs as judges, particularly\n",
      "the issues of high uncertainty and positional bias. To address these challenges, we propose the\n",
      "following two evaluation procedures when using lightweight LLMs as the judge.\n",
      "\n",
      "Independent Checklist Item Judgment. To avoid interference from the judgments on other check-\n",
      "list questions, we prompt the LLM judge to evaluate each question in the checklist independently.\n",
      "While this method requires multiple queries per instance, the computational cost can be significantly\n",
      "reduced by leveraging prefix caching (Zheng et al., 2024) since they share the same prefix.\n",
      "\n",
      "Normalized Score. Given the high uncertainty associated with lightweight LLMs, relying solely\n",
      "on binary outcomes such as \"Yes\" or \"No\" can result in significant errors in the final judgment. To\n",
      "mitigate this error, we introduce the conditional normalized score as the basis for judgment, which\n",
      "accounts for the certainty of the result. Assuming the probability of output token t from judge\n",
      "LLM parameterized by θ with context x is pθ(t|x), with the target query with context x and the\n",
      "corresponding response y. Then the conditional normalized score on checklist item c is defined as:\n",
      "\n",
      "p̂(x,y, c) =\n",
      "pθ(Yes|x,y, c)\n",
      "\n",
      "pθ(Yes|x,y, c) + pθ(No|x,y, c)\n",
      ". (1)\n",
      "\n",
      "In this manner, the judgments with low certainty have less significant impact on the final judgment.\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "3.3 SCORE PREDICTION\n",
      "\n",
      "After obtaining all judgments for a single query-response instance, we can predict the final score\n",
      "by the normalized scores of all checklist items. Given the checklist c = [c1, c2, ..., cN ] ∈ C and\n",
      "the normalized scores p = [p̂(x,y, c1), p̂(x,y, c2), ..., p̂(x,y, cN )] ∈ P . The score predictor f :\n",
      "S → R can predict the score s by s = f(p). This methodology ensures a more reliable and accurate\n",
      "evaluation by addressing the inherent uncertainties and biases in lightweight LLM judgments. The\n",
      "score predictor can be any statistical method, hand-crafted rules, or machine learning models. For\n",
      "simplicity, here we use the arithmetic mean of all normalized scores as the score sunsup:\n",
      "\n",
      "sunsup =\n",
      "\n",
      "N∑\n",
      "i=1\n",
      "\n",
      "p̂(x,y, ci). (2)\n",
      "\n",
      "This method does not require additional data or effort to obtain the predictor. However, checklist\n",
      "items may have varying impacts on the final results. In many LLM evaluation scenarios, such as\n",
      "LLM development, the benchmark data often comes with annotations from humans and powerful\n",
      "LLMs that serve as the gold standard for evaluation. In this context, we can utilize these annotations\n",
      "to align our checklist judgment results with supervised learning. Specifically, we consider the judg-\n",
      "ments on the checklist items as features and the annotations as labels. Given a judgment set with\n",
      "N samples, with features P ∈ PN and labels r ∈ RN , a predictor fsup = minθL(θ;P, r) can\n",
      "be derived by minimizing any loss function L. The predictor can be any classification or regression\n",
      "model, depending on the type of annotations. In this case, we select the Extremely Randomized\n",
      "Tree (Geurts et al., 2006) as estimator to learn a robust ensemble predictor with a limited number of\n",
      "annotations and unknown distributions of the judgment results.\n",
      "\n",
      "Meanwhile, we have observed that queries may not always yield ideal separable annotations for\n",
      "predictor learning. Some queries may be too easy, too hard, or have vague descriptions, resulting\n",
      "in similar good or bad performance across all test models. This can cause significant performance\n",
      "degradation when learning a supervised predictor. Therefore, we propose a strategy to adjust the im-\n",
      "pact of the scores predicted by the supervised predictor based on the distribution of the annotations.\n",
      "Specifically, given the annotations r ∈ R|P|, we define the weight factor αr as\n",
      "\n",
      "αr =\n",
      "ϵ−KL(Pr∥Pideal)\n",
      "\n",
      "ϵ\n",
      ", ϵ = max\n",
      "\n",
      "X∼RN\n",
      "KL(X∥Pideal), (3)\n",
      "\n",
      "where ϵ is the maximum Kullback–Leibler (KL) divergence of any distribution X from the ideal\n",
      "distribution Pideal. In existing work (Kim et al., 2024a; Murugadoss et al., 2024), rubrics have\n",
      "been used as a reference in evaluation, including examples or standards for different rating levels.\n",
      "Therefore, we expect the annotated scores to be varied at different levels, providing the rubrics for\n",
      "the predictor. Hence, we use the uniform distribution across the score range (for example, 1-10) as\n",
      "Pideal ∼ UN .\n",
      "\n",
      "After deriving the weight factor αr and a fitted predictor fsup for each query, the final score assigned\n",
      "to the corresponding response is\n",
      "\n",
      "ssup = (1− αr)sunsup + αrfsup(p). (4)\n",
      "\n",
      "This methodology ensures a more reliable and accurate evaluation by addressing the inherent uncer-\n",
      "tainties and biases in the supervised learning process.\n",
      "\n",
      "4 EXPERIMENTS\n",
      "\n",
      "In this section, we evaluate the effectiveness of the RocketEval framework for automated evaluation\n",
      "of LLMs. Initially, we analyze how well RocketEval aligns with human preferences at both the in-\n",
      "stance and the list levels. Next, we investigate the expenses associated with the evaluation procedure.\n",
      "Lastly, we perform an analysis of the checklist’s content and its impact on the evaluation.\n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "4.1 HUMAN AGREEMENT ON THE EVALUATION\n",
      "\n",
      "The initial step involves comparing the proposed RocketEval framework with the traditional evalu-\n",
      "ation method, both with and without the inclusion of Chain of Thought (CoT). The idea of adapting\n",
      "checklist or aspect to enhance the robustness in LLM evaluation has been widely adopted in existing\n",
      "works (Lee et al., 2024; Zhou et al., 2024; Fu et al., 2024), where most of them come with a fixed\n",
      "human-curated lists to evaluate responses. To further validate the impact of the instance-level check-\n",
      "list, we introduce a baseline employing six fixed questions as the checklist. These questions, derived\n",
      "by analyzing the MT-BENCH (Zheng et al., 2023) evaluation prompt, encompass dimensions such\n",
      "as helpfulness, relevance, accuracy, depth, creativity, and detail. This baseline is henceforth referred\n",
      "to as the \"Fixed\". Details of the experimental setup are provided in Appendix A.3.\n",
      "\n",
      "Instance-level Agreement. To measure the agreement ratio of different automated evaluation meth-\n",
      "ods with MT-BENCH HUMAN JUDGMENTS, we adhere to the settings outlined in Section 2.2. As\n",
      "illustrated in Table 2, the proposed method consistently enhances agreement with human judgment\n",
      "across various LLMs acting as evaluators. Notably, the latest 7-12B parameter lightweight LLMs,\n",
      "such as Llama-3-8B and Mistral-Nemo, achieve over 64% agreement with human judgments and at-\n",
      "taining a similar level of agreement as 70B open-source LLMs and human-to-human agreement. For\n",
      "smaller-sized LLMs, which initially exhibit agreement accuracy close to random choice, RocketE-\n",
      "val significantly improves performance to over 60%, outperforming GPT-4. Conversely, the fixed\n",
      "checklist shows no notable performance enhancement and may even degrade performance. This\n",
      "suggests that checklist questions tailored to the specific query topic can better provide a knowledge\n",
      "context, thereby enhancing the performance of lightweight LLMs as judges.\n",
      "\n",
      "Table 2: Agreement ratios of different LLM judges with\n",
      "MT-BENCH HUMAN JUDGMENTS.\n",
      "\n",
      "Method CoT Direct Fixed Ours (Unsup.) Ours (Sup.)\n",
      "\n",
      "Baseline\n",
      "GPT-4 (Pairwise): 65.8% GPT-4 (Single): 59.6%\n",
      "Human-to-human: 64.7% GPT-4o: 66.6%\n",
      "Prometheus-7B-v2.0: 55.7%\n",
      "\n",
      "Llama-3-70B 60.4% 64.8% - - -\n",
      "Qwen2-72B 63.6% 64.0% - - -\n",
      "\n",
      "Mistral-Nemo 57.8% 53.0% 53.1% 63.2% 64.2%\n",
      "Llama-3-8B 55.6% 51.4% 46.9% 63.8% 62.9%\n",
      "Qwen2-7B 54.7% 47.6% 42.1% 58.6% 59.8%\n",
      "Mistral-7B-v0.3 55.3% 54.4% 42.8% 58.8% 58.3%\n",
      "\n",
      "Phi-3-mini-4k 52.8% 33.8% 42.1% 61.2% 60.9%\n",
      "Qwen2.5-3B 49.8% 52.2% 35.6% 57.4% 58.7%\n",
      "Llama-3.2-3B 53.2% 26.6% 54.1% 58.6% 58.8%\n",
      "Gemma-2-2B 37.9% 39.2% 43.9% 57.9% 57.3%\n",
      "InternLM2.5-1.8B 29.2% 22.3% 38.4% 51.7% 48.4%\n",
      "\n",
      "Qwen2.5-1.5B 31.1% 25.1% 46.5% 60.7% 60.2%\n",
      "Qwen2-1.5B 30.4% 21.7% 41.0% 56.2% 55.6%\n",
      "Llama-3.2-1B 32.6% 22.5% 36.5% 33.6% 41.9%\n",
      "Qwen2.5-0.5B 24.9% 25.1% 40.8% 54.3% 50.9%\n",
      "\n",
      "M\n",
      "ist\n",
      "\n",
      "ra\n",
      "l-N\n",
      "\n",
      "em\n",
      "o\n",
      "\n",
      "Ll\n",
      "am\n",
      "\n",
      "a-\n",
      "3-\n",
      "\n",
      "8B\n",
      "Ph\n",
      "\n",
      "i-3\n",
      "-m\n",
      "\n",
      "in\n",
      "i-4\n",
      "\n",
      "k\n",
      "Q\n",
      "\n",
      "w\n",
      "en\n",
      "\n",
      "2.\n",
      "5-\n",
      "\n",
      "3B\n",
      "Ll\n",
      "\n",
      "am\n",
      "a-\n",
      "\n",
      "3.\n",
      "2-\n",
      "\n",
      "3B\n",
      "Ge\n",
      "\n",
      "m\n",
      "m\n",
      "\n",
      "a-\n",
      "2-\n",
      "\n",
      "2B\n",
      "\n",
      "In\n",
      "te\n",
      "\n",
      "rn\n",
      "LM\n",
      "\n",
      "2.\n",
      "5-\n",
      "\n",
      "1.\n",
      "8B\n",
      "\n",
      "Q\n",
      "w\n",
      "\n",
      "en\n",
      "2.\n",
      "\n",
      "5-\n",
      "1.\n",
      "\n",
      "5B\n",
      "Ll\n",
      "\n",
      "am\n",
      "a-\n",
      "\n",
      "3.\n",
      "2-\n",
      "\n",
      "1B\n",
      "Q\n",
      "\n",
      "w\n",
      "en\n",
      "\n",
      "2.\n",
      "5-\n",
      "\n",
      "0.\n",
      "5B\n",
      "\n",
      "Mistral-Nemo\n",
      "\n",
      "Llama-3-8B\n",
      "\n",
      "Phi-3-mini-4k\n",
      "\n",
      "Qwen2.5-3B\n",
      "\n",
      "Llama-3.2-3B\n",
      "\n",
      "Gemma-2-2B\n",
      "\n",
      "InternLM2.5-1.8B\n",
      "\n",
      "Qwen2.5-1.5B\n",
      "\n",
      "Llama-3.2-1B\n",
      "\n",
      "Qwen2.5-0.5B\n",
      "\n",
      "99 97 93 95 97 81 93 83 86\n",
      "\n",
      "93 96 93 95 97 83 93 79 85\n",
      "\n",
      "62 50 93 95 93 88 93 88 83\n",
      "\n",
      "72 69 31 97 93 75 100 74 92\n",
      "\n",
      "81 76 40 69 93 83 97 76 87\n",
      "\n",
      "83 72 86 46 65 79 93 81 83\n",
      "\n",
      "6 19 23 35 24 4 75 79 61\n",
      "\n",
      "46 50 77 28 53 71 51 74 92\n",
      "\n",
      "5 -4 -1 32 24 6 0 9 67\n",
      "\n",
      "-38 -36 -19 -25 -9 -23 -6 8 30\n",
      "\n",
      "CoT\n",
      "\n",
      "RocketEval (Sup.)\n",
      "\n",
      "Figure 6: Spearman correlation\n",
      "(in percentage) of test model\n",
      "rankings on WILDBENCH across\n",
      "different LLM judges.\n",
      "\n",
      "List-level Correlation. A critical objective of LLM evaluation is to compare the performance of\n",
      "LLM in specific or general scenarios, which makes the final ranking of LLM essential. Therefore,\n",
      "we conduct the experiment to compare the LLM rankings derived from different approaches. The de-\n",
      "tailed setup and results are elaborated in Appendix A.3. Table 3 presents the correlation coefficients\n",
      "of the score rankings on WILDBENCH with the CHATBOT ARENA ELO RATING (Hard Prompt - En-\n",
      "glish). The table clearly indicates that RocketEval significantly improves the quality of by achieving\n",
      "a higher correlation with human annotations. Specifically, when Mistral-Nemo is used as a judge, the\n",
      "Spearman correlation reaches 0.986, surpassing GPT-4o. The smaller Gemma-2-2B also achieves\n",
      "a correlation of 0.965, surpassing Qwen2-72B and the fine-tuned judge model Prometheus-7B-v2.0\n",
      "(Kim et al., 2024b). Furthermore, the supervised version of RocketEval achieves higher correlations\n",
      "compared to the unsupervised version. Compared to instance-level agreement, the supervised score\n",
      "data provides more significant improvements in list-level correlation. In addition to the agreement\n",
      "with humans, we compare the correlations between different lightweight LLM judges. As shown in\n",
      "Figure 6, RocketEval brings significant improvements in cross-judge correlation, especially on the\n",
      "lightweight LLMs. It suggests RocketEval exhibits high consistency and reliability in evaluation.\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Table 3: Correlation of ranking with CHATBOT ARENA ELO RATING (Hard prompts-English) on\n",
      "WILDBENCH dataset. \"Kend.\" and \"Spea.\" are denoted as Kendall’s Tau and Spearman correlation\n",
      "coefficient respectively.\n",
      "\n",
      "Method CoT Direct Fixed Ours (Unsup.) Ours (Sup.)\n",
      "Coefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\n",
      "\n",
      "GPT-4o 0.909 0.979 - - - - - - - -\n",
      "Prometheus-7B-v2.0 0.848 0.949 - - - - - - - -\n",
      "Llama-3-70B 0.909 0.979 0.787 0.923 - - - - - -\n",
      "Qwen2-72B 0.848 0.958 0.848 0.958 - - - - - -\n",
      "\n",
      "Mistral-Nemo 0.870 0.956 0.788 0.909 0.879 0.958 0.939 0.986 0.939 0.986\n",
      "Llama-3-8B 0.818 0.923 0.848 0.944 0.879 0.972 0.909 0.979 0.909 0.979\n",
      "Qwen2-7B 0.788 0.909 0.545 0.727 0.636 0.804 0.758 0.895 0.818 0.930\n",
      "Mistral-7B-v0.3 0.727 0.895 0.545 0.699 0.515 0.678 0.758 0.874 0.818 0.930\n",
      "Phi-3-mini-4k 0.424 0.587 0.576 0.762 0.182 0.273 0.788 0.916 0.848 0.958\n",
      "Qwen2.5-3B 0.697 0.839 0.848 0.951 0.727 0.895 0.848 0.944 0.848 0.944\n",
      "Llama-3.2-3B 0.606 0.797 0.848 0.951 0.818 0.937 0.848 0.944 0.848 0.944\n",
      "Gemma-2-2B 0.636 0.818 0.758 0.888 0.727 0.867 0.879 0.965 0.879 0.965\n",
      "InternLM2.5-1.8B 0.121 0.175 0.273 0.357 0.273 0.427 0.576 0.748 0.606 0.769\n",
      "Qwen2.5-1.5B 0.394 0.517 0.273 0.364 0.606 0.727 0.818 0.923 0.848 0.944\n",
      "Qwen2-1.5B -0.061 0.035 0.303 0.420 -0.061 -0.014 0.455 0.622 0.667 0.825\n",
      "Llama-3.2-1B 0.091 0.133 0.152 0.182 -0.273 -0.357 -0.273 -0.357 0.697 0.846\n",
      "Qwen2.5-0.5B -0.212 -0.315 0.424 0.503 0.394 0.587 0.667 0.811 0.758 0.895\n",
      "\n",
      "4.2 EVALUATION COST ESTIMATION\n",
      "\n",
      "In this section, we analyze the costs of various evaluation methods when different LLMs serve as\n",
      "judges. For the purpose of this analysis, we assume that all responses required for evaluation are\n",
      "pre-generated, thereby excluding the inference costs for generating these responses.\n",
      "\n",
      "In RocketEval, the evaluation process is comprised of two primary components: checklist genera-\n",
      "tion and checklist grading. The supervised evaluation method incorporates an additional fitting and\n",
      "prediction process, whose costs are minimal since no LLM inference is involved. For proprietary\n",
      "LLMs serving as judges, we calculate the number of input and output tokens required for a single\n",
      "evaluation and derive the cost based on the official pricing 2. For open-source LLMs, we deploy\n",
      "them on NVIDIA RTX A5000 GPUs using vLLM (Kwon et al., 2023), and the cost is calculated\n",
      "based on the average execution time and the rental price of the GPU. We reference the pricing on\n",
      "RunPod 3 at $0.36 per hour. To maximize efficiency, experiments are conducted in batch mode.\n",
      "\n",
      "In practical evaluation scenarios, various LLMs, each with distinct tuning options, decoding settings,\n",
      "and prompting techniques, can yield numerous versions of responses. Consequently, evaluations on\n",
      "the same benchmark can be performed hundreds or even thousands of times across different models\n",
      "and their respective versions. We therefore compare the cost of LLM evaluation methods with\n",
      "different number of tests N . As shown in Table 4, the cost incurred for generating a checklist for\n",
      "each question is equivalent to the expense of running a single test using GPT-4o. Since the checklist\n",
      "generation is a one-time process, the cost of the checklist grading process becomes increasingly\n",
      "significant as the number of tests escalates. For instance, conducting 1000 tests on the WILDBENCH\n",
      "would incur a cost of $3400 when using GPT-4o as the evaluator, whereas employing Llama-3-8B\n",
      "would only require $71, with achieving a higher correlation with human preferences.\n",
      "4.3 QUALITATIVE ANALYSIS\n",
      "\n",
      "Checklist Statistical Analysis. To discern the distribution of checklist items, we extracted the re-\n",
      "lationships within the knowledge graphs associated with each checklist item, including the subject,\n",
      "predicate, and object, in a format consistent with the methodology introduced by Sun et al. (2024).\n",
      "We conducted a statistical analysis of all predicates in the checklist items generated from WILD-\n",
      "BENCH (all predicates have been lemmatized). Further statistics are provided in Appendix A.5. As\n",
      "shown in Figure 7, the predicate distribution within checklist items reflects the intrinsic demands\n",
      "of various original questions. Predominant predicates such as \"Include\" and \"Provide\" underscore\n",
      "the necessity for comprehensive and supportive responses, ensuring that all pertinent information is\n",
      "considered. This is crucial for addressing the complex and multifaceted nature of questions across\n",
      "various general tasks. Meanwhile, predicates such as \"Calculate\" and \"Specify\" highlight the pre-\n",
      "\n",
      "2https://openai.com/api/pricing/\n",
      "3https://www.runpod.io/pricing\n",
      "\n",
      "9\n",
      "\n",
      "https://openai.com/api/pricing/\n",
      "https://www.runpod.io/pricing\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Table 4: Evaluation cost on WILDBENCH with different LLM judges.\n",
      "\n",
      "Method LLM Judge Deploying\n",
      "Environment Price Usage for\n",
      "\n",
      "Single Test\n",
      "Extra\n",
      "Cost\n",
      "\n",
      "Total Cost for N Tests\n",
      "\n",
      "N=10 N=100 N=1000\n",
      "\n",
      "CoT\n",
      "GPT-4o\n",
      "(20240806) proprietary\n",
      "\n",
      "I/O: $1.25 / $5.00\n",
      "/ 1M Tokens I/O:\n",
      "\n",
      "1.84M/220k\n",
      "tokens\n",
      "\n",
      "N/A\n",
      "$34.0 $340 $3400\n",
      "\n",
      "GPT-4o-mini\n",
      "(20240718)\n",
      "\n",
      "I/O: $0.075 / $0.30\n",
      "/ 1M Tokens $2.00 $20.0 $200\n",
      "\n",
      "RocketEval\n",
      "Llama-3-70BAWQ 4 x A5000 $1.44 / hours 3760s\n",
      "\n",
      "$2.87*\n",
      "$15.1 $125 $1224\n",
      "\n",
      "Llama-3-8B 1 x A5000 $0.36 / hours 685s $3.55 $9.72 $71.4\n",
      "Gemma-2-2B 1 x A5000 $0.36 / hours 248s $3.12 $5.35 $27.7\n",
      "Qwen2.5-1.5B 1 x A5000 $0.36 / hours 165s $3.04 $4.52 $19.4\n",
      "\n",
      "*The checklist generation process on WILDBENCH consumes 1.38M input tokens and 228k output tokens on GPT-4o.\n",
      "\n",
      "cision required in quantitative and advisory responses. This distribution pattern not only guides the\n",
      "assessment of answer quality but also ensures that responses meet the specific criteria of each ques-\n",
      "tion type, thereby enhancing the overall reliability and applicability of the information conveyed.\n",
      "\n",
      "Checklist Item Reweighting. Adjusting the weights of checklist items is crucial for accurately\n",
      "assessing the effectiveness of responses to the original question, especially when a gold standard is\n",
      "available. Initially, each checklist item was given equal weight, assuming an equal impact on the\n",
      "overall assessment. However, this does not accurately reflect the true importance of each item in\n",
      "validating the response’s accuracy and completeness. As shown in Figure 8, Assigning the value\n",
      "of radial stress in item 1 (0.296) is critical, being a core part of the answer, significantly impacts\n",
      "subsequent calculations and the overall analysis. In contrast, item 3 (0.069), which calculates the\n",
      "inner radius, is fundamental but simple, less prone to error, and less critical than stress calculations,\n",
      "thus bearing a lower weight. By reweighting, we emphasize the steps that are the most crucial to\n",
      "the accurate responses, ensuring that the evaluation process is both rigorous and reflective of the\n",
      "actual importance of each component, which leads to more reliable and credible results, ultimately\n",
      "enhancing the overall validity and consistency with the gold standard of the assessment.\n",
      "\n",
      "15%\n",
      "15%\n",
      "\n",
      "9%\n",
      "\n",
      "8%\n",
      "\n",
      "6%\n",
      "5%\n",
      "\n",
      "5% 5% 3%\n",
      "3%\n",
      "\n",
      "3%\n",
      "3%\n",
      "2%\n",
      "\n",
      "18%\n",
      "\n",
      "Include\n",
      "Provide\n",
      "\n",
      "Explain\n",
      "\n",
      "Mention\n",
      "\n",
      "Describe\n",
      "\n",
      "Identify\n",
      "Address Discuss\n",
      "\n",
      "Maintain\n",
      "Use\n",
      "Suggest\n",
      "Calculate\n",
      "Specify\n",
      "\n",
      "Others\n",
      "\n",
      "Figure 7: The predicates used in\n",
      "checklist items on WILDBENCH.\n",
      "\n",
      "1\n",
      " Does  the response  calculate  the radial stress ( r)\n",
      "\n",
      "using the correct formula \\( \\sigma_r = -P \\) and provide\n",
      "the correct value of -5 MPa?\n",
      "\n",
      "2\n",
      " Does  the  response  calculate  the tangential stress\n",
      "\n",
      "( t) using the correct formula \\( \\sigma_t = \\frac{Pr}{t}\n",
      "\\) and provide the correct value of 375 MPa?\n",
      "\n",
      "3 Does  the response  correctly identify  and use the inner\n",
      "radius as 150 mm (half of the diameter)?\n",
      "\n",
      "4 Does the  response  apply the principle correctly to find\n",
      "the tangential strain ( t)?\n",
      "\n",
      "5\n",
      " Does the response include the correct Poisson's ratio ( )\n",
      "\n",
      "and modulus of elasticity (E) in relevant strain\n",
      "calculations?\n",
      "\n",
      "6\n",
      " Does the response mention the necessary calculation for\n",
      "\n",
      "the initial volume of the cylinder \\( V_0 \\) in terms of\n",
      "the unspecified height (h)?\n",
      "\n",
      "7\n",
      "\n",
      " Does the response correctly calculate the volumetric\n",
      "strain ( v) by  summing individual strains and use that to\n",
      "\n",
      "express the change in volume ( V) in reference to the\n",
      "initial volume?\n",
      "\n",
      " A thin-walled cylinder has a diameter\n",
      "of 300 mm and a wall thickness of 2 mm.\n",
      "The cylinder is subjected to an internal\n",
      "pressure of 5 MPa. The material of the\n",
      "cylinder has a Poisson's ratio (v) of\n",
      "0.3 and a modulus of elasticity of 200\n",
      "GPa. Determine:\n",
      "a. The radial stress and tangential\n",
      "stress in the cylinder.\n",
      "b. The volumetric strain and the change\n",
      "in volume of the cylinder due to the\n",
      "internal pressure\n",
      "\n",
      "0.296\n",
      "\n",
      "0.272\n",
      "\n",
      "0.069\n",
      "\n",
      "0.146\n",
      "\n",
      "0.064\n",
      "\n",
      "0.094\n",
      "\n",
      "0.059\n",
      "\n",
      "Figure 8: Visualization of checklist item reweighting. (De-\n",
      "fault weight is the reciprocal of the number of checklists)\n",
      "\n",
      "5 CONCLUSION\n",
      "\n",
      "In this paper, we introduce RocketEval, an innovative evaluation framework that uses lightweight\n",
      "LLMs to achieve high efficiency, low cost, interpretability, and alignment with human preferences.\n",
      "By reframing the evaluation task as a multi-faceted Q&A using instance-specific checklists, we ad-\n",
      "dressed the challenges of high uncertainty and positional bias inherent in lightweight LLMs. Our\n",
      "method demonstrated a high correlation with human preferences, achieving a Spearman correla-\n",
      "tion of 0.965 with the Gemma-2-2B model, comparable to GPT-4o, but at a fraction of the cost.\n",
      "This significant cost reduction makes RocketEval a feasible solution for large-scale evaluation and\n",
      "comparison scenarios.\n",
      "\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "This work was partially supported by the National Natural Science Foundation of China (Project No.\n",
      "62202122 and No. 62073272), the Shenzhen Science and Technology Program under Grant No.\n",
      "GXWD20231130110308001, and the Guangdong Basic and Applied Basic Research Foundation\n",
      "under Grant No. 2024A1515011949.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng\n",
      "Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot\n",
      "arena: an open platform for evaluating llms by human preference. In Proceedings of the 41st\n",
      "International Conference on Machine Learning, ICML’24. JMLR.org, 2024. URL https://dl.\n",
      "acm.org/doi/abs/10.5555/3692070.3692401.\n",
      "\n",
      "Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba,\n",
      "Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simu-\n",
      "lation framework for methods that learn from human feedback. In A. Oh, T. Nau-\n",
      "mann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neu-\n",
      "ral Information Processing Systems, volume 36, pp. 30039–30069. Curran Associates,\n",
      "Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n",
      "5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf.\n",
      "\n",
      "Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open\n",
      "llm leaderboard v2. https://huggingface.co/spaces/open-llm-leaderboard/open_llm_\n",
      "leaderboard, 2024.\n",
      "\n",
      "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. GPTScore: Evaluate as you desire.\n",
      "In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Confer-\n",
      "ence of the North American Chapter of the Association for Computational Linguistics: Human\n",
      "Language Technologies (Volume 1: Long Papers), pp. 6556–6576, Mexico City, Mexico, June\n",
      "2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.365. URL\n",
      "https://aclanthology.org/2024.naacl-long.365/.\n",
      "\n",
      "Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine Learning,\n",
      "63(1):3–42, Apr 2006. ISSN 1573-0565. doi: 10.1007/s10994-006-6226-1. URL https://doi.\n",
      "org/10.1007/s10994-006-6226-1.\n",
      "\n",
      "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\n",
      "Steinhardt. Measuring massive multitask language understanding. In Proceedings of the Interna-\n",
      "tional Conference on Learning Representations (ICLR), 2021.\n",
      "\n",
      "Hui Huang, Yingqi Qu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, and Tiejun Zhao. On the\n",
      "limitations of fine-tuned judge models for llm evaluation, 2024. URL https://arxiv.org/pdf/\n",
      "2403.02839.\n",
      "\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\n",
      "Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gi-\n",
      "anna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-\n",
      "Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le\n",
      "Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\n",
      "Mixtral of experts, 2024a. URL https://arxiv.org/abs/2401.04088.\n",
      "\n",
      "Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. TIGER-\n",
      "Score: Towards building explainable metric for all text generation tasks. Transactions on Ma-\n",
      "chine Learning Research, 2024b. ISSN 2835-8856. URL https://openreview.net/forum?\n",
      "id=EE1CBKC0SZ.\n",
      "\n",
      "Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun,\n",
      "Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-\n",
      "grained evaluation capability in language models. In The Twelfth International Conference on\n",
      "Learning Representations, 2024a. URL https://openreview.net/forum?id=8euJaTveKw.\n",
      "\n",
      "11\n",
      "\n",
      "https://dl.acm.org/doi/abs/10.5555/3692070.3692401\n",
      "https://dl.acm.org/doi/abs/10.5555/3692070.3692401\n",
      "https://proceedings.neurips.cc/paper_files/paper/2023/file/5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf\n",
      "https://proceedings.neurips.cc/paper_files/paper/2023/file/5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf\n",
      "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n",
      "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n",
      "https://aclanthology.org/2024.naacl-long.365/\n",
      "https://doi.org/10.1007/s10994-006-6226-1\n",
      "https://doi.org/10.1007/s10994-006-6226-1\n",
      "https://arxiv.org/pdf/2403.02839\n",
      "https://arxiv.org/pdf/2403.02839\n",
      "https://arxiv.org/abs/2401.04088\n",
      "https://openreview.net/forum?id=EE1CBKC0SZ\n",
      "https://openreview.net/forum?id=EE1CBKC0SZ\n",
      "https://openreview.net/forum?id=8euJaTveKw\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Gra-\n",
      "ham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source\n",
      "language model specialized in evaluating other language models. In Yaser Al-Onaizan, Mohit\n",
      "Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing, pp. 4334–4353, Miami, Florida, USA, November 2024b.\n",
      "Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.248. URL\n",
      "https://aclanthology.org/2024.emnlp-main.248/.\n",
      "\n",
      "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\n",
      "Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\n",
      "serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Princi-\n",
      "ples, SOSP ’23, pp. 611–626, New York, NY, USA, 2023. Association for Computing Machinery.\n",
      "ISBN 9798400702297. doi: 10.1145/3600006.3613165. URL https://doi.org/10.1145/\n",
      "3600006.3613165.\n",
      "\n",
      "Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, and Pilsung Kang. Checkeval: Robust\n",
      "evaluation framework using large language model via checklist, 2024. URL https://arxiv.\n",
      "org/abs/2403.18771.\n",
      "\n",
      "Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gon-\n",
      "zalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and\n",
      "benchbuilder pipeline, 2024a. URL https://arxiv.org/abs/2406.11939.\n",
      "\n",
      "Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang\n",
      "Liu. Split and merge: Aligning position biases in LLM-based evaluators. In Yaser Al-Onaizan,\n",
      "Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical\n",
      "Methods in Natural Language Processing, pp. 11084–11108, Miami, Florida, USA, November\n",
      "2024b. Association for Computational Linguistics. URL https://aclanthology.org/2024.\n",
      "emnlp-main.621.\n",
      "\n",
      "Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\n",
      "Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\n",
      "models. Transactions on Machine Learning Research, 2023. URL https://openreview.net/\n",
      "pdf?id=iO4LZibEqW.\n",
      "\n",
      "Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Abhilasha Ravichander, Valentina Pyatkin, Nouha\n",
      "Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking LLMs with challenging tasks\n",
      "from real users in the wild. In The Thirteenth International Conference on Learning Representa-\n",
      "tions, 2025. URL https://openreview.net/forum?id=MKEHCx25xp.\n",
      "\n",
      "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG\n",
      "evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika\n",
      "Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\n",
      "Processing, pp. 2511–2522, Singapore, December 2023. Association for Computational Lin-\n",
      "guistics. doi: 10.18653/v1/2023.emnlp-main.153. URL https://aclanthology.org/2023.\n",
      "emnlp-main.153.\n",
      "\n",
      "Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catan-\n",
      "zaro. ChatQA: Surpassing GPT-4 on conversational QA and RAG. In The Thirty-eighth Annual\n",
      "Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/\n",
      "forum?id=bkUvKPKafQ.\n",
      "\n",
      "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con-\n",
      "duct electricity? a new dataset for open book question answering. In Ellen Riloff, David\n",
      "Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on\n",
      "Empirical Methods in Natural Language Processing, pp. 2381–2391, Brussels, Belgium, oct\n",
      "- nov 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL\n",
      "https://aclanthology.org/D18-1260.\n",
      "\n",
      "Bhuvanashree Murugadoss, Christian Poelitz, Ian Drosos, Vu Le, Nick McKenna, Carina Suzana\n",
      "Negreanu, Chris Parnin, and Advait Sarkar. Evaluating the evaluator: Measuring llms’ adherence\n",
      "to task evaluation instructions, 2024. URL https://arxiv.org/abs/2408.08781.\n",
      "\n",
      "12\n",
      "\n",
      "https://aclanthology.org/2024.emnlp-main.248/\n",
      "https://doi.org/10.1145/3600006.3613165\n",
      "https://doi.org/10.1145/3600006.3613165\n",
      "https://arxiv.org/abs/2403.18771\n",
      "https://arxiv.org/abs/2403.18771\n",
      "https://arxiv.org/abs/2406.11939\n",
      "https://aclanthology.org/2024.emnlp-main.621\n",
      "https://aclanthology.org/2024.emnlp-main.621\n",
      "https://openreview.net/pdf?id=iO4LZibEqW\n",
      "https://openreview.net/pdf?id=iO4LZibEqW\n",
      "https://openreview.net/forum?id=MKEHCx25xp\n",
      "https://aclanthology.org/2023.emnlp-main.153\n",
      "https://aclanthology.org/2023.emnlp-main.153\n",
      "https://openreview.net/forum?id=bkUvKPKafQ\n",
      "https://openreview.net/forum?id=bkUvKPKafQ\n",
      "https://aclanthology.org/D18-1260\n",
      "https://arxiv.org/abs/2408.08781\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and\n",
      "Yang You. Mixeval: Deriving wisdom of the crowd from LLM benchmark mixtures. In The\n",
      "Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https:\n",
      "//openreview.net/forum?id=6A29LUZhfv.\n",
      "\n",
      "Zhaopeng Qiu, Xian Wu, and Wei Fan. Automatic distractor generation for multiple choice questions\n",
      "in standard tests. In Proceedings of the 28th International Conference on Computational Linguis-\n",
      "tics, pp. 2096–2106, 2020. URL https://aclanthology.org/2020.coling-main.189/.\n",
      "\n",
      "Kai Sun, Yifan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: How knowledgeable\n",
      "are large language models (llms)? aka will llms replace knowledge graphs? In Proceedings\n",
      "of the 2024 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 311–325, 2024. URL\n",
      "https://aclanthology.org/2024.naacl-long.18/.\n",
      "\n",
      "Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhu-\n",
      "patiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Fer-\n",
      "ret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Char-\n",
      "line Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin,\n",
      "Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur,\n",
      "Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchi-\n",
      "son, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge,\n",
      "Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar,\n",
      "Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Wein-\n",
      "berger, Dimple Vijaykumar, Dominika Rogozińska, Dustin Herbison, Elisa Bandy, Emma Wang,\n",
      "Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin,\n",
      "Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen\n",
      "Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway,\n",
      "Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez,\n",
      "Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mo-\n",
      "hamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir\n",
      "Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leti-\n",
      "cia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Mar-\n",
      "tins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth,\n",
      "Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi,\n",
      "Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khat-\n",
      "wani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Os-\n",
      "car Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko\n",
      "Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana,\n",
      "Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah\n",
      "Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth,\n",
      "Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Ko-\n",
      "cisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren\n",
      "Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao\n",
      "Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris\n",
      "Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine\n",
      "Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu,\n",
      "Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen\n",
      "Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at a\n",
      "practical size, 2024. URL https://arxiv.org/abs/2408.00118.\n",
      "\n",
      "Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong,\n",
      "Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Lun-Wei\n",
      "Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers), pp. 9440–9450, Bangkok,\n",
      "Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.\n",
      "acl-long.511. URL https://aclanthology.org/2024.acl-long.511.\n",
      "\n",
      "Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao\n",
      "Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue\n",
      "\n",
      "13\n",
      "\n",
      "https://openreview.net/forum?id=6A29LUZhfv\n",
      "https://openreview.net/forum?id=6A29LUZhfv\n",
      "https://aclanthology.org/2020.coling-main.189/\n",
      "https://aclanthology.org/2024.naacl-long.18/\n",
      "https://arxiv.org/abs/2408.00118\n",
      "https://aclanthology.org/2024.acl-long.511\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Zhang. PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimiza-\n",
      "tion. In The Twelfth International Conference on Learning Representations, 2024b. URL\n",
      "https://openreview.net/forum?id=5Nn2BLV7SB.\n",
      "\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V\n",
      "Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language mod-\n",
      "els. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Ad-\n",
      "vances in Neural Information Processing Systems, volume 35, pp. 24824–24837. Curran As-\n",
      "sociates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/\n",
      "file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.\n",
      "\n",
      "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\n",
      "Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\n",
      "Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jin-\n",
      "gren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin\n",
      "Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao,\n",
      "Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wen-\n",
      "bin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng\n",
      "Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,\n",
      "Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL\n",
      "https://arxiv.org/abs/2407.10671.\n",
      "\n",
      "Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text gen-\n",
      "eration. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.),\n",
      "Advances in Neural Information Processing Systems, volume 34, pp. 27263–27277. Curran As-\n",
      "sociates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/\n",
      "file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf.\n",
      "\n",
      "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\n",
      "chine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association\n",
      "for Computational Linguistics, 2019.\n",
      "\n",
      "Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore:\n",
      "Evaluating text generation with bert. In International Conference on Learning Representations,\n",
      "2020. URL https://openreview.net/forum?id=SkeHuCVFDr.\n",
      "\n",
      "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\n",
      "Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez,\n",
      "and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In A. Oh,\n",
      "T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in\n",
      "Neural Information Processing Systems, volume 36, pp. 46595–46623. Curran Associates,\n",
      "Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n",
      "91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf.\n",
      "\n",
      "Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao,\n",
      "Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. SGLang: Ef-\n",
      "ficient execution of structured language model programs. In The Thirty-eighth Annual Conference\n",
      "on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=\n",
      "VqkAKQibpq.\n",
      "\n",
      "Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F. Wong, Xiaowei Huang,\n",
      "Qiufeng Wang, and Kaizhu Huang. Is your model really a good math reasoner? evaluating\n",
      "mathematical reasoning with checklist, 2024. URL https://arxiv.org/abs/2407.08733.\n",
      "\n",
      "Lianghui Zhu, Xinggang Wang, and Xinlong Wang. JudgeLM: Fine-tuned large language models\n",
      "are scalable judges. In The Thirteenth International Conference on Learning Representations,\n",
      "2025. URL https://openreview.net/forum?id=xsELpEPn4A.\n",
      "\n",
      "14\n",
      "\n",
      "https://openreview.net/forum?id=5Nn2BLV7SB\n",
      "https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\n",
      "https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\n",
      "https://arxiv.org/abs/2407.10671\n",
      "https://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf\n",
      "https://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf\n",
      "https://openreview.net/forum?id=SkeHuCVFDr\n",
      "https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf\n",
      "https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf\n",
      "https://openreview.net/forum?id=VqkAKQibpq\n",
      "https://openreview.net/forum?id=VqkAKQibpq\n",
      "https://arxiv.org/abs/2407.08733\n",
      "https://openreview.net/forum?id=xsELpEPn4A\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "A APPENDIX\n",
      "\n",
      "A.1 PROMPTS USED IN ROCKETEVAL\n",
      "\n",
      "Prompts for Checklist Creation. The following prompt is designed to generate an instance-level\n",
      "checklist.\n",
      "\n",
      "Prompt for Checklist Creation\n",
      "\n",
      "# Instruction\n",
      "You are an helpful assistant who identifies and summarizes key factors in large\n",
      "\n",
      "↪→ language models (LLMs) evaluation to help humans evaluate LLMs\n",
      "↪→ efficiently.\n",
      "\n",
      "Feed any query into different LLMs, I will get various responses. I need to\n",
      "↪→ know in quick whether these responses follows the instructions and\n",
      "↪→ answers the question in the user query better.\n",
      "\n",
      "I’ll provide you with a user query. Your task is to identify those key factors\n",
      "↪→ that will affect my judgment and summarize them into a list to improve\n",
      "↪→ the efficiency of my evaluation.\n",
      "\n",
      "# Conversation between User and AI\n",
      "<|begin_of_history|>\n",
      "\n",
      "{history}\n",
      "\n",
      "<|end_of_history|>\n",
      "\n",
      "## Current User Query\n",
      "<|begin_of_query|>\n",
      "\n",
      "{user_query}\n",
      "\n",
      "<|end_of_query|>\n",
      "\n",
      "## Reference Response\n",
      "<|begin_of_reference_response|>\n",
      "\n",
      "{reference_response}\n",
      "\n",
      "<|end_of_reference_response|>\n",
      "\n",
      "# Task\n",
      "Given the above information, I need you to create a binary question list, so\n",
      "\n",
      "↪→ that I can perform an efficient and accurate evaluation through\n",
      "↪→ answering several questions.\n",
      "\n",
      "Your question should be concise and include any necessary key content and\n",
      "↪→ information (such as keywords, formats, correct counts and values) in\n",
      "↪→ the user query or expected to be shown in responses. Your questions\n",
      "↪→ should not only consider evaluating the reference response, but all\n",
      "↪→ possible responses. Avoid creating duplicate, cumbersome or vague\n",
      "↪→ questions. For example, you should ask \"Is this response contain the\n",
      "↪→ correct answer ...\" instead of \"Is this response’s answer correct?\". Ask\n",
      "↪→ fewer questions by aggregating questions with repeated contexts into\n",
      "↪→ one question.\n",
      "\n",
      "## Output Format\n",
      "Please provide your outputs in the following markdown format by filling in the\n",
      "\n",
      "↪→ placeholders in {{}}:\n",
      "‘‘‘\n",
      "\n",
      "15\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "1. {{question1}}\n",
      "2. {{question2}}\n",
      "...\n",
      "‘‘‘\n",
      "\n",
      "Prompts for Response Judgment. To keep the consistency with the previous work, we adopt the\n",
      "prompt in Lin et al. (2025) and make several modifications to adapt to different settings.\n",
      "\n",
      "• For MT-BENCH, to keep consistent with the original paper which utilizes the reference\n",
      "answer to guide the judgment, we replace the part of the checklist with the reference answer\n",
      "and modify the prompt correspondingly.\n",
      "\n",
      "• For other benchmarks that utilize pairwise comparison, we use the same prompt as WILD-\n",
      "BENCH, and create the checklist using GPT-4o for each instance in those benchmarks.\n",
      "\n",
      "• When prompting LLM to output the score directly without conducting analysis, we modify\n",
      "the output format part in the prompt and change the score range from 1-10 to 0-9. This\n",
      "aims to avoid the ambiguity of scoring 1 or 10 when only the first token is captured.\n",
      "\n",
      "The prompts with different versions of the modification are listed as follows.\n",
      "\n",
      "Prompts for Response Judgment\n",
      "\n",
      "# Instruction\n",
      "You are an expert evaluator. Your task is to evaluate the quality of the\n",
      "\n",
      "↪→ responses generated by AI models.\n",
      "We will provide you with the user query and an AI-generated responses.\n",
      "You should first read the user query and the conversation history carefully for\n",
      "\n",
      "↪→ analyzing the task, and then evaluate the quality of the responses\n",
      "↪→ based on and rules provided below.\n",
      "\n",
      "# Conversation between User and AI\n",
      "## History\n",
      "<|begin_of_history|>\n",
      "{history}\n",
      "<|end_of_history|>\n",
      "## Current User Query\n",
      "<|begin_of_query|>\n",
      "{user_query}\n",
      "<|end_of_query|>\n",
      "\n",
      "Checklist disabled\n",
      "\n",
      "## Reference Response\n",
      "<|begin_of_reference_response|>\n",
      "{ref_answer}\n",
      "<|end_of_reference_response|>\n",
      "\n",
      "## AI Response\n",
      "<|begin_of_response|>\n",
      "{model_output}\n",
      "<|end_of_response|>\n",
      "\n",
      "# Evaluation\n",
      "\n",
      "16\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Checklist disabled\n",
      "\n",
      "## Rules\n",
      "You should first compare the AI response and reference response based on\n",
      "\n",
      "↪→ your analysis of the user queries and the conversation history,\n",
      "↪→ and then provide your assessment by scoring the AI response.\n",
      "\n",
      "Checklist enabled\n",
      "\n",
      "## Checklist\n",
      "<|begin_of_checklist|>\n",
      "{checklist}\n",
      "<|end_of_checklist|>\n",
      "Please use this checklist to guide your evaluation, but do not limit your\n",
      "\n",
      "↪→ assessment to the checklist.\n",
      "## Rules\n",
      "You should compare the above response based on your analysis of the user\n",
      "\n",
      "↪→ queries and the conversation history.\n",
      "You should first write down your analysis and the checklist that you used\n",
      "\n",
      "↪→ for the evaluation, and then provide your assessment according to\n",
      "↪→ the checklist.\n",
      "\n",
      "CoT enabled\n",
      "\n",
      "The scores are in the range of 1~10, where 1 means the response is very\n",
      "↪→ poor and 10 means the response is perfect.\n",
      "\n",
      "Here are more detailed criteria for the scores:\n",
      "\n",
      "- Score 1~2: The response is very poor and does not make sense at all.\n",
      "- Score 3~4: The response is poor and does help user solve the problem in\n",
      "\n",
      "↪→ a meaningful way.\n",
      "- Score 5~6: The response is fair but has some issues (e.g., factual\n",
      "\n",
      "↪→ errors, hallucinations, missing key information).\n",
      "- Score 7~8: The response is good enough but could be improved in some\n",
      "\n",
      "↪→ ways.\n",
      "- Score 9~10: The response is perfect and provides helpful information\n",
      "\n",
      "↪→ that can help user solve the problem.\n",
      "\n",
      "## Output Format\n",
      "First, please output your analysis for the model response, and then\n",
      "\n",
      "↪→ summarize your assessment to two aspects: \"strengths\" and \"\n",
      "↪→ weaknesses\"; Finally, please write down your rating for the\n",
      "↪→ assessment.\n",
      "\n",
      "Please provide your evaluation results in the following json format by\n",
      "↪→ filling in the placeholders in []:\n",
      "\n",
      "‘‘‘\n",
      "{\n",
      "\n",
      "\"strengths\": \"[analysis for the strengths of the response]\",\n",
      "\"weaknesses\": \"[analysis for the weaknesses of the response]\",\n",
      "\"score\": \"[1~10]\"\n",
      "\n",
      "}\n",
      "‘‘‘\n",
      "\n",
      "17\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "CoT disabled\n",
      "\n",
      "The scores are in the range of 0~9, where 0 means the response is very\n",
      "↪→ poor and 9 means the response is perfect.\n",
      "\n",
      "Here are more detailed criteria for the scores:\n",
      "\n",
      "- Score 0~1: The response is very poor and does not make sense at all.\n",
      "- Score 2~3: The response is poor and does help user solve the problem in\n",
      "\n",
      "↪→ a meaningful way.\n",
      "- Score 4~5: The response is fair but has some issues (e.g., factual\n",
      "\n",
      "↪→ errors, hallucinations, missing key information).\n",
      "- Score 6~7: The response is good enough but could be improved in some\n",
      "\n",
      "↪→ ways.\n",
      "- Score 8~9: The response is perfect and provides helpful information\n",
      "\n",
      "↪→ that can help user solve the problem.\n",
      "\n",
      "## Output Format\n",
      "Please output the score directly as a digit from 0-9. Do not output other\n",
      "\n",
      "↪→ text.\n",
      "Your score:\n",
      "\n",
      "Prompts for Checklist Grading. The prompt for grading checklist items is shown below.\n",
      "\n",
      "Prompts for Response Judgment\n",
      "\n",
      "# Instruction\n",
      "\n",
      "You are an expert evaluator. Your task is to evaluate the quality of the\n",
      "↪→ responses generated by AI models.\n",
      "\n",
      "We will provide you with the user query and an AI-generated responses.\n",
      "You should first read the user query and the conversation history carefully for\n",
      "\n",
      "↪→ analyzing the task, and then evaluate the quality of the responses by\n",
      "↪→ answer the question provided below.\n",
      "\n",
      "# Conversation between User and AI\n",
      "\n",
      "## History\n",
      "<|begin_of_history|>\n",
      "\n",
      "{history}\n",
      "\n",
      "<|end_of_history|>\n",
      "\n",
      "## Current User Query\n",
      "<|begin_of_query|>\n",
      "\n",
      "{user_query}\n",
      "\n",
      "<|end_of_query|>\n",
      "\n",
      "## AI Response\n",
      "<|begin_of_response|>\n",
      "\n",
      "{$model_output}\n",
      "\n",
      "<|end_of_response|>\n",
      "\n",
      "# Evaluation\n",
      "\n",
      "18\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "## Question\n",
      "<|begin_of_question|>\n",
      "\n",
      "{checklist_item}\n",
      "\n",
      "<|end_of_question|>\n",
      "\n",
      "Please answer the given question based on the conversation history and the AI\n",
      "↪→ response. You can only answer ’Yes’ or ’No’.\n",
      "\n",
      "Your answer (Yes/No):\n",
      "\n",
      "A.2 DATASETS AND BASELINES\n",
      "\n",
      "In addition to the results reported by Zheng et al. (2023) on instance-level agreement and Lin et al.\n",
      "(2025) on list-level correlation, we add more baselines with the following setup:\n",
      "\n",
      "• GPT-4o: We replace GPT-4 with GPT-4o as judge and rerun the experiments using the\n",
      "publicly available code provided by Zheng et al. (2023). For WILDBENCH, we use the\n",
      "judgment of GPT-4o provided by Lin et al. (2025) as the baseline results. All baseline\n",
      "results are produced following the template of scoring from WILDBENCH to keep the con-\n",
      "sistency.\n",
      "\n",
      "• Prometheus-2: We add the state-of-the-art fine-tuned judge model Prometheus-7B-v2.0\n",
      "(Kim et al., 2024b) as the baseline. This model introduces custom scoring criteria and\n",
      "rubrics to conduct the evaluation. For simplicity, we use the rubrics in WILDBENCH (Lin\n",
      "et al., 2025), and set the criteria as \"The response is in high quality and provides correct,\n",
      "relevant, and helpful information that focuses on the user query.\". The complete prompt is\n",
      "shown below.\n",
      "\n",
      "Prompts for Prometheus-2 judge\n",
      "\n",
      "###Task Description:\n",
      "An instruction (might include an Input inside it), a response to evaluate, a\n",
      "\n",
      "↪→ reference answer that gets a score of 5, and a score rubric representing\n",
      "↪→ a evaluation criteria are given.\n",
      "\n",
      "1. Write a detailed feedback that assess the quality of the response strictly\n",
      "↪→ based on the given score rubric, not evaluating in general.\n",
      "\n",
      "2. After writing a feedback, write a score that is an integer between 1 and 5.\n",
      "↪→ You should refer to the score rubric.\n",
      "\n",
      "3. The output format should look as follows: \"Feedback: (write a feedback for\n",
      "↪→ criteria) [RESULT] (an integer number between 1 and 5)\"\n",
      "\n",
      "4. Please do not generate any other opening, closing, and explanations.\n",
      "\n",
      "###The instruction to evaluate:\n",
      "{user_query}\n",
      "\n",
      "###Response to evaluate:\n",
      "{model_output}\n",
      "\n",
      "###Reference Answer (Score 5):\n",
      "{ref_answer}\n",
      "\n",
      "###Score Rubrics:\n",
      "[The response is in high quality and provides correct, relevant, and helpful\n",
      "\n",
      "↪→ information that focuses on the user query.]\n",
      "Score 1: The response is very poor and does not make sense at all.\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Score 2: The response is poor and does help user solve the problem in a\n",
      "↪→ meaningful way.\n",
      "\n",
      "Score 3: The response is fair but has some issues (e.g., factual errors,\n",
      "↪→ hallucinations, missing key information).\n",
      "\n",
      "Score 4: The response is good enough but could be improved in some ways.\n",
      "Score 5: The response is perfect and provides helpful information that can help\n",
      "\n",
      "↪→ user solve the problem.\n",
      "\n",
      "###Feedback:\n",
      "\n",
      "A.3 DETAILS OF EXPERIMENTS ON LIST-LEVEL CORRELATION\n",
      "\n",
      "To comprehensively assess the performance of RocketEval, we conduct the experiments by adding\n",
      "two additional benchmark datasets, ALPACAEVAL (Dubois et al., 2023) and ARENA-HARD (Li et al.,\n",
      "2024a). The statistics of all benchmarks are listed in Table 5.\n",
      "\n",
      "Table 5: Statistics of benchmark datasets.\n",
      "\n",
      "Dataset #Instances Turns QueryLen PromptLen\n",
      "\n",
      "MT-BENCH (Zheng et al., 2023) 160* 1-2 202.2 1123.4\n",
      "WILDBENCH (Lin et al., 2025) 1024 1-5 978.5 3402.1\n",
      "ALPACAEVAL (Dubois et al., 2023) 805 1 164.9 164.9\n",
      "ARENA-HARD (Li et al., 2024a) 500 1 406.4 406.4\n",
      "\n",
      "*Here we treat each 2-turn dialogues as 2 instances.\n",
      "\n",
      "A.3.1 EXPERIMENTAL SETUP\n",
      "\n",
      "We draw inspiration from the study by Lin et al. (2025), where 12 models 4 were selected for a\n",
      "correlation analysis between their rankings in the CHATBOT ARENA ELO RATING (Hard prompts-\n",
      "English) and WILDBENCH. However, upon closer inspection, we notice that some of the chosen\n",
      "models have overlapping elo rating confidence intervals, which may compromise the reliability of\n",
      "the correlation results. To address this issue, we revise the model selection to ensure that there is\n",
      "no overlap in the 95% CI. Similarly, when selecting test models for other benchmarks, we take into\n",
      "account the availability of model responses in the official released data and the elo ratings of the test\n",
      "models. The elo ratings of the selected models are presented in Figure 9.\n",
      "\n",
      "A.3.2 RESULTS\n",
      "\n",
      "Score Distribution. We visualize the distribution of the scores from all instances in a single bench-\n",
      "mark. Figure 10 shows when lightweight LLMs are employed as the judge and using CoT to grade\n",
      "the responses, the distribution of scores are highly skewed, which indicates the poor ability of such\n",
      "models in distinguish the responses with different qualities. Meanwhile, the scores graded by the\n",
      "same judge under RocketEval are highly distinguishable and close to the distribution of GPT-4o.\n",
      "Also, the supervised scorer shows the strong ability to reduce the score deviation, making the score\n",
      "distribution closer to the ideal distribution like GPT-4o. This suggests the proposed method can\n",
      "provide valuable context information via checklist and guide the LLM to give differential judgment.\n",
      "\n",
      "Elo Ratings. Given that the scores assigned by the same LLM judge can be utilized to establish\n",
      "pairwise comparisons between different test model responses, we follow the approach of (Chiang\n",
      "et al., 2024) by introducing a match simulator and importing all pairwise comparison results from\n",
      "an LLM judge. Specifically, we convert the scores of two responses to determine the winner, with\n",
      "considering differences in scores smaller than 0.1 as ties. For each pair of test models, we derive the\n",
      "\n",
      "4The selected models are gpt-4-turbo-2024-04-09, claude-3-opus-20240229, Meta-Llama-3-70B-Instruct,\n",
      "Qwen1.5-72B-Chat, claude-3-sonnet-20240229, mistral-large-2402, dbrx-instruct@together, Mixtral-8x7B-\n",
      "Instruct-v0.1, Meta-Llama-3-8B-Instruct, tulu-2-dpo-70b, Llama-2-70b-chat-hf, Llama-2-7b-chat-hf, gemma-\n",
      "7b-it and gemma-2b-it.\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "1000\n",
      "\n",
      "1050\n",
      "\n",
      "1100\n",
      "\n",
      "1150\n",
      "\n",
      "1200\n",
      "\n",
      "1250\n",
      "\n",
      "Ch\n",
      "at\n",
      "\n",
      "bo\n",
      "t A\n",
      "\n",
      "re\n",
      "na\n",
      "\n",
      " E\n",
      "LO\n",
      "\n",
      " R\n",
      "at\n",
      "\n",
      "in\n",
      "g \n",
      "\n",
      "(H\n",
      "ar\n",
      "\n",
      "d-\n",
      "En\n",
      "\n",
      ")\n",
      "\n",
      "GPT-4o Mini (20240718)\n",
      "\n",
      "Yi Large Preview\n",
      "\n",
      "Llama-3-70B-Instruct\n",
      "\n",
      "GPT-4 (0314)\n",
      "\n",
      "Yi-1.5-34B-Chat\n",
      "Llama-3-8B-Instruct\n",
      "\n",
      "Claude v1\n",
      "Claude Instant v1\n",
      "\n",
      "Vicuna-33B-v1.3\n",
      "\n",
      "Vicuna-13B-v1.2\n",
      "\n",
      "MT-Bench\n",
      "\n",
      "GPT-4 Turbo (20240409)\n",
      "\n",
      "Claude 3 Opus (20240229)\n",
      "Llama-3-70B-Instruct\n",
      "\n",
      "Claude 3 Sonnet (20240229)\n",
      "\n",
      "Mistral Large (2402)\n",
      "Llama-3-8B-Instruct\n",
      "\n",
      "DBRX Instruct Preview\n",
      "\n",
      "GPT-3.5-Turbo (1106)\n",
      "\n",
      "Llama-2-70B-Chat\n",
      "\n",
      "Gemma-7B-It\n",
      "\n",
      "Llama-2-7B-Chat\n",
      "\n",
      "Gemma-2B-It\n",
      "\n",
      "AlpacaEval\n",
      "\n",
      "GPT-4 Turbo (20240409)\n",
      "\n",
      "Claude 3 Opus (20240229)\n",
      "Llama-3-70B-Instruct\n",
      "\n",
      "Claude 3 Sonnet (20240229)\n",
      "\n",
      "Claude 3 Haiku (20240307)\n",
      "Mistral Large (2402)\n",
      "Llama-3-8B-Instruct\n",
      "\n",
      "DBRX Instruct Preview\n",
      "\n",
      "Llama-2-70B-Chat\n",
      "\n",
      "Gemma-7B-It\n",
      "\n",
      "Gemma-1.1-2B-It\n",
      "\n",
      "Gemma-2B-It\n",
      "\n",
      "Arena Hard\n",
      "\n",
      "GPT-4 Turbo (20240409)\n",
      "\n",
      "Claude 3 Opus (20240229)\n",
      "Llama-3-70B-Instruct\n",
      "\n",
      "Claude 3 Sonnet (20240229)\n",
      "Reka Core (20240501)\n",
      "\n",
      "Mistral Large (2402)\n",
      "Llama-3-8B-Instruct\n",
      "\n",
      "DBRX Instruct Preview\n",
      "\n",
      "Llama-2-70B-Chat\n",
      "\n",
      "Gemma-7B-It\n",
      "\n",
      "Llama-2-7B-Chat\n",
      "\n",
      "Gemma-2B-It\n",
      "\n",
      "WildBench\n",
      "\n",
      "Figure 9: The scores of selected test models on CHATBOT ARENA ELO RATING (Hard prompts\n",
      "English, 2024-09-17).\n",
      "\n",
      "GPT-4o\n",
      "\n",
      "Mistral-Nemo\n",
      "\n",
      "Llama-3-8B\n",
      "\n",
      "Qwen2-7B\n",
      "\n",
      "Phi-3-mini-4k\n",
      "\n",
      "Gemma-2-2B\n",
      "\n",
      "1 3 5 7 9 1 3 5 7 9\n",
      "\n",
      "Qwen2-1.5B\n",
      "\n",
      "1 3 5 7 9\n",
      "\n",
      "Method\n",
      "CoT RocketEval (Unsup.) RocketEval (Sup.)\n",
      "\n",
      "Figure 10: Distribution of scores on WILDBENCH dataset.\n",
      "\n",
      "pairwise comparison results as the match data. Subsequently, the Elo rating is calculated based on all\n",
      "match results. We employ the same Bradley-Terry model-based Maximum Likelihood Estimation\n",
      "\n",
      "21\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Table 6: Correlation of ranking with CHATBOT ARENA ELO RATING (Hard prompts-English) on\n",
      "different benchmarks.\n",
      "\n",
      "MT-BENCH\n",
      "\n",
      "Method CoT Direct Ours (Unsup.) Ours (Sup.)\n",
      "\n",
      "Coefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\n",
      "\n",
      "GPT-4o Kendall’s Tau (Kend.): 1.0 : Spearman (Spea.): 1.0\n",
      "\n",
      "Llama-3-70B 1.000 1.000 0.944 0.985 - - - -\n",
      "Qwen2-72B 1.000 1.000 0.956 0.988 - - - -\n",
      "\n",
      "Mistral-Nemo 0.911 0.976 0.867 0.964 0.867 0.952 0.911 0.976\n",
      "Llama-3-8B 0.867 0.952 0.867 0.964 0.822 0.927 0.911 0.976\n",
      "Qwen2-7B 0.867 0.952 0.867 0.952 0.778 0.927 0.911 0.976\n",
      "Mistral-7B-v0.3 0.733 0.903 0.822 0.927 0.822 0.939 0.822 0.939\n",
      "Phi-3-mini-4k 0.778 0.915 0.822 0.939 0.867 0.952 0.956 0.988\n",
      "Qwen2.5-3B 0.764 0.912 0.644 0.867 0.822 0.927 0.822 0.927\n",
      "Llama-3.2-3B 0.867 0.952 0.422 0.600 0.822 0.927 0.956 0.988\n",
      "Gemma-2-2B 0.556 0.721 0.629 0.796 0.778 0.915 0.822 0.927\n",
      "InternLM2.5-1.8B 0.378 0.430 -0.296 -0.451 0.644 0.794 0.689 0.855\n",
      "Qwen2.5-1.5B 0.556 0.745 -0.068 -0.128 0.778 0.915 0.911 0.976\n",
      "Qwen2-1.5B 0.511 0.745 0.523 0.665 0.822 0.927 0.822 0.939\n",
      "Llama-3.2-1B 0.511 0.697 0.114 0.146 -0.067 -0.200 0.733 0.879\n",
      "Qwen2.5-0.5B 0.333 0.394 0.422 0.479 0.600 0.733 0.600 0.806\n",
      "\n",
      "ALPACAEVAL\n",
      "\n",
      "Method CoT Direct Ours (Unsup.) Ours (Sup.)\n",
      "\n",
      "Coefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\n",
      "\n",
      "GPT-4o Kendall’s Tau (Kend.): 0.879 : Spearman (Spea.): 0.972\n",
      "\n",
      "Mistral-Nemo 0.848 0.958 0.779 0.900 0.848 0.951 0.848 0.951\n",
      "Llama-3-8B 0.758 0.895 0.818 0.923 0.758 0.916 0.758 0.916\n",
      "Qwen2-7B 0.667 0.825 0.606 0.734 0.788 0.923 0.818 0.937\n",
      "Mistral-7B-v0.3 0.636 0.804 0.515 0.692 0.727 0.881 0.788 0.923\n",
      "Phi-3-mini-4k 0.606 0.762 0.565 0.708 0.818 0.937 0.818 0.944\n",
      "Qwen2.5-3B 0.697 0.811 0.667 0.783 0.727 0.888 0.788 0.923\n",
      "Llama-3.2-3B 0.606 0.804 0.515 0.699 0.909 0.979 0.909 0.979\n",
      "Gemma-2-2B 0.636 0.818 0.848 0.951 0.939 0.986 0.909 0.979\n",
      "InternLM2.5-1.8B 0.182 0.371 0.455 0.587 0.636 0.804 0.667 0.818\n",
      "Qwen2.5-1.5B 0.303 0.483 0.333 0.469 0.879 0.965 0.879 0.965\n",
      "Qwen2-1.5B 0.273 0.420 0.303 0.406 0.545 0.706 0.818 0.909\n",
      "Llama-3.2-1B 0.000 -0.063 -0.061 -0.007 0.515 0.720 0.727 0.867\n",
      "Qwen2.5-0.5B 0.182 0.287 0.485 0.636 0.606 0.769 0.636 0.818\n",
      "\n",
      "ARENA-HARD\n",
      "\n",
      "Method CoT Direct Ours (Unsup.) Ours (Sup.)\n",
      "\n",
      "Coefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\n",
      "\n",
      "GPT-4o Kendall’s Tau (Kend.): 0.939 : Spearman (Spea.): 0.986\n",
      "\n",
      "Mistral-Nemo 1.000 1.000 0.939 0.986 1.000 1.000 0.970 0.993\n",
      "Llama-3-8B 0.909 0.972 0.939 0.986 1.000 1.000 0.970 0.993\n",
      "Qwen2-7B 0.901 0.974 0.879 0.965 0.970 0.993 0.970 0.993\n",
      "Mistral-7B-v0.3 0.818 0.937 0.818 0.930 0.970 0.993 0.939 0.986\n",
      "\n",
      "Phi-3-mini-4k 0.879 0.958 0.758 0.881 1.000 1.000 1.000 1.000\n",
      "Qwen2.5-3B 0.962 0.988 0.939 0.979 0.970 0.993 0.970 0.993\n",
      "Llama-3.2-3B 0.818 0.937 0.333 0.392 0.970 0.993 1.000 1.000\n",
      "Gemma-2-2B 0.545 0.692 0.879 0.958 0.939 0.986 0.970 0.993\n",
      "InternLM2.5-1.8B 0.424 0.559 0.504 0.666 0.818 0.937 0.909 0.972\n",
      "Qwen2.5-1.5B 0.394 0.476 0.515 0.545 0.970 0.993 0.939 0.986\n",
      "Qwen2-1.5B 0.121 0.224 0.636 0.748 0.879 0.965 0.939 0.979\n",
      "Llama-3.2-1B -0.121 -0.147 0.321 0.522 -0.606 -0.727 0.909 0.979\n",
      "Qwen2.5-0.5B 0.091 0.098 0.769 0.902 0.576 0.811 0.939 0.986\n",
      "\n",
      "(MLE), as used by (Chiang et al., 2024), to fit the Elo rating and use bootstrap to estimate confidence\n",
      "intervals.\n",
      "\n",
      "Figure 11 illustrates the elo rating from CHATBOT ARENA, ARENA-HARD (Li et al., 2024a) and the\n",
      "results of judges Gemma-2-2B, Llama-3.2-3B, Qwen2.5-3B, and the ensemble result on the ARENA-\n",
      "HARD benchmark dataset. We utilize the average score and the elo ratings derived by combining all\n",
      "matches from the three judges. We conduct an experiment on all available responses from Li et al.\n",
      "(2024a) and exclude 10 models used to fit the predictor in RocketEval and the GPT-4o-2024-05-13,\n",
      "which is used as the judge to produce labels, resulting in 50 test models. It is evident that the scores\n",
      "and elo ratings produced by the LLM judge follow similar trends and are more closely aligned with\n",
      "the results derived from human preferences. Meanwhile, we notice that the test models that exhibit\n",
      "a large deviation from human judgments belong to the same Llama series, indicating the potential\n",
      "bias on different patterns of responses.\n",
      "\n",
      "22\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "5 15 25 35 45\n",
      "Model Rank in Chatbot Arena (Hard-En)\n",
      "\n",
      "800\n",
      "\n",
      "900\n",
      "\n",
      "1000\n",
      "\n",
      "1100\n",
      "\n",
      "1200\n",
      "\n",
      "1300\n",
      "\n",
      "1400\n",
      "\n",
      "EL\n",
      "O\n",
      "\n",
      " R\n",
      "at\n",
      "\n",
      "in\n",
      "g\n",
      "\n",
      "Chatbot Arena Elo (Hard-En)\n",
      "Llama-3.1-405B-Instruct-FP8\n",
      "\n",
      "Llama-3.1-70B-Instruct\n",
      "\n",
      "Llama-3.1-8B-Instruct\n",
      "RocketEval Elo (Combined)\n",
      "Arena-Hard-Auto Elo\n",
      "RocketEval Score (Average)\n",
      "CoT Score (Average) 3.0\n",
      "\n",
      "3.5\n",
      "\n",
      "4.0\n",
      "\n",
      "4.5\n",
      "\n",
      "5.0\n",
      "\n",
      "5.5\n",
      "\n",
      "6.0\n",
      "\n",
      "Sc\n",
      "or\n",
      "\n",
      "e\n",
      "\n",
      "Figure 11: Scores and elo ratings on ARENA-HARD benchmark dataset. We use the CHATBOT\n",
      "ARENA ELO RATING (Hard-En, 2024-09-27) and reproduce the result of ARENA-HARD-AUTO\n",
      "using the official implementation with the default setting (Judge: GPT-4-1106-preview, Baseline:\n",
      "GPT-4-0314). ARENA-HARD-AUTO Elo, RocketEval elo rating and CoT score are adjusted by\n",
      "scaling and adding a constant value for better visualization.\n",
      "\n",
      "Table 7: Ablation study on instance-level agreement with MT-BENCH HUMAN JUDGMENTS.\n",
      "\n",
      "Method RocketEval (Unsup.) w/o Norm Score w/o Indep. Judgment w/o Weight Factor RocketEval (Sup.)\n",
      "\n",
      "Mistral-Nemo 63.2% 62.6% 63.0% 63.0% 64.2%\n",
      "Llama-3-8B 63.8% 57.5% 60.8% 59.9% 62.9%\n",
      "Qwen2-7B 58.6% 49.3% 59.0% 57.8% 59.8%\n",
      "Mistral-7B-v0.3 58.8% 47.4% 57.5% 52.9% 58.3%\n",
      "\n",
      "Phi-3-mini-4k 61.2% 55.5% 60.5% 57.4% 60.9%\n",
      "Qwen2.5-3B 57.4% 51.3% 59.2% 55.7% 58.7%\n",
      "Llama-3.2-3B 58.6% 48.7% 57.6% 56.5% 58.8%\n",
      "Gemma-2-2B 57.9% 49.6% 55.9% 56.5% 57.3%\n",
      "InternLM2.5-1.8B 51.7% 39.7% 42.3% 37.4% 48.4%\n",
      "\n",
      "Qwen2.5-1.5B 60.7% 46.3% 55.7% 57.5% 60.2%\n",
      "Qwen2-1.5B 56.2% 31.2% 54.3% 52.7% 55.6%\n",
      "Llama-3.2-1B 33.6% 23.4% 40.9% 42.0% 41.9%\n",
      "Qwen2.5-0.5B 54.3% 33.4% 49.4% 47.2% 50.9%\n",
      "\n",
      "A.4 ABLATION STUDY\n",
      "\n",
      "To validate the effectiveness of strategies adopted in RocketEval, we conduct ablation study by\n",
      "testing the performance on the following variants:\n",
      "\n",
      "• w/o Norm Score: It removes the conditional normalized score and simply use the decoding\n",
      "result as the judgment.\n",
      "\n",
      "• w/o Indep. Judgment: It inputs the checklist into the LLM judge in a multi-turn format, so\n",
      "that the LLM can see its previous judgment result when judging on the current checklist\n",
      "item.\n",
      "\n",
      "• w/o Weight Factor: It sets the weight factor αr to the constant 1.\n",
      "\n",
      "The results, presented in Tables 7 and 8, demonstrate that incorporating conditional normalized\n",
      "score consistently enhances the performance of LLM judges, particularly for smaller-sized LLMs.\n",
      "This observation confirms the high uncertainty associated with lightweight LLMs and supports the\n",
      "inference that introducing conditional normalized score can increase their reliability when serving\n",
      "as judges. Simultaneously, setting the weight factor αr to 1 causes the final score to be entirely\n",
      "determined by the supervised predictor. Predictors trained on a limited number of annotations may\n",
      "struggle to provide accurate scoring results but exhibit superior performance in aligning with human\n",
      "\n",
      "23\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Table 8: Ablation study on list-level correlation with CHATBOT ARENA ELO RATING (Hard-En) on\n",
      "WILDBENCH dataset.\n",
      "\n",
      "Method RocketEval (Unsup.) w/o Norm Score w/o Indep. Judgment w/o Weight Factor RocketEval (Sup.)\n",
      "\n",
      "Coefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\n",
      "\n",
      "Mistral-Nemo 0.939 0.986 0.909 0.979 0.879 0.965 0.939 0.986 0.939 0.986\n",
      "Llama-3-8B 0.909 0.979 0.909 0.979 0.848 0.958 0.939 0.986 0.909 0.979\n",
      "Qwen2-7B 0.758 0.895 0.758 0.895 0.788 0.916 0.909 0.979 0.818 0.930\n",
      "Mistral-7B-v0.3 0.758 0.874 0.758 0.874 0.788 0.881 0.879 0.965 0.818 0.930\n",
      "\n",
      "Phi-3-mini-4k 0.788 0.916 0.758 0.902 0.818 0.930 0.909 0.979 0.848 0.958\n",
      "Qwen2.5-3B 0.848 0.944 0.848 0.944 0.848 0.944 0.909 0.979 0.848 0.944\n",
      "Llama-3.2-3B 0.848 0.944 0.848 0.944 0.818 0.930 0.939 0.979 0.848 0.944\n",
      "Gemma-2-2B 0.879 0.965 0.879 0.965 0.879 0.965 0.939 0.986 0.879 0.965\n",
      "InternLM2.5-1.8B 0.576 0.748 0.576 0.748 0.545 0.700 0.636 0.790 0.606 0.769\n",
      "\n",
      "Qwen2.5-1.5B 0.818 0.923 0.788 0.916 0.697 0.867 0.879 0.951 0.848 0.944\n",
      "Qwen2-1.5B 0.455 0.622 0.515 0.643 0.636 0.804 0.758 0.874 0.667 0.825\n",
      "Llama-3.2-1B -0.273 -0.357 -0.212 -0.231 0.606 0.755 0.848 0.923 0.697 0.846\n",
      "Qwen2.5-0.5B 0.667 0.811 0.667 0.811 0.697 0.839 0.848 0.951 0.758 0.895\n",
      "\n",
      "preferences at the list level. In such scenarios, the weight factor αr proves to be effective in miti-\n",
      "gating the negative influences of biased annotations, thereby achieving strong performance in both\n",
      "instance-level agreement and list-level ranking correlation. Meanwhile, although there is a perfor-\n",
      "mance drop in the variant without independent checklist item judgment, the drop is not significant.\n",
      "This may be due to the fact that the position bias exists in all tests and is further alleviated in sub-\n",
      "sequent score prediction stage. Although the position bias has limited impact on the final prediction\n",
      "results, the form of multi-round dialogue prevents batch processing during LLM inference, thereby\n",
      "reducing efficiency. In conclusion, we believe that independent checklist judgment in RocketEval\n",
      "remains an optimal choice.\n",
      "\n",
      "A.5 CHECKLIST ANALYSIS\n",
      "\n",
      "As mentioned in Section 4.3 , we undertake a more detailed examination of the checklists generated\n",
      "by RocketEval on the WILDBENCH. This benchmark can be categorized into five major task types:\n",
      "Math & Data Analysis, Coding & Debugging, Creative Tasks, Information/Advice Seeking, and\n",
      "Planning & Reasoning.\n",
      "\n",
      "In this section, we focus on extracting knowledge graph relationships from all the checklists and\n",
      "conducting a comprehensive analysis of these relationships. Furthermore, we investigate instances\n",
      "of checklist item reweighting across a broader spectrum of tasks to provide a more extensive under-\n",
      "standing of the underlying dynamics.\n",
      "\n",
      "Subject Distribution. As shown in Figure 12, the distribution of subject keywords in validat-\n",
      "ing original question responses ensures a universal, compatible, and effective checklist for various\n",
      "tasks. High-frequency keywords like \"explanation\", and \"code\" are crucial for Coding & Debug-\n",
      "ging, aiding in verifying code functionality and clarity. Keywords like \"essay\", \"example\", and\n",
      "\"story\" are vital for Creative Tasks and Information/Advice seeking, ensuring creativity, relevance,\n",
      "and clarity. For Planning & Reasoning, keywords such as \"strategy\", \"method\" and \"process\" en-\n",
      "sure comprehensive and practical solutions. In Math & Data Analysis, keywords like \"calculation\",\n",
      "\"algorithm\", and \"solution\" validate mathematical logic and data analysis. Universally applicable\n",
      "keywords like \"response\", \"explanation\", and \"example\" consistently evaluate clarity, relevance,\n",
      "and accuracy across all tasks. This multifaceted approach ensures a robust, flexible, and thorough\n",
      "evaluation process, enhancing the overall effectiveness and reliability of the responses.\n",
      "\n",
      "Task-Predicate Relationship. We categorized the checklist items according to the types of tasks\n",
      "and conducted a statistical analysis of the corresponding predicates. As shown in Figure 13, the\n",
      "distribution of predicate keywords within checklists mirrors the distinct demands inherent to dif-\n",
      "ferent task categories, such as Coding & Debugging, Planning & Reasoning, Mathematical & Data\n",
      "Analysis, Information/Advice Seeking, and Creative Tasks. Predicate keywords that are exclusive\n",
      "to certain domains, such as \"handle\" within the context of Coding & Debugging or \"calculate\" in\n",
      "Mathematical & Data Analysis, denote actions that are specific and pertinent to those respective\n",
      "\n",
      "24\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "Figure 12: Distribution of Subjects in checklist items which generated from WILDBENCH.\n",
      "\n",
      "Creative Tasks\n",
      "\n",
      "include\n",
      "Planning & Reasoning\n",
      "\n",
      "provide\n",
      "Coding & Debugging\n",
      "\n",
      "Information/Advice seeking mention\n",
      "\n",
      "explain\n",
      "\n",
      "discuss\n",
      "\n",
      "describe\n",
      "Math & Data Analysis\n",
      "\n",
      "calculate\n",
      "\n",
      "maintain\n",
      "\n",
      "address\n",
      "\n",
      "identify\n",
      "\n",
      "handleoutline\n",
      "\n",
      "use\n",
      "\n",
      "suggest\n",
      "\n",
      "apply\n",
      "\n",
      "reflect\n",
      "\n",
      "implement\n",
      "\n",
      "acknowledge\n",
      "\n",
      "state\n",
      "\n",
      "avoid\n",
      "\n",
      "detail\n",
      "\n",
      "contain\n",
      "incorporate\n",
      "\n",
      "consider\n",
      "\n",
      "have\n",
      "\n",
      "analyze specify\n",
      "\n",
      "highlight\n",
      "present\n",
      "\n",
      "introduce\n",
      "\n",
      "list\n",
      "\n",
      "integrate emphasize\n",
      "\n",
      "Figure 13: Visualization of Task-Predicate Relationship.\n",
      "\n",
      "fields. Conversely, common predicates like \"explain\" and \"describe\" are universally applicable,\n",
      "serving general verification objectives across all question types.\n",
      "\n",
      "These predicate keywords augment the verification process by imparting explicit and goal-oriented\n",
      "directives. Specifically, exclusive keywords concentrate on criteria that are specific to each task\n",
      "type, ensuring a detailed and contextually relevant assessment. Meanwhile, common keywords\n",
      "ensure uniformity and exhaustiveness in the evaluation of responses. This dual strategy ensures\n",
      "\n",
      "25\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "1 Does  the  response  correctly import necessary PyTorch components such as `torch`,\n",
      "`nn`, `optim`, `DataLoader`, and other relevant libraries?\n",
      "\n",
      "2 Is there a GRU-based Encoder class with the `__init__`, `forward`, and `init_hidden`\n",
      "methods implemented?\n",
      "\n",
      "3 Is  there a GRU-based  Decoder class with `__init__`, `forward`, and an output layer\n",
      "using `LogSoftmax` or equivalent?\n",
      "\n",
      "4 Does the response set up a comprehensive character-to-index mapping and word pair\n",
      "handling, specifically through a custom dataset class?\n",
      "\n",
      "5 Is there  an   explanation or  implementation showing  how padding and batching are\n",
      "managed in mini-batch training?\n",
      "\n",
      "6 Does the response show a complete training function that includes initializing hidden\n",
      "states, zeroing gradients, and executing forward and backward passes?\n",
      "\n",
      "7 Are  start  and  end of sequence token indices correctly implemented and used within\n",
      "input handling in the training function?\n",
      "\n",
      "8 Does  the  response  correctly set  up the training infrastructure including the dataset,\n",
      "DataLoader, model instantiation, and the optimizer?\n",
      "\n",
      "9 Are there clear details of a training loop with epoch iterations, batch processing from\n",
      "a DataLoader, and intermediate loss outputs?\n",
      "\n",
      "10 Is the loss  function for  sequence-to-sequence learning in NLP, like CrossEntropy or\n",
      "NLLLoss, clearly explained or demonstrated in the training loop?\n",
      "\n",
      "using pytorch, implement a character\n",
      "level sequence to sequence encoder-\n",
      "decoder model with GRU . Trained the\n",
      "model on a dataset of word pairs with\n",
      "minibatch training.\n",
      "\n",
      "0.050\n",
      "\n",
      "0.228\n",
      "\n",
      "0.212\n",
      "\n",
      "0.085\n",
      "\n",
      "0.050\n",
      "\n",
      "0.130\n",
      "\n",
      "0.050\n",
      "\n",
      "0.050\n",
      "\n",
      "0.093\n",
      "\n",
      "0.050\n",
      "\n",
      "Figure 14: Visualization of checklist item reweighting in Coding & Debugging.\n",
      "\n",
      "that each answer is evaluated comprehensively and from multiple perspectives, thereby significantly\n",
      "enhancing the efficacy of the verification process.\n",
      "\n",
      "Checklist Item Reweighting Analysis. Here we present more reweighted case examples under a\n",
      "variety of tasks. As shown in Figure 14, the checklist generated for Coding & Debugging tasks\n",
      "ensures the complete generation of the code for the target problem. After reweighting, there is a\n",
      "greater emphasis on critical steps within the code, such as the structures of the forward, backward,\n",
      "and loss function in a neural network. For Planning & Reasoning tasks, reweighting makes key\n",
      "reasoning items more prominent. For example, As shown in Figure 17, based on the symptoms\n",
      "described in the problem, the response needs to deduce diabetic ketoacidosis (DKA) to provide a\n",
      "correct and reasonable treatment in subsequent answers. For other Creative Tasks, see Figure 15,\n",
      "and for Information/Advice seeking tasks, see Figure 16. By reweighting checklist items, we can\n",
      "reasonably focus on critical steps or results across different types of tasks, ensuring effectiveness\n",
      "when using lightweight LLM as judges.\n",
      "\n",
      "1 Does the response include dialogue that effectively develops the story and character\n",
      "interactions?\n",
      "\n",
      "2 Is   the  setting,  Luther's old residence, described to contribute an eerie atmosphere\n",
      "appropriate for a mystery story?\n",
      "\n",
      "3 Is    the   central   mystery   (Luther's murder by suffocation) introduced clearly and\n",
      "maintained as the focus of the story?\n",
      "\n",
      "4 Do  all suspects  have  distinct  roles or characteristics that contribute to the story's\n",
      "depth?\n",
      "\n",
      "5 Are the  series  of  murders integrated into the story in a way that escalates tension\n",
      "and drives the narrative forward?\n",
      "\n",
      "6 Does the narrative  convey  an increasing sense of urgency to solve the mystery as\n",
      "the story progresses?\n",
      "\n",
      "7 Does  the  resolution  of  the  mystery   logically  follow from the clues and events\n",
      "described in the story?\n",
      "\n",
      "8 Is the murderer's identity revealed in a manner consistent with the development of\n",
      "the plot and character motivations?\n",
      "\n",
      "9 Is  emotional  tension  and  suspicion  among the suspects effectively conveyed to\n",
      "engage the reader?\n",
      "\n",
      "10 Are clues  and  red herrings  integrated within the narrative to challenge the reader\n",
      "while keeping the solution attainable and fair?\n",
      "\n",
      "Write an intricate mystery story,\n",
      "including dialogue, about the murder of\n",
      "Luther Watson. All five suspects,\n",
      "Christian Vaughn, Selina Morano, Ted\n",
      "Mosley, Blii Paxrott and Samntha\n",
      "Williams, are gathered at Luther's old\n",
      "residence. One of them is the murderer\n",
      "and they have to find out who it is. The\n",
      "only thing they know, is that Luther was\n",
      "suffocated. Each night one of the\n",
      "suspects gets killed by the murderer and\n",
      "the pressure to solve the case\n",
      "increases.\n",
      "\n",
      "0.058\n",
      "\n",
      "0.083\n",
      "\n",
      "0.123\n",
      "\n",
      "0.046\n",
      "\n",
      "0.062\n",
      "\n",
      "0.074\n",
      "\n",
      "0.176\n",
      "\n",
      "0.046\n",
      "\n",
      "0.102\n",
      "\n",
      "0.229\n",
      "\n",
      "Figure 15: Visualization of checklist item reweighting in Creative Tasks.\n",
      "\n",
      "26\n",
      "\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "\n",
      "1\n",
      " Does the response provide a  clear and detailed explanation of how mining\n",
      "\n",
      "engineering can connect to renewable energy resources, including future\n",
      "potential?\n",
      "\n",
      "2 Does the response mention the importance of renewable energy for climate\n",
      "change mitigation and reducing dependence on fossil fuels?\n",
      "\n",
      "3\n",
      " Does the response specify essential skills for mining engineering\n",
      "\n",
      "graduates to integrate  with renewable energy fields, such as understanding\n",
      "of raw materials, sustainable practices, and technical skills?\n",
      "\n",
      "4 Does the response  describe  some academic or career positions relevant to\n",
      "mining engineering and renewable energy?\n",
      "\n",
      "5 Does the response include examples of renewable energy technologies like\n",
      "wind turbines, solar panels, and energy storage systems?\n",
      "\n",
      "6\n",
      " Is the response structured  in a clear and well-organized manner, breaking\n",
      "down different topics such as connection, importance, required skills, and\n",
      "\n",
      "career opportunities?\n",
      "\n",
      " how to connect mining engineering to\n",
      "renewable energy resources and future of\n",
      "this connection, importance of renewable\n",
      "energy and its future and what skills of\n",
      "a graduate student in mining engineering\n",
      "does need to this connection and carrier\n",
      "job and academic positions\n",
      "\n",
      "0.118\n",
      "\n",
      "0.233\n",
      "\n",
      "0.118\n",
      "\n",
      "0.118\n",
      "\n",
      "0.297\n",
      "\n",
      "0.118\n",
      "\n",
      "Figure 16: Visualization of checklist item reweighting in Information/Advice seeking.\n",
      "\n",
      "1 Does   the   response correctly  identify  the  most   likely\n",
      "diagnosis as diabetic ketoacidosis (DKA)?\n",
      "\n",
      "2\n",
      " Does the response  mention a relevant  test  for\n",
      "\n",
      "proving DKA, specifically mentioning blood ketone levels\n",
      "or beta-hydroxybutyrate?\n",
      "\n",
      "3 Does the response correctly calculate the anion gap using\n",
      "the  formula: Na+ - (Cl- + HCO3-) and provide the value?\n",
      "\n",
      "4\n",
      " Does the  response  outline  appropriate lines of treatment\n",
      "\n",
      "for DKA covering fluid resuscitation, insulin therapy,\n",
      "electrolyte management, and acidosis correction?\n",
      "\n",
      "5\n",
      " Does the response address the patient's infected foot ulcer\n",
      "as a possible secondary infection and suggest appropriate\n",
      "\n",
      "interventions like antibiotics?\n",
      "\n",
      "6\n",
      " Does the response  mention additional  patient\n",
      "\n",
      "education on diabetes management and foot care as part of\n",
      "the treatment plan or prevention of future episodes?\n",
      "\n",
      "7 Is the  response  clear, concise, and appropriately detailed\n",
      "in addressing each part of the original query?\n",
      "\n",
      "A 25-year-old man with type-1 diabetes\n",
      "mellitus on insulin presents to ED with\n",
      "fever,cough, vomiting and abdominal\n",
      "pain. Examination reveals tachypnia, dry\n",
      "mucosa,decreased skin turgor, a\n",
      "temperature of 37.8 °C and infected\n",
      "ulcer in the left foot.Investigations\n",
      "show RPG of 350 mg/dl, Na+ 130 mmol/l,\n",
      "K+ 5.7 mmol/l, Bicarbonate\n",
      "12 mmol/L, Chloride 105 mmol/L, urea 60\n",
      "mg/dl.\n",
      "A. What is the most likely diagnosis?\n",
      "B. Mention one test to prove the\n",
      "diagnosis.\n",
      "C. What is the value of anion gap in\n",
      "this patient?\n",
      "D. What are the lines of treatment?\n",
      "\n",
      "0.637\n",
      "\n",
      "0.052\n",
      "\n",
      "0.094\n",
      "\n",
      "0.053\n",
      "\n",
      "0.052\n",
      "\n",
      "0.052\n",
      "\n",
      "0.061\n",
      "\n",
      "Figure 17: Visualization of checklist item reweighting in Planning & Reasoning.\n",
      "\n",
      "27\n",
      "\n",
      "\n",
      "\tIntroduction\n",
      "\tHow lightweight LLMs perform as a judge?\n",
      "\tSetup\n",
      "\tHow do lightweight LLM judges perform?\n",
      "\tWhere do lightweight LLMs underperform as a judge?\n",
      "\tWhy do lightweight LLMs not good at judging?\n",
      "\n",
      "\tMethodology\n",
      "\tChecklist Creation\n",
      "\tChecklist Grading\n",
      "\tScore Prediction\n",
      "\n",
      "\tExperiments\n",
      "\tHuman Agreement on the Evaluation\n",
      "\tEvaluation Cost Estimation\n",
      "\tQualitative Analysis\n",
      "\n",
      "\tConclusion\n",
      "\tAppendix\n",
      "\tPrompts used in RocketEval\n",
      "\tDatasets and Baselines\n",
      "\tDetails of Experiments on List-level Correlation\n",
      "\tExperimental Setup\n",
      "\tResults\n",
      "\n",
      "\tAblation Study\n",
      "\tChecklist Analysis\n",
      "\n",
      "\n",
      ".\n",
      "                    \n",
      "                    Extract the following information from the research paper:\n",
      "                    \n",
      "                    - Problem/Research Question: What is the research trying to address or investigate? \n",
      "                    - Methods/Approach: How did the researchers conduct their study? \n",
      "                    - Results/Findings: What did the research uncover? \n",
      "                    - Conclusions/Implications: What are the main takeaways and significance of the research? \n",
      "                    - Limitations and Future Directions: What\n",
      "                    \n",
      "                    Generate a high-engagement Linked In post about the challenges and solutions in the research paper.\n",
      "                    \n",
      "                    Extract keywords and use bing search to explain the keywords in the research paper.                    \n",
      "                    The tone should be authoritative yet engaging. \n",
      "                    Follow the Hook → Context → Insights → Engagement Trigger structure. End with a question to spark discussion. Add 3-5 relevant hashtags.\n",
      "    \n",
      "Created agent, ID: asst_mHgLNucphmWQaqcZKYoDIfY4\n",
      "Created thread, ID: thread_mREGRY1l1s7HxapI8iEDmhho\n",
      "SMS: {'id': 'msg_QgLttwM8D4ibgh5zJ2s7xY4h', 'object': 'thread.message', 'created_at': 1742748818, 'assistant_id': None, 'thread_id': 'thread_mREGRY1l1s7HxapI8iEDmhho', 'run_id': None, 'role': 'user', 'content': [{'type': 'text', 'text': {'value': 'Problem/Research Question of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST', 'annotations': []}}], 'attachments': [], 'metadata': {}}\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Deleted agent\n",
      "Messages:The core research problem addressed by \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" is the cost and limitations associated with human evaluations of large language models (LLMs). The paper proposes using a lightweight LLM as a judge through a grading checklist, which addresses performance disparities and improves judgment accuracy. This method aims to provide a straightforward, replicable, and cost-effective way to automate LLM evaluations, achieving high correlation with human preferences while significantly reducing expenses【0†source】【1†source】.\n",
      "Created agent, ID: asst_8l027Kzl95Q0AcOi5C1c5eM3\n",
      "Created thread, ID: thread_5Yx3UaslUt859ngC2xmNtyBA\n",
      "SMS: {'id': 'msg_b5zS3A2Z5NzF6R2tKFusrpA4', 'object': 'thread.message', 'created_at': 1742748829, 'assistant_id': None, 'thread_id': 'thread_5Yx3UaslUt859ngC2xmNtyBA', 'run_id': None, 'role': 'user', 'content': [{'type': 'text', 'text': {'value': 'Methods/Approach of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST', 'annotations': []}}], 'attachments': [], 'metadata': {}}\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Deleted agent\n",
      "Messages:The RocketEval approach involves three primary steps:\n",
      "\n",
      "1. **Checklist Creation**: Building a comprehensive, instance-specific checklist for the scenarios and questions under consideration.\n",
      "2. **Checklist Grading**: Lightweight LLMs, such as Gemma-2-2B, grade the checklists.\n",
      "3. **Reweighting**: Reweighting the checklist items to align with supervised annotations, ensuring the evaluation reflects human preferences accurately【0†source】【1†source】.\n",
      "Created agent, ID: asst_K6xubAGkUMcasfSM6h66QJoR\n",
      "Created thread, ID: thread_Jv0qjGvONHZver8E4o5oQjwv\n",
      "SMS: {'id': 'msg_4DQEBF765IiYKTz8PqUre4uQ', 'object': 'thread.message', 'created_at': 1742748840, 'assistant_id': None, 'thread_id': 'thread_Jv0qjGvONHZver8E4o5oQjwv', 'run_id': None, 'role': 'user', 'content': [{'type': 'text', 'text': {'value': 'Results/Findings of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST', 'annotations': []}}], 'attachments': [], 'metadata': {}}\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Deleted agent\n",
      "Messages:RocketEval leverages a lightweight LLM as a judge to achieve efficient automated evaluations. By framing evaluations as multi-faceted Q&A tasks with a checklist, it manages to reduce costs and maintain high correlation with human preferences. Specifically, RocketEval, using Gemma-2-2B as the judge, shows a correlation of 0.965 with human preferences, comparable to GPT-4, and offers over 50-fold cost savings for large-scale scenarios【5†source】.\n",
      "Created agent, ID: asst_I5mOcbiAGjVirrmgEb4bQZZo\n",
      "Created thread, ID: thread_5bptUv8eDlAAV2Iic5u4aYdT\n",
      "SMS: {'id': 'msg_136DTluLDNDGbRqsn1ECGS9C', 'object': 'thread.message', 'created_at': 1742748853, 'assistant_id': None, 'thread_id': 'thread_5bptUv8eDlAAV2Iic5u4aYdT', 'run_id': None, 'role': 'user', 'content': [{'type': 'text', 'text': {'value': 'Conclusions/Implications of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST', 'annotations': []}}], 'attachments': [], 'metadata': {}}\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Deleted agent\n",
      "Messages:The conclusions and implications of the study \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" reveal that RocketEval, using lightweight LLMs for grading through an instance-specific checklist, achieves a high correlation with human preferences (0.965) comparable to more powerful models like GPT-4, while reducing evaluation costs by over 50-fold. This approach addresses challenges such as high expenses, privacy, security, and reproducibility issues associated with human or powerful LLM-based evaluations【0†source】【1†source】.\n",
      "Created agent, ID: asst_jM5bYuRTTRq3lAch3QdSp9Y5\n",
      "Created thread, ID: thread_rYNTDCpcs6RsgqaroB8UMKtR\n",
      "SMS: {'id': 'msg_JHIm0tiQjr0ITaPlUeHHM0ef', 'object': 'thread.message', 'created_at': 1742748864, 'assistant_id': None, 'thread_id': 'thread_rYNTDCpcs6RsgqaroB8UMKtR', 'run_id': None, 'role': 'user', 'content': [{'type': 'text', 'text': {'value': 'Limitations and Future Directions of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST', 'annotations': []}}], 'attachments': [], 'metadata': {}}\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Deleted agent\n",
      "Messages:RocketEval addresses the challenge of evaluating large language models (LLMs) with reduced costs by using lightweight LLMs and a checklist grading approach. However, it faces limitations such as reduced judgment accuracy due to high uncertainty and positional bias of lightweight LLMs. Future directions include improving adaptability to various scenarios, further reducing biases, and enhancing reproducibility in different contexts【9†source】【3†source】.\n",
      "---------- assistant ----------\n",
      "[FunctionCall(id='call_tlomk9A0GsqWpUR2MLugyY39', arguments='{\"query\": \"Problem/Research Question of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent'), FunctionCall(id='call_R29Sdqjd21lRzxDtiBtSKQrj', arguments='{\"query\": \"Methods/Approach of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent'), FunctionCall(id='call_wl20TARio3Z6BszejZUgLyfB', arguments='{\"query\": \"Results/Findings of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent'), FunctionCall(id='call_B3X1dwhE4TajPLEVBrRvdtK8', arguments='{\"query\": \"Conclusions/Implications of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent'), FunctionCall(id='call_gZwgF6BE7O3ksMTFUgiI1eGc', arguments='{\"query\": \"Limitations and Future Directions of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent')]\n",
      "---------- assistant ----------\n",
      "[FunctionExecutionResult(content='The core research problem addressed by \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" is the cost and limitations associated with human evaluations of large language models (LLMs). The paper proposes using a lightweight LLM as a judge through a grading checklist, which addresses performance disparities and improves judgment accuracy. This method aims to provide a straightforward, replicable, and cost-effective way to automate LLM evaluations, achieving high correlation with human preferences while significantly reducing expenses【0†source】【1†source】.', name='web_ai_agent', call_id='call_tlomk9A0GsqWpUR2MLugyY39', is_error=False), FunctionExecutionResult(content='The RocketEval approach involves three primary steps:\\n\\n1. **Checklist Creation**: Building a comprehensive, instance-specific checklist for the scenarios and questions under consideration.\\n2. **Checklist Grading**: Lightweight LLMs, such as Gemma-2-2B, grade the checklists.\\n3. **Reweighting**: Reweighting the checklist items to align with supervised annotations, ensuring the evaluation reflects human preferences accurately【0†source】【1†source】.', name='web_ai_agent', call_id='call_R29Sdqjd21lRzxDtiBtSKQrj', is_error=False), FunctionExecutionResult(content='RocketEval leverages a lightweight LLM as a judge to achieve efficient automated evaluations. By framing evaluations as multi-faceted Q&A tasks with a checklist, it manages to reduce costs and maintain high correlation with human preferences. Specifically, RocketEval, using Gemma-2-2B as the judge, shows a correlation of 0.965 with human preferences, comparable to GPT-4, and offers over 50-fold cost savings for large-scale scenarios【5†source】.', name='web_ai_agent', call_id='call_wl20TARio3Z6BszejZUgLyfB', is_error=False), FunctionExecutionResult(content='The conclusions and implications of the study \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" reveal that RocketEval, using lightweight LLMs for grading through an instance-specific checklist, achieves a high correlation with human preferences (0.965) comparable to more powerful models like GPT-4, while reducing evaluation costs by over 50-fold. This approach addresses challenges such as high expenses, privacy, security, and reproducibility issues associated with human or powerful LLM-based evaluations【0†source】【1†source】.', name='web_ai_agent', call_id='call_B3X1dwhE4TajPLEVBrRvdtK8', is_error=False), FunctionExecutionResult(content='RocketEval addresses the challenge of evaluating large language models (LLMs) with reduced costs by using lightweight LLMs and a checklist grading approach. However, it faces limitations such as reduced judgment accuracy due to high uncertainty and positional bias of lightweight LLMs. Future directions include improving adaptability to various scenarios, further reducing biases, and enhancing reproducibility in different contexts【9†source】【3†source】.', name='web_ai_agent', call_id='call_gZwgF6BE7O3ksMTFUgiI1eGc', is_error=False)]\n",
      "---------- assistant ----------\n",
      "The core research problem addressed by \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" is the cost and limitations associated with human evaluations of large language models (LLMs). The paper proposes using a lightweight LLM as a judge through a grading checklist, which addresses performance disparities and improves judgment accuracy. This method aims to provide a straightforward, replicable, and cost-effective way to automate LLM evaluations, achieving high correlation with human preferences while significantly reducing expenses【0†source】【1†source】.\n",
      "The RocketEval approach involves three primary steps:\n",
      "\n",
      "1. **Checklist Creation**: Building a comprehensive, instance-specific checklist for the scenarios and questions under consideration.\n",
      "2. **Checklist Grading**: Lightweight LLMs, such as Gemma-2-2B, grade the checklists.\n",
      "3. **Reweighting**: Reweighting the checklist items to align with supervised annotations, ensuring the evaluation reflects human preferences accurately【0†source】【1†source】.\n",
      "RocketEval leverages a lightweight LLM as a judge to achieve efficient automated evaluations. By framing evaluations as multi-faceted Q&A tasks with a checklist, it manages to reduce costs and maintain high correlation with human preferences. Specifically, RocketEval, using Gemma-2-2B as the judge, shows a correlation of 0.965 with human preferences, comparable to GPT-4, and offers over 50-fold cost savings for large-scale scenarios【5†source】.\n",
      "The conclusions and implications of the study \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" reveal that RocketEval, using lightweight LLMs for grading through an instance-specific checklist, achieves a high correlation with human preferences (0.965) comparable to more powerful models like GPT-4, while reducing evaluation costs by over 50-fold. This approach addresses challenges such as high expenses, privacy, security, and reproducibility issues associated with human or powerful LLM-based evaluations【0†source】【1†source】.\n",
      "RocketEval addresses the challenge of evaluating large language models (LLMs) with reduced costs by using lightweight LLMs and a checklist grading approach. However, it faces limitations such as reduced judgment accuracy due to high uncertainty and positional bias of lightweight LLMs. Future directions include improving adaptability to various scenarios, further reducing biases, and enhancing reproducibility in different contexts【9†source】【3†source】.\n",
      "---------- write_agent ----------\n",
      "🚀 **Revolutionizing LLM Evaluation: The Rise of RocketEval** 🚀\n",
      "\n",
      "In an era where large language models (LLMs) are rapidly evolving, the efficacy of their evaluation has become an urgent necessity. Traditional methods often involve costly human evaluations that can compromise privacy and reproducibility. **Enter \"RocketEval.\"**\n",
      "\n",
      "**Context:** This groundbreaking research presented at ICLR 2025 explores how lightweight LLMs can serve as highly-efficient evaluators through a structured checklist approach. By transitioning the evaluation process from subjective human judgments to a systematic, automated framework, researchers have opened new avenues for cost-effective, scalable assessments that align closely with human preferences.\n",
      "\n",
      "**Insights:** \n",
      "- **Key Findings**: The RocketEval framework demonstrates a remarkable correlation of **0.965** with human evaluations using a lightweight model (Gemma-2-2B), achieving results comparable to powerful models like GPT-4, but at **over a 50-fold** cost reduction! \n",
      "- **Innovative Approach**: By reframing evaluations into multi-faceted checklists, the study addresses common pitfalls in LLM assessments: high uncertainty and positional bias. This systematic approach ensures that critical aspects of a model's response are thoroughly evaluated, leading to more reliable outputs.\n",
      "\n",
      "**Engagement Trigger:** As the landscape of AI continues to shift, how can we ensure that our evaluation methods keep pace with the technology? Do you believe automated systems can fully replace human evaluators, or is there still value in the human touch in this process?\n",
      "\n",
      "💬 *Let's discuss the future of LLM evaluations!*\n",
      "\n",
      "#LLM #Automation #AIResearch #DataScience #MachineLearning \n",
      "\n",
      "---\n",
      "\n",
      "**Keywords Explained**:\n",
      "\n",
      "1. **Large Language Models (LLMs)**: These are complex AI models designed to understand and generate human language. They are foundational in various applications, from chatbots to content generation.\n",
      "   \n",
      "2. **Automated Evaluation**: This refers to the use of technology, especially AI, to assess the performance of models. It aims to provide faster, scalable, and cost-effective assessments compared to traditional methods.\n",
      "\n",
      "3. **Checklist Grading**: A systematic approach where specific criteria are outlined (checklists) to guide the evaluation process, ensuring comprehensive and consistent assessments.\n",
      "\n",
      "4. **High Correlation with Human Preferences**: A metric indicating how closely the automated evaluation results match human evaluations, highlighting the effectiveness of the evaluation method.\n",
      "\n",
      "5. **Positional Bias**: A cognitive bias that affects decision-making based on the order of presented information. Addressing positional bias is crucial in evaluations to enhance fairness and accuracy. \n",
      "\n",
      "Feel free to ask if you have more specific questions or need additional information!\n",
      "This is Code Interpreter for Azure AI Agent Service .......\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Messages: {'object': 'list', 'data': [{'id': 'msg_5uaD7hF5PGCvKg7pRQRYkP7D', 'object': 'thread.message', 'created_at': 1742748904, 'assistant_id': 'asst_Zxj6NCX2c3ShA2yXVH2YAOdA', 'thread_id': 'thread_vaoODXuLsk8DR4WqgxM9kYWe', 'run_id': 'run_eADscsJapwOBd51TZeFalcbI', 'role': 'assistant', 'content': [{'type': 'text', 'text': {'value': 'The blog content has been successfully saved. You can download the file using the following link:\\n\\n[Download blog-250323165503.md](sandbox:/mnt/data/blog-250323165503.md)', 'annotations': [{'type': 'file_path', 'text': 'sandbox:/mnt/data/blog-250323165503.md', 'start_index': 131, 'end_index': 169, 'file_path': {'file_id': 'assistant-6iQsF576r6cVV6ZsxcjRAz'}}]}}], 'attachments': [{'file_id': 'assistant-6iQsF576r6cVV6ZsxcjRAz', 'tools': [{'type': 'code_interpreter'}]}], 'metadata': {}}, {'id': 'msg_EvGGHBUWwEVwUKV7vl1E41NV', 'object': 'thread.message', 'created_at': 1742748894, 'assistant_id': None, 'thread_id': 'thread_vaoODXuLsk8DR4WqgxM9kYWe', 'run_id': None, 'role': 'user', 'content': [{'type': 'text', 'text': {'value': '\\n        \\n                    You are my Python programming assistant. Generate code,save 🚀 **Revolutionizing LLM Evaluation: The Rise of RocketEval** 🚀\\n\\nIn an era where large language models (LLMs) are rapidly evolving, the efficacy of their evaluation has become an urgent necessity. Traditional methods often involve costly human evaluations that can compromise privacy and reproducibility. **Enter \"RocketEval.\"\\n**\\n**Context:** This groundbreaking research presented at ICLR 2025 explores how lightweight LLMs can serve as highly-efficient evaluators through a structured checklist approach. By transitioning the evaluation process from subjective human judgments to a systematic, automated framework, researchers have opened new avenues for cost-effective, scalable assessments that align closely with human preferences.\\n\\n**Insights:** \\n- **Key Findings**: The RocketEval framework demonstrates a remarkable correlation of **0.965** with human evaluations using a lightweight model (Gemma-2-2B), achieving results comparable to powerful models like GPT-4, but at **over a 50-fold** cost reduction! \\n- **Innovative Approach**: By reframing evaluations into multi-faceted checklists, the study addresses common pitfalls in LLM assessments: high uncertainty and positional bias. This systematic approach ensures that critical aspects of a model\\'s response are thoroughly evaluated, leading to more reliable outputs.\\n\\n**Engagement Trigger:** As the landscape of AI continues to shift, how can we ensure that our evaluation methods keep pace with the technology? Do you believe automated systems can fully replace human evaluators, or is there still value in the human touch in this process?\\n\\n💬 *Let\\'s discuss the future of LLM evaluations!*\\n\\n#LLM #Automation #AIResearch #DataScience #MachineLearning \\n\\n---\\n\\n**Keywords Explained**:\\n\\n1. **Large Language Models (LLMs)**: These are complex AI models designed to understand and generate human language. They are foundational in various applications, from chatbots to content generation.\\n   \\n2. **Automated Evaluation**: This refers to the use of technology, especially AI, to assess the performance of models. It aims to provide faster, scalable, and cost-effective assessments compared to traditional methods.\\n\\n3. **Checklist Grading**: A systematic approach where specific criteria are outlined (checklists) to guide the evaluation process, ensuring comprehensive and consistent assessments.\\n\\n4. **High Correlation with Human Preferences**: A metric indicating how closely the automated evaluation results match human evaluations, highlighting the effectiveness of the evaluation method.\\n\\n5. **Positional Bias**: A cognitive bias that affects decision-making based on the order of presented information. Addressing positional bias is crucial in evaluations to enhance fairness and accuracy. \\n\\nFeel free to ask if you have more specific questions or need additional information!    \\n                    and execute it according to the following requirements\\n\\n                    1. Save blog content to blog-{YYMMDDHHMMSS}.md\\n\\n                    2. give me the download this file link\\n                ', 'annotations': []}}], 'attachments': [], 'metadata': {}}], 'first_id': 'msg_5uaD7hF5PGCvKg7pRQRYkP7D', 'last_id': 'msg_EvGGHBUWwEVwUKV7vl1E41NV', 'has_more': False}\n",
      "Last Message: The blog content has been successfully saved. You can download the file using the following link:\n",
      "\n",
      "[Download blog-250323165503.md](sandbox:/mnt/data/blog-250323165503.md)\n",
      "Deleted agent\n",
      "---------- save_post_content_agent ----------\n",
      "[FunctionCall(id='call_Ec2ws0LMS1vKHPDEwSO7YS5G', arguments='{\"blog_content\":\"🚀 **Revolutionizing LLM Evaluation: The Rise of RocketEval** 🚀\\\\n\\\\nIn an era where large language models (LLMs) are rapidly evolving, the efficacy of their evaluation has become an urgent necessity. Traditional methods often involve costly human evaluations that can compromise privacy and reproducibility. **Enter \\\\\"RocketEval.\\\\\"\\\\n**\\\\n**Context:** This groundbreaking research presented at ICLR 2025 explores how lightweight LLMs can serve as highly-efficient evaluators through a structured checklist approach. By transitioning the evaluation process from subjective human judgments to a systematic, automated framework, researchers have opened new avenues for cost-effective, scalable assessments that align closely with human preferences.\\\\n\\\\n**Insights:** \\\\n- **Key Findings**: The RocketEval framework demonstrates a remarkable correlation of **0.965** with human evaluations using a lightweight model (Gemma-2-2B), achieving results comparable to powerful models like GPT-4, but at **over a 50-fold** cost reduction! \\\\n- **Innovative Approach**: By reframing evaluations into multi-faceted checklists, the study addresses common pitfalls in LLM assessments: high uncertainty and positional bias. This systematic approach ensures that critical aspects of a model\\'s response are thoroughly evaluated, leading to more reliable outputs.\\\\n\\\\n**Engagement Trigger:** As the landscape of AI continues to shift, how can we ensure that our evaluation methods keep pace with the technology? Do you believe automated systems can fully replace human evaluators, or is there still value in the human touch in this process?\\\\n\\\\n💬 *Let\\'s discuss the future of LLM evaluations!*\\\\n\\\\n#LLM #Automation #AIResearch #DataScience #MachineLearning \\\\n\\\\n---\\\\n\\\\n**Keywords Explained**:\\\\n\\\\n1. **Large Language Models (LLMs)**: These are complex AI models designed to understand and generate human language. They are foundational in various applications, from chatbots to content generation.\\\\n   \\\\n2. **Automated Evaluation**: This refers to the use of technology, especially AI, to assess the performance of models. It aims to provide faster, scalable, and cost-effective assessments compared to traditional methods.\\\\n\\\\n3. **Checklist Grading**: A systematic approach where specific criteria are outlined (checklists) to guide the evaluation process, ensuring comprehensive and consistent assessments.\\\\n\\\\n4. **High Correlation with Human Preferences**: A metric indicating how closely the automated evaluation results match human evaluations, highlighting the effectiveness of the evaluation method.\\\\n\\\\n5. **Positional Bias**: A cognitive bias that affects decision-making based on the order of presented information. Addressing positional bias is crucial in evaluations to enhance fairness and accuracy. \\\\n\\\\nFeel free to ask if you have more specific questions or need additional information!\"}', name='save_blog_agent')]\n",
      "---------- save_post_content_agent ----------\n",
      "[FunctionExecutionResult(content='Saved', name='save_blog_agent', call_id='call_Ec2ws0LMS1vKHPDEwSO7YS5G', is_error=False)]\n",
      "---------- save_post_content_agent ----------\n",
      "Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='\\n                    I\\'m writing a linked in post about a research paper. The content of the research paper is \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nROCKETEVAL: EFFICIENT AUTOMATED LLM\\nEVALUATION VIA GRADING CHECKLIST\\n\\nTianjun Wei∗† 12, Wei Wen∗ 2, Ruizhi Qiao‡2, Xing Sun 2, Jianghong Ma‡ 3\\n\\n1 City University of Hong Kong, 2 Tencent Youtu Lab, 3 Harbin Institute of Technology Shenzhen.\\ntjwei2-c@my.cityu.edu.hk {jawnrwen,ruizhiqiao,winfredsun}@tencent.com\\nmajianghong@hit.edu.cn\\n\\nABSTRACT\\n\\nEvaluating large language models (LLMs) in diverse and challenging scenarios\\nis essential to align them with human preferences. To mitigate the prohibitive\\ncosts associated with human evaluations, utilizing a powerful LLM as a judge\\nhas emerged as a favored approach. Nevertheless, this methodology encounters\\nseveral challenges, including substantial expenses, concerns regarding privacy\\nand security, and reproducibility. In this paper, we propose a straightforward,\\nreplicable, and accurate automated evaluation method by leveraging a lightweight\\nLLM as the judge, named RocketEval. Initially, we identify that the performance\\ndisparity between lightweight and powerful LLMs in evaluation tasks primarily\\nstems from their ability to conduct comprehensive analyses, which is not easily\\nenhanced through techniques such as chain-of-thought reasoning. By reframing\\nthe evaluation task as a multi-faceted Q&A using an instance-specific checklist,\\nwe demonstrate that the limited judgment accuracy of lightweight LLMs is largely\\nattributes to high uncertainty and positional bias. To address these challenges, we\\nintroduce an automated evaluation process grounded in checklist grading, which\\nis designed to accommodate a variety of scenarios and questions. This process en-\\ncompasses the creation of checklists, the grading of these checklists by lightweight\\nLLMs, and the reweighting of checklist items to align with the supervised annota-\\ntions. Our experiments carried out on the automated evaluation benchmarks, MT-\\nBENCH and WILDBENCH datasets, reveal that RocketEval, when using Gemma-\\n2-2B as the judge, achieves a high correlation (0.965) with human preferences,\\nwhich is comparable to GPT-4o. Moreover, RocketEval provides a cost reduction\\nexceeding 50-fold for large-scale evaluation and comparison scenarios. Our code\\nis available at https://github.com/Joinn99/RocketEval-ICLR.\\n\\n1 INTRODUCTION\\n\\nWhy is automated LLM evaluation necessary? In recent years, the progress in large language\\nmodels (LLMs) has been remarkable (Jiang et al., 2024a; Team et al., 2024; Yang et al., 2024), driven\\nby continuous technological advancements. The rapid emergence of new models and techniques has\\nbroadened their applications, encompassing both general-purpose textual and visual LLMs, as well\\nas those fine-tuned for specific tasks in various domains. These LLMs exhibit a range of capabili-\\nties and performances across different application scenarios. Therefore, evaluating their capabilities\\neffectively has become crucial for guiding their development. Since most tasks performed by LLMs\\ninvolve human interaction, human preferences are often considered the gold standard for LLM eval-\\nuation (Zheng et al., 2023). Currently, crowd-sourcing platforms like CHATBOT ARENA (Chiang\\net al., 2024) collect a significant number of human votes to evaluate LLMs. However, this approach\\nrelies on extensive and long-term human annotation, which is costly and challenging to reproduce\\nand interpret (Ni et al., 2024). Considering the typical applications of LLM evaluation today, which\\n\\n∗Equal Contribution.\\n†Work done during internship at Tencent.\\n‡Corresponding Authors: Ruizhi Qiao and Jianghong Ma.\\n1Unless otherwise stated, all names of LLM in this paper refer to their \"Instruct\" or \"Chat\" versions.\\n\\n1\\n\\nar\\nX\\n\\niv\\n:2\\n\\n50\\n3.\\n\\n05\\n14\\n\\n2v\\n1 \\n\\n [\\ncs\\n\\n.C\\nL\\n\\n] \\n 7\\n\\n M\\nar\\n\\n 2\\n02\\n\\n5\\n\\nhttps://github.com/Joinn99/RocketEval-ICLR\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n5 10 15 20 25 30 35 40 45\\nOpenLLM LeaderBoard 2 Average\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\nA\\ngr\\n\\nee\\nm\\n\\nen\\nt \\n\\nw\\nit\\n\\nh \\nhu\\n\\nm\\nan\\n\\n %\\n\\nQwen2-72B\\n\\nLlama-3-70B\\n\\nYi-1.5-34B\\n\\nMistral-Nemo\\n\\nGemma-2-9B\\n\\nYi-1.5-9B\\n\\nQwen2-7B\\nMistral-7B-v0.3\\n\\nQwen1.5-4B\\n\\nPhi-3-mini-4k\\nGemma-2-2B\\n\\nLlama-3.2-3B\\n\\nInternLM2.5-1.8B\\n\\nQwen2-1.5B\\nLlama-3.2-1B\\n\\nYi-1.5-6B\\n\\nLlama-3-8B\\n\\nQwen1.5-14B\\n\\nPhi-3.5-mini\\n\\nPhi-3-small-8k\\n\\nPhi-3-medium-4k\\nGLM-4-9B\\n\\nQwen2.5-3B\\n\\nQwen2.5-1.5B\\n\\nQwen2.5-0.5B\\n\\nInternLM2.5-20B\\nInternLM2.5-7B\\n\\nLlama-3.1-8B\\n\\nAgreement (CoT)\\nAgreement (Ours)\\nHuman (64.8%)\\nGPT-4o (66.7%)\\nRandom (33.3%)\\n\\nFigure 1: Agreements with MT-BENCH HUMAN JUDGMENTS with different LLM 1judges. \"CoT\"\\nindicates the judgments derived using the original Chain-of-thought (CoT) prompting, and \"Ours\"\\nindicates the judgments derived using our proposed RocketEval framework.\\n\\ninclude validating the effectiveness of LLM development and assisting users in selecting the best-\\nperforming models for specific tasks, there is a growing demand for more efficient, reproducible,\\nand interpretable evaluations of LLMs. This demand has led to the proposal of various methods\\n(Wang et al., 2024b; Kim et al., 2024a) and benchmarks (Lin et al., 2025; Zheng et al., 2023) aimed\\nat achieving reliable and efficient automated LLM evaluation.\\n\\nPros and cons of the existing automated LLM evaluation paths. Automated LLM evaluation\\nmethods can generally be categorized into three primary types:\\n\\nMultiple-Choice Questions (MCQ) and Keyword Matching Benchmarks. These methods evaluate\\nthe accuracy of a model’s responses by designing a series of closed-ended questions and comparing\\nthe model’s answers to a predefined standard ground truth. Benchmarks constructed using this\\napproach have proven effective for quickly assessing a model’s capabilities across several tasks,\\nsuch as reasoning (Zellers et al., 2019; Qiu et al., 2020), comprehension (Mihaylov et al., 2018; Liu\\net al., 2024), and knowledge retention (Hendrycks et al., 2021; Liang et al., 2023). However, the\\nrequirement for specific response formats limits the comprehensiveness of this evaluation method.\\nIn practice, most LLM applications involve response styles that differ significantly from simple\\nchoices and keywords. Notably, many open-ended tasks cannot be judged by a fixed ground truth,\\nwhich strictly limits the applicability of this approach.\\n\\nLLM-as-a-Judge. Early studies have attempted to adopt language models in automated evaluation\\n(Zhang* et al., 2020; Yuan et al., 2021). Given the robust generalization capabilities of LLMs,\\nemploying a powerful LLM as an evaluator has emerged as a viable solution. This method typically\\ninvolves prompting an LLM to serve as a judge, evaluating the responses from different LLMs to\\na set of well-designed queries. A crucial prerequisite for LLM-as-a-Judge is that the LLM must\\npossess sufficient capability to fully comprehend the queries and discern the quality of different\\nresponses. Consequently, many existing benchmarks (Zheng et al., 2023; Dubois et al., 2023; Li\\net al., 2024a; Liu et al., 2023) tend to employ the most powerful proprietary LLMs, such as GPT-4o,\\nas the judge. However, the use of these models for evaluation not only incurs high costs but also\\nraises other issues such as reproducibility and data privacy.\\n\\nFine-Tuned Judge Models. These models are designed to address the limitations associated with\\npowerful LLM-as-a-Judge approaches. Fine-tuned judge models, derived from lightweight base\\nmodels, are trained with high-quality evaluation data to more closely align with human preferences\\n(Jiang et al., 2024b; Zhu et al., 2025; Wang et al., 2024b; Kim et al., 2024a). Compared to pro-\\nprietary LLMs, such fine-tuned judge models exhibit competitive evaluation capabilities in a more\\ntransparent and cost-effective manner. However, simply fine-tuning the model on evaluation task\\ndata may degrade other capabilities of the lightweight model, which are already weaker compared\\nto those of powerful proprietary LLMs. This degradation can result in the model failing to correctly\\nunderstand complex instructions in the queries, thereby deteriorating subsequent evaluation perfor-\\nmance (Huang et al., 2024). Additionally, as the capabilities of the base models rapidly evolve and\\n\\n2\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n\\xa0 \\xa0\"Yes\": -0.25\\n\"No\":\\xa0 -1.27\\n\\nNormalized\\xa0 \\xa0 \\xa0\\xa0\\nProbability %\\xa0 \\xa0\\n\\n���\\n\\n���\\n\\n\\xa0 \\xa0What is the date of\\xa0\\n\\n\\xa0 \\xa0the\\xa015th day\\xa0after\\n\\xa0 \\xa0Christmas\\xa0in 2024?\\n\\xa0 \\xa0Think step\\xa0by\\xa0step.\\n\\n\\xa0 \\xa0Answer me with a\\xa0\\n\\xa0 \\xa0JSON\\xa0containing\\n\\xa0 \\xa0\"Thought\"\\xa0and\\n\\n\\xa0 \\xa0\"Answer\".\\n\\n\\xa0 \\xa0A {\"Thought\"���\"Answer\": \"Jan 9 2025\"}\\n\\n\\xa0 \\xa0B Thought: The 15th ��� is 2025-1-9.\\n\\n\\xa0 \\xa0C {\"Thought\"���\"Answer\": \"2024-12-30\"}\\n\\n\\xa0 \\xa0Does the response provide\\n\\n\\xa0 \\xa0detailed analysis?\\n\\n\\xa0 \\xa0Is the response correctly\\xa0\\n\\n\\xa0 \\xa0formatted as JSON?\\n\\n\\xa0 \\xa0Does the response include\\xa0\\xa0\\n\\n\\xa0 \\xa0answer \"Jan 9 2025\"?\\n\\n🤖 X\\xa0🤖 X\\xa0\\xa0 \\xa0X (Response)\\n\\n0\\n82\\n\\n55\\n\\n75 3\\n62\\n\\n99 80 72\\n\\n0\\n82\\n\\n550 82\\n55\\n\\n0 82\\n555 97\\n30\\n\\n\\xa0 \\xa09/10📝\\n👤\\n\\n9.7\\n\\n6.1\\n\\n2.3\\n\\n74\\n\\nChecklist Generation\\n\\nQuery with History\\n\\nAnnotationsResponses\\n\\nChecklist\\n\\n🤖\\n\\nJudgment\\n\\nAlignment\\n\\nScore\\n\\n🤖\\n\\n👤\\n\\n👤\\n\\n🤖\\n\\n�\\n\\n�\\n\\n�\\n\\n🤖\\n\\n🤖\\n\\n🤖\\n\\n🤖\\n\\n📊\\n\\nFigure 2: Illustration of RocketEval framework for automated LLM evaluation. The framework\\nconsists of three components: Checklist Creation, Checklist Grading and Score Prediction.\\n\\nnew data continuously emerges, these judge models may need iterative updates, leading to signifi-\\ncant cost escalations and reproducibility issues.\\n\\nRocketEval: Towards efficient automated LLM evaluation. Building on the aforementioned\\nconcepts, we find a pathway that synergizes the language modeling and human preference align-\\nment capabilities of the most powerful LLMs with the evaluation efficiency of lightweight models.\\nSpecifically, we conduct a thorough analysis of the evaluation capabilities of various large models\\nand observe the following:\\n\\n1. The agreement between LLM judges and humans is significantly influenced by the inherent\\ncapabilities of the LLMs. More powerful LLMs tend to generate evaluation results that\\nmore accurately reflect human preferences.\\n\\n2. Prompt engineering techniques, such as Chain-of-Thought (CoT), exert minimal impact on\\nthe evaluation capabilities of the model, particularly when a lightweight LLM is employed\\nas the judge. High uncertainty and positional bias during the decoding process are potential\\ncontributing factors to this phenomenon.\\n\\nInspired by these observations, we introduce a novel evaluation framework named RocketEval, de-\\nsigned to meet the demands of evaluation scenarios that require high efficiency, low cost, alignment\\nwith human preferences, reproducibility, and interpretability. As illustrated in Figure 2, RocketE-\\nval operates through a three-stage framework to generate evaluations. Initially, an instance-level\\nchecklist is created, providing essential knowledge and critical focus areas to overcome the limi-\\ntations of lightweight LLMs in constructing analyses. Subsequently, lightweight LLMs assess the\\nquality of responses for each checklist item independently, resulting in a multifaceted and unbi-\\nased judgment. The normalized score of each judgment is then collected and aggregated to predict\\nthe final score, aiming to mitigate the uncertainty associated with lightweight LLMs. Considering\\nthe widespread availability of human annotations across various evaluation scenarios, we further\\nintroduce a supervised prediction process to align the scores from lightweight LLMs with these\\nannotations. Experimental results demonstrate that RocketEval significantly enhances agreement\\nwith human judgments, achieving a remarkable Spearman correlation of 0.965 when utilizing the\\nGemma-2-2B model as the judge to rank test models. This offers a comparable solution to GPT-\\n4o at only 2% of the evaluation cost in large-scale evaluation scenarios, rendering it suitable for\\nperforming efficient, reliable, and reproducible LLM evaluations.\\n\\n2 HOW LIGHTWEIGHT LLMS PERFORM AS A JUDGE?\\n\\nIn this section, we verify the capability of lightweight LLMs in automated evaluation by conducting\\na series of experiments employing various LLM judges.\\n\\n2.1 SETUP\\n\\nWe selected two benchmark datasets for our experiments:\\n\\n3\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n• MT-BENCH (Zheng et al., 2023) is a classic benchmark that includes 80 multi-round\\nqueries from multiple domains and uses GPT-4 as the judge for multi-round evaluations.\\n\\n• WILDBENCH (Lin et al., 2025) is a newly released benchmark containing 1,024 real-world\\nuser queries. WildBench first introduces a manually revised checklist as contextual infor-\\nmation to guide the evaluation and uses GPT-4o as the judge.\\n\\nBoth benchmarks include pairwise comparison and point-wise scoring evaluation methods. Since\\nthe results of pairwise comparisons can be derived from comparing the scores derived from point-\\nwise scoring, we focus on the point-wise scoring method. In point-wise setting, both benchmarks\\nprompt the judge to first generate an analysis of the response, followed by a score in 1-10 as the\\nfinal judgment. This can be seen as a chain-of-thought (CoT) (Wei et al., 2022) process, aimed at\\nenhancing the ability of the LLM on the evaluation task that involves a reasoning process.\\n\\n2.2 HOW DO LIGHTWEIGHT LLM JUDGES PERFORM?\\n\\nFirst, we aim to understand the capabilities of different models when they serve as judges. A key\\nmetric for evaluating LLMs’ evaluation capability in aligning user preferences is their agreement\\nwith humans. Here, we measure this agreement using MT-BENCH HUMAN JUDGMENTS (Zheng\\net al., 2023), which provides human-annotated results for MT-BENCH across six test models. Each\\nsample includes a target query, responses from two LLMs, and human-annotated match results (in-\\ncluding ties). We structure the scores obtained in a point-wise manner into the same format by\\ncomparing the scores of each pair of responses. Unlike the previous study (Kim et al., 2024a) that\\nuses sampling decoding to derive results without ties, we follow the setting of Lin et al. (2025) and\\nretain cases with scores difference smaller than 0.1 as ties. We then compared the agreement of\\ndifferent LLM judges with human annotations. The human-human agreement reported by Zheng\\net al. (2023) and the agreement between GPT-4o and human are listed as the baseline. As illus-\\ntrated in Figure 1, agreement between LLMs and humans typically ranges from human-to-human\\nagreement (64.8%) to lower than random outcomes (33.3%). This suggests that the existing evalua-\\ntion method using CoT scoring imposes significant demands on the judges’ abilities. The ability of\\nLLMs, reflected by OPENLLM 2 (Fourrier et al., 2024) scores, significantly impacts its alignment\\nwith human preferences during evaluation, thereby complicating the process of performing efficient\\nand reliable evaluations with lightweight LLMs.\\n\\n2.3 WHERE DO LIGHTWEIGHT LLMS UNDERPERFORM AS A JUDGE?\\n\\nIn this part, our objective is to delve deeper and identify the key components that affect the judg-\\nment of lightweight LLMs when they serve as judges. We start by conducting an analysis of the two\\nprocesses involved in evaluation: analysis generation and scoring. To determine whether a compre-\\nhensive analysis can boost or degrade the performance of judgment, we instruct the LLM judges to\\nscore the responses under three different settings:\\n\\n• CoT: The original Chain-of-Thought style, generating the analysis and score step-by-step.\\n\\n• Direct: The judge is prompted to skip the analysis step and score the response directly.\\n\\n• CoTGPT-4o: We extract the analysis generated by GPT-4o to replace the part in judge\\nmodel’s output, then prompt the judge model to score the response accordingly.\\n\\nThen, the scores of all responses are averaged to derive the score of the tested model for ranking.\\nFor baselines, we derive the score predicted by GPT-4o and Claude-3.5-Sonnet as judges equipped\\nwith CoT. We use the CHATBOT ARENA ELO RATINGS (Hard-prompt English) (Chiang et al., 2024)\\nas the ground-truth rankings of human preferences in these models. Figure 3 shows the scores of\\n12 test models and the Spearman correlation coefficient of ranking lists with GPT-4o when different\\nLLMs served as judge. Also, we convert the scores into pairwise comparison results and com-\\npare the agreement of different LLM judges with strong baselines. The result is shown in Table\\n1. Compared to direct scoring, the process of lightweight models conducting their own analysis\\ndid not yield significant gains. However, utilizing analysis from powerful LLM as the context sig-\\nnificantly improves the evaluation performance in terms of instance-level agreement and list-level\\ncorrelations. This indicates that lightweight judges are capable of calibrating their judgments with\\n\\n4\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n1 3 5 7 9 11\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\nSc\\n\\nor\\ne\\n\\n0.035\\n0.909\\n0.958\\n\\n0.923\\n0.979\\n\\n0.979\\n0.986\\n\\nAnalysis + Score\\n\\n1 3 5 7 9 11\\nModel Ranking in Arena Elo (Hard-EN)\\n\\n0.420\\n0.727\\n0.958\\n\\n0.944\\n0.923\\n\\nOnly Score\\n\\n1 3 5 7 9 11\\n\\n0.958\\n0.979\\n0.979\\n\\n0.979\\n0.979\\n\\nGPT-4o Analysis + Score\\n\\nQwen2-1.5B\\nQwen2-7B\\nQwen2-72B\\n\\nLlama-3-8B\\nLlama-3-70B\\n\\nGPT-4o\\nClaude-3.5-Sonnet\\n\\nFigure 3: WILDBENCH scores predicted by different\\nLLM judges and the ranking correlation with GPT-4o.\\n\\nTable 1: Agreements of different judges\\non WILDBENCH.\\n\\nAgreement with GPT-4o\\n\\nDirect CoT CoTGPT-4o\\n\\nClaude-3.5-Sonnet 60.7%\\n\\nQwen2-1.5B 40.1% 36.4% 70.3%\\nQwen2-7B 43.1% 45.0% 76.6%\\nQwen2-72B 61.0% 61.4% 74.0%\\nLlama-3-8B 46.7% 48.4% 74.0%\\nLlama-3-70B 57.2% 59.3% 74.9%\\n\\nAgreement with Claude-3.5-Sonnet\\n\\nGPT-4o 60.7%\\n\\nQwen2-1.5B 40.6% 36.4% 58.2%\\nQwen2-7B 42.8% 45.3% 60.3%\\nQwen2-72B 61.5% 62.3% 62.9%\\nLlama-3-8B 48.2% 49.2% 61.0%\\nLlama-3-70B 58.3% 62.0% 63.4%\\n\\npowerful LLM when high-quality and comprehensive analysis is provided. In other words, the abil-\\nity of such lightweight LLM judges is limited mostly due to the poor comprehension and analysis\\nprocess when they dealing with hard queries and complicated responses.\\n\\n2.4 WHY DO LIGHTWEIGHT LLMS NOT GOOD AT JUDGING?\\n\\nIn the evaluation scenario, conducting analysis can be viewed as a series of judgments on the target\\nresponse. In order to explore the reasons why the lightweight model are struggling in the conducting\\nanalysis, we further conduct a fine-grained level experiments. Specifically, we transform the process\\nof analyzing into a process of judging on a series of checklist questions. We treat each item in the\\nchecklist provided in WILDBENCH as a independent question, and prompt the LLM judge to make\\ndecisions. Since these questions can be viewed as binary choice questions (for example, \"Does the\\nresponse correctly identify...\"), the judges are asked to simply output \"Yes\" or \"No\" for each question\\nbased on the content of the query and response. Then, we obtain judgments of different LLM judges\\non the checklist questions through repeated sampling, and calculate the ratio of disagreement in dif-\\nferent sampling results among all checklist questions. Figure 4 shows that the ratio of disagreement\\nvaries significantly across different model sizes. Lightweight LLM such as Qwen-2-1.5B, shows an\\ndisagreement ratio exceeding 50% across 3 sampling results. This indicates a high uncertainty in\\nmaking decisions on these checklist questions.\\n\\n1 2 3 4 5 6 7\\nNumber of sampling times\\n\\n0.0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\nRa\\ntio\\n\\n o\\nf d\\n\\nis\\nag\\n\\nre\\nem\\n\\nen\\nt\\n\\nLlama-3-70B\\nLlama-3-8B\\nQwen2-72B\\nQwen2-7B\\nQwen2-1.5B\\n\\nFigure 4: Ratio of disagreement on WILDBENCH\\nwith different number of sampling times.\\n\\n1 2 3 4 5 6 7\\nPosition of the checklist item\\n\\n0.0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\nRa\\ntio\\n\\n o\\nf d\\n\\nis\\nag\\n\\nre\\nem\\n\\nen\\nt\\n\\nLlama3-70B\\nLlama-3-8B\\nQwen2-72B\\nQwen2-7B\\nQwen2-1.5B\\n\\nFigure 5: Ratio of disagreement on WILDBENCH\\nwith checklist items in different positions.\\n\\nWhen lightweight LLM judges conduct the CoT style analysis, this high uncertainty may lead to\\ngreater deviations in the final scoring results and degrade the performance. Although previous stud-\\nies (Li et al., 2024b; Wang et al., 2024a) has demonstrated bias of response order in pairwise compar-\\nisons, bias in the analytical process remains unexplored. Therefore, we want to understand whether\\nthis form can affect the models’ judgment results. We transform the item-level questions from the\\nprevious step into a multi-turn dialogue format, with the sequence consistent with the original check-\\nlist order. To show the impact of different judgment results, we set all previous judgments to \"Yes\"\\nor \"No\" and then compare the disagreements in the current item’s judgment results under the two\\nsettings. As shown in Figure 5, with an increase in the number of previous questions, the proportion\\nof inconsistencies in all models shows a growing trend. Interestingly, the Llama-3 series show an\\noverall higher consistency compared to the Qwen-2 series. Additionally, we notice a correlation\\n\\n5\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nbetween model size and consistency, with smaller models tending to produce higher inconsistency.\\nThis suggests that the uncertainty of lightweight LLMs is more likely to be amplified in the process\\nof setting up sequential analyses with CoT, thus reducing confidence in automated evaluation.\\n\\nFrom the above analysis, we demonstrate that lightweight LLM judges exhibit high uncertainty and\\nposition bias, which can lead to difficulties in making reliable judgments. A feasible and efficient\\nevaluation method should avoid the above defects, which inspired us to propose the RocketEval.\\n\\n3 METHODOLOGY\\n\\nIn this section, based on the analysis on existing evaluation benchmarks, we introduce a new auto-\\nmated LLM evaluation framework named RocketEval. As shown in Figure 2, the entire RocketEval\\nframework can be divided into three stages. First, we employ a powerful LLM such as GPT-4o to\\nconduct meta-analysis and create a checklist for assessing user query. Subsequently, lightweight\\nLLMs are employed to evaluate the checklist items for responses from each test model. Finally, the\\nevaluations for each item are collected to derive the final score, which can be predicted using either\\nan unsupervised arithmetic mean or a supervised predictor learned from annotations.\\n\\n3.1 CHECKLIST CREATION\\n\\nEvaluating LLM responses is challenging for both human evaluators and LLM judges. Human\\nevaluations can be subjective, while LLM judges may struggle with query understanding, detailed\\nanalysis, and interpretation, especially when the lightweight LLMs are employed as the judge. Ex-\\nisting methods improve the accuracy of LLM judgments by providing additional information to the\\nLLM Judge’s analytical process, such as reference answers (Zheng et al., 2023; Dubois et al., 2023)\\nand hand-crafted rubrics (Kim et al., 2024a). However, reference answers are of limited use in open-\\nended queries, while hand-crafted rubrics face a trade-off between labor cost, generalizability, and\\naccuracy. Therefore, here we follow WILDBENCH (Lin et al., 2025) to create a instance-specific\\nchecklist, guiding judge LLMs in evaluation. The checklist items are expected to have the follow-\\ning characteristics: 1) Relevance to the topic of the query. 2) Capability to effectively distinguish\\nbetween different responses. 3) Independence from each other, as independent questions can ensure\\na complete evaluation of the response’s quality. Essentially, checklist questions can be considered\\nas a distillation of knowledge from powerful LLMs to prompt the lightweight judges. A question\\nlike \"Does the response include the correct reasoning steps/final answer as X?\" can be helpful when\\nlightweight LLMs are struggling to identify all key factors or solve the problem by itself. We em-\\nploy GPT-4o as the checklist creator, with 5-10 questions created for each instance. This process\\nonly needs to be executed once, while the checklist can be leveraged by lightweight LLM judge to\\nevaluate any number of responses. The prompts used are shown in Appendix A.1.\\n\\n3.2 CHECKLIST GRADING\\n\\nIn Section 2.3, we examine the limitations of employing lightweight LLMs as judges, particularly\\nthe issues of high uncertainty and positional bias. To address these challenges, we propose the\\nfollowing two evaluation procedures when using lightweight LLMs as the judge.\\n\\nIndependent Checklist Item Judgment. To avoid interference from the judgments on other check-\\nlist questions, we prompt the LLM judge to evaluate each question in the checklist independently.\\nWhile this method requires multiple queries per instance, the computational cost can be significantly\\nreduced by leveraging prefix caching (Zheng et al., 2024) since they share the same prefix.\\n\\nNormalized Score. Given the high uncertainty associated with lightweight LLMs, relying solely\\non binary outcomes such as \"Yes\" or \"No\" can result in significant errors in the final judgment. To\\nmitigate this error, we introduce the conditional normalized score as the basis for judgment, which\\naccounts for the certainty of the result. Assuming the probability of output token t from judge\\nLLM parameterized by θ with context x is pθ(t|x), with the target query with context x and the\\ncorresponding response y. Then the conditional normalized score on checklist item c is defined as:\\n\\np̂(x,y, c) =\\npθ(Yes|x,y, c)\\n\\npθ(Yes|x,y, c) + pθ(No|x,y, c)\\n. (1)\\n\\nIn this manner, the judgments with low certainty have less significant impact on the final judgment.\\n\\n6\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n3.3 SCORE PREDICTION\\n\\nAfter obtaining all judgments for a single query-response instance, we can predict the final score\\nby the normalized scores of all checklist items. Given the checklist c = [c1, c2, ..., cN ] ∈ C and\\nthe normalized scores p = [p̂(x,y, c1), p̂(x,y, c2), ..., p̂(x,y, cN )] ∈ P . The score predictor f :\\nS → R can predict the score s by s = f(p). This methodology ensures a more reliable and accurate\\nevaluation by addressing the inherent uncertainties and biases in lightweight LLM judgments. The\\nscore predictor can be any statistical method, hand-crafted rules, or machine learning models. For\\nsimplicity, here we use the arithmetic mean of all normalized scores as the score sunsup:\\n\\nsunsup =\\n\\nN∑\\ni=1\\n\\np̂(x,y, ci). (2)\\n\\nThis method does not require additional data or effort to obtain the predictor. However, checklist\\nitems may have varying impacts on the final results. In many LLM evaluation scenarios, such as\\nLLM development, the benchmark data often comes with annotations from humans and powerful\\nLLMs that serve as the gold standard for evaluation. In this context, we can utilize these annotations\\nto align our checklist judgment results with supervised learning. Specifically, we consider the judg-\\nments on the checklist items as features and the annotations as labels. Given a judgment set with\\nN samples, with features P ∈ PN and labels r ∈ RN , a predictor fsup = minθL(θ;P, r) can\\nbe derived by minimizing any loss function L. The predictor can be any classification or regression\\nmodel, depending on the type of annotations. In this case, we select the Extremely Randomized\\nTree (Geurts et al., 2006) as estimator to learn a robust ensemble predictor with a limited number of\\nannotations and unknown distributions of the judgment results.\\n\\nMeanwhile, we have observed that queries may not always yield ideal separable annotations for\\npredictor learning. Some queries may be too easy, too hard, or have vague descriptions, resulting\\nin similar good or bad performance across all test models. This can cause significant performance\\ndegradation when learning a supervised predictor. Therefore, we propose a strategy to adjust the im-\\npact of the scores predicted by the supervised predictor based on the distribution of the annotations.\\nSpecifically, given the annotations r ∈ R|P|, we define the weight factor αr as\\n\\nαr =\\nϵ−KL(Pr∥Pideal)\\n\\nϵ\\n, ϵ = max\\n\\nX∼RN\\nKL(X∥Pideal), (3)\\n\\nwhere ϵ is the maximum Kullback–Leibler (KL) divergence of any distribution X from the ideal\\ndistribution Pideal. In existing work (Kim et al., 2024a; Murugadoss et al., 2024), rubrics have\\nbeen used as a reference in evaluation, including examples or standards for different rating levels.\\nTherefore, we expect the annotated scores to be varied at different levels, providing the rubrics for\\nthe predictor. Hence, we use the uniform distribution across the score range (for example, 1-10) as\\nPideal ∼ UN .\\n\\nAfter deriving the weight factor αr and a fitted predictor fsup for each query, the final score assigned\\nto the corresponding response is\\n\\nssup = (1− αr)sunsup + αrfsup(p). (4)\\n\\nThis methodology ensures a more reliable and accurate evaluation by addressing the inherent uncer-\\ntainties and biases in the supervised learning process.\\n\\n4 EXPERIMENTS\\n\\nIn this section, we evaluate the effectiveness of the RocketEval framework for automated evaluation\\nof LLMs. Initially, we analyze how well RocketEval aligns with human preferences at both the in-\\nstance and the list levels. Next, we investigate the expenses associated with the evaluation procedure.\\nLastly, we perform an analysis of the checklist’s content and its impact on the evaluation.\\n\\n7\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n4.1 HUMAN AGREEMENT ON THE EVALUATION\\n\\nThe initial step involves comparing the proposed RocketEval framework with the traditional evalu-\\nation method, both with and without the inclusion of Chain of Thought (CoT). The idea of adapting\\nchecklist or aspect to enhance the robustness in LLM evaluation has been widely adopted in existing\\nworks (Lee et al., 2024; Zhou et al., 2024; Fu et al., 2024), where most of them come with a fixed\\nhuman-curated lists to evaluate responses. To further validate the impact of the instance-level check-\\nlist, we introduce a baseline employing six fixed questions as the checklist. These questions, derived\\nby analyzing the MT-BENCH (Zheng et al., 2023) evaluation prompt, encompass dimensions such\\nas helpfulness, relevance, accuracy, depth, creativity, and detail. This baseline is henceforth referred\\nto as the \"Fixed\". Details of the experimental setup are provided in Appendix A.3.\\n\\nInstance-level Agreement. To measure the agreement ratio of different automated evaluation meth-\\nods with MT-BENCH HUMAN JUDGMENTS, we adhere to the settings outlined in Section 2.2. As\\nillustrated in Table 2, the proposed method consistently enhances agreement with human judgment\\nacross various LLMs acting as evaluators. Notably, the latest 7-12B parameter lightweight LLMs,\\nsuch as Llama-3-8B and Mistral-Nemo, achieve over 64% agreement with human judgments and at-\\ntaining a similar level of agreement as 70B open-source LLMs and human-to-human agreement. For\\nsmaller-sized LLMs, which initially exhibit agreement accuracy close to random choice, RocketE-\\nval significantly improves performance to over 60%, outperforming GPT-4. Conversely, the fixed\\nchecklist shows no notable performance enhancement and may even degrade performance. This\\nsuggests that checklist questions tailored to the specific query topic can better provide a knowledge\\ncontext, thereby enhancing the performance of lightweight LLMs as judges.\\n\\nTable 2: Agreement ratios of different LLM judges with\\nMT-BENCH HUMAN JUDGMENTS.\\n\\nMethod CoT Direct Fixed Ours (Unsup.) Ours (Sup.)\\n\\nBaseline\\nGPT-4 (Pairwise): 65.8% GPT-4 (Single): 59.6%\\nHuman-to-human: 64.7% GPT-4o: 66.6%\\nPrometheus-7B-v2.0: 55.7%\\n\\nLlama-3-70B 60.4% 64.8% - - -\\nQwen2-72B 63.6% 64.0% - - -\\n\\nMistral-Nemo 57.8% 53.0% 53.1% 63.2% 64.2%\\nLlama-3-8B 55.6% 51.4% 46.9% 63.8% 62.9%\\nQwen2-7B 54.7% 47.6% 42.1% 58.6% 59.8%\\nMistral-7B-v0.3 55.3% 54.4% 42.8% 58.8% 58.3%\\n\\nPhi-3-mini-4k 52.8% 33.8% 42.1% 61.2% 60.9%\\nQwen2.5-3B 49.8% 52.2% 35.6% 57.4% 58.7%\\nLlama-3.2-3B 53.2% 26.6% 54.1% 58.6% 58.8%\\nGemma-2-2B 37.9% 39.2% 43.9% 57.9% 57.3%\\nInternLM2.5-1.8B 29.2% 22.3% 38.4% 51.7% 48.4%\\n\\nQwen2.5-1.5B 31.1% 25.1% 46.5% 60.7% 60.2%\\nQwen2-1.5B 30.4% 21.7% 41.0% 56.2% 55.6%\\nLlama-3.2-1B 32.6% 22.5% 36.5% 33.6% 41.9%\\nQwen2.5-0.5B 24.9% 25.1% 40.8% 54.3% 50.9%\\n\\nM\\nist\\n\\nra\\nl-N\\n\\nem\\no\\n\\nLl\\nam\\n\\na-\\n3-\\n\\n8B\\nPh\\n\\ni-3\\n-m\\n\\nin\\ni-4\\n\\nk\\nQ\\n\\nw\\nen\\n\\n2.\\n5-\\n\\n3B\\nLl\\n\\nam\\na-\\n\\n3.\\n2-\\n\\n3B\\nGe\\n\\nm\\nm\\n\\na-\\n2-\\n\\n2B\\n\\nIn\\nte\\n\\nrn\\nLM\\n\\n2.\\n5-\\n\\n1.\\n8B\\n\\nQ\\nw\\n\\nen\\n2.\\n\\n5-\\n1.\\n\\n5B\\nLl\\n\\nam\\na-\\n\\n3.\\n2-\\n\\n1B\\nQ\\n\\nw\\nen\\n\\n2.\\n5-\\n\\n0.\\n5B\\n\\nMistral-Nemo\\n\\nLlama-3-8B\\n\\nPhi-3-mini-4k\\n\\nQwen2.5-3B\\n\\nLlama-3.2-3B\\n\\nGemma-2-2B\\n\\nInternLM2.5-1.8B\\n\\nQwen2.5-1.5B\\n\\nLlama-3.2-1B\\n\\nQwen2.5-0.5B\\n\\n99 97 93 95 97 81 93 83 86\\n\\n93 96 93 95 97 83 93 79 85\\n\\n62 50 93 95 93 88 93 88 83\\n\\n72 69 31 97 93 75 100 74 92\\n\\n81 76 40 69 93 83 97 76 87\\n\\n83 72 86 46 65 79 93 81 83\\n\\n6 19 23 35 24 4 75 79 61\\n\\n46 50 77 28 53 71 51 74 92\\n\\n5 -4 -1 32 24 6 0 9 67\\n\\n-38 -36 -19 -25 -9 -23 -6 8 30\\n\\nCoT\\n\\nRocketEval (Sup.)\\n\\nFigure 6: Spearman correlation\\n(in percentage) of test model\\nrankings on WILDBENCH across\\ndifferent LLM judges.\\n\\nList-level Correlation. A critical objective of LLM evaluation is to compare the performance of\\nLLM in specific or general scenarios, which makes the final ranking of LLM essential. Therefore,\\nwe conduct the experiment to compare the LLM rankings derived from different approaches. The de-\\ntailed setup and results are elaborated in Appendix A.3. Table 3 presents the correlation coefficients\\nof the score rankings on WILDBENCH with the CHATBOT ARENA ELO RATING (Hard Prompt - En-\\nglish). The table clearly indicates that RocketEval significantly improves the quality of by achieving\\na higher correlation with human annotations. Specifically, when Mistral-Nemo is used as a judge, the\\nSpearman correlation reaches 0.986, surpassing GPT-4o. The smaller Gemma-2-2B also achieves\\na correlation of 0.965, surpassing Qwen2-72B and the fine-tuned judge model Prometheus-7B-v2.0\\n(Kim et al., 2024b). Furthermore, the supervised version of RocketEval achieves higher correlations\\ncompared to the unsupervised version. Compared to instance-level agreement, the supervised score\\ndata provides more significant improvements in list-level correlation. In addition to the agreement\\nwith humans, we compare the correlations between different lightweight LLM judges. As shown in\\nFigure 6, RocketEval brings significant improvements in cross-judge correlation, especially on the\\nlightweight LLMs. It suggests RocketEval exhibits high consistency and reliability in evaluation.\\n\\n8\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nTable 3: Correlation of ranking with CHATBOT ARENA ELO RATING (Hard prompts-English) on\\nWILDBENCH dataset. \"Kend.\" and \"Spea.\" are denoted as Kendall’s Tau and Spearman correlation\\ncoefficient respectively.\\n\\nMethod CoT Direct Fixed Ours (Unsup.) Ours (Sup.)\\nCoefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\\n\\nGPT-4o 0.909 0.979 - - - - - - - -\\nPrometheus-7B-v2.0 0.848 0.949 - - - - - - - -\\nLlama-3-70B 0.909 0.979 0.787 0.923 - - - - - -\\nQwen2-72B 0.848 0.958 0.848 0.958 - - - - - -\\n\\nMistral-Nemo 0.870 0.956 0.788 0.909 0.879 0.958 0.939 0.986 0.939 0.986\\nLlama-3-8B 0.818 0.923 0.848 0.944 0.879 0.972 0.909 0.979 0.909 0.979\\nQwen2-7B 0.788 0.909 0.545 0.727 0.636 0.804 0.758 0.895 0.818 0.930\\nMistral-7B-v0.3 0.727 0.895 0.545 0.699 0.515 0.678 0.758 0.874 0.818 0.930\\nPhi-3-mini-4k 0.424 0.587 0.576 0.762 0.182 0.273 0.788 0.916 0.848 0.958\\nQwen2.5-3B 0.697 0.839 0.848 0.951 0.727 0.895 0.848 0.944 0.848 0.944\\nLlama-3.2-3B 0.606 0.797 0.848 0.951 0.818 0.937 0.848 0.944 0.848 0.944\\nGemma-2-2B 0.636 0.818 0.758 0.888 0.727 0.867 0.879 0.965 0.879 0.965\\nInternLM2.5-1.8B 0.121 0.175 0.273 0.357 0.273 0.427 0.576 0.748 0.606 0.769\\nQwen2.5-1.5B 0.394 0.517 0.273 0.364 0.606 0.727 0.818 0.923 0.848 0.944\\nQwen2-1.5B -0.061 0.035 0.303 0.420 -0.061 -0.014 0.455 0.622 0.667 0.825\\nLlama-3.2-1B 0.091 0.133 0.152 0.182 -0.273 -0.357 -0.273 -0.357 0.697 0.846\\nQwen2.5-0.5B -0.212 -0.315 0.424 0.503 0.394 0.587 0.667 0.811 0.758 0.895\\n\\n4.2 EVALUATION COST ESTIMATION\\n\\nIn this section, we analyze the costs of various evaluation methods when different LLMs serve as\\njudges. For the purpose of this analysis, we assume that all responses required for evaluation are\\npre-generated, thereby excluding the inference costs for generating these responses.\\n\\nIn RocketEval, the evaluation process is comprised of two primary components: checklist genera-\\ntion and checklist grading. The supervised evaluation method incorporates an additional fitting and\\nprediction process, whose costs are minimal since no LLM inference is involved. For proprietary\\nLLMs serving as judges, we calculate the number of input and output tokens required for a single\\nevaluation and derive the cost based on the official pricing 2. For open-source LLMs, we deploy\\nthem on NVIDIA RTX A5000 GPUs using vLLM (Kwon et al., 2023), and the cost is calculated\\nbased on the average execution time and the rental price of the GPU. We reference the pricing on\\nRunPod 3 at $0.36 per hour. To maximize efficiency, experiments are conducted in batch mode.\\n\\nIn practical evaluation scenarios, various LLMs, each with distinct tuning options, decoding settings,\\nand prompting techniques, can yield numerous versions of responses. Consequently, evaluations on\\nthe same benchmark can be performed hundreds or even thousands of times across different models\\nand their respective versions. We therefore compare the cost of LLM evaluation methods with\\ndifferent number of tests N . As shown in Table 4, the cost incurred for generating a checklist for\\neach question is equivalent to the expense of running a single test using GPT-4o. Since the checklist\\ngeneration is a one-time process, the cost of the checklist grading process becomes increasingly\\nsignificant as the number of tests escalates. For instance, conducting 1000 tests on the WILDBENCH\\nwould incur a cost of $3400 when using GPT-4o as the evaluator, whereas employing Llama-3-8B\\nwould only require $71, with achieving a higher correlation with human preferences.\\n4.3 QUALITATIVE ANALYSIS\\n\\nChecklist Statistical Analysis. To discern the distribution of checklist items, we extracted the re-\\nlationships within the knowledge graphs associated with each checklist item, including the subject,\\npredicate, and object, in a format consistent with the methodology introduced by Sun et al. (2024).\\nWe conducted a statistical analysis of all predicates in the checklist items generated from WILD-\\nBENCH (all predicates have been lemmatized). Further statistics are provided in Appendix A.5. As\\nshown in Figure 7, the predicate distribution within checklist items reflects the intrinsic demands\\nof various original questions. Predominant predicates such as \"Include\" and \"Provide\" underscore\\nthe necessity for comprehensive and supportive responses, ensuring that all pertinent information is\\nconsidered. This is crucial for addressing the complex and multifaceted nature of questions across\\nvarious general tasks. Meanwhile, predicates such as \"Calculate\" and \"Specify\" highlight the pre-\\n\\n2https://openai.com/api/pricing/\\n3https://www.runpod.io/pricing\\n\\n9\\n\\nhttps://openai.com/api/pricing/\\nhttps://www.runpod.io/pricing\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nTable 4: Evaluation cost on WILDBENCH with different LLM judges.\\n\\nMethod LLM Judge Deploying\\nEnvironment Price Usage for\\n\\nSingle Test\\nExtra\\nCost\\n\\nTotal Cost for N Tests\\n\\nN=10 N=100 N=1000\\n\\nCoT\\nGPT-4o\\n(20240806) proprietary\\n\\nI/O: $1.25 / $5.00\\n/ 1M Tokens I/O:\\n\\n1.84M/220k\\ntokens\\n\\nN/A\\n$34.0 $340 $3400\\n\\nGPT-4o-mini\\n(20240718)\\n\\nI/O: $0.075 / $0.30\\n/ 1M Tokens $2.00 $20.0 $200\\n\\nRocketEval\\nLlama-3-70BAWQ 4 x A5000 $1.44 / hours 3760s\\n\\n$2.87*\\n$15.1 $125 $1224\\n\\nLlama-3-8B 1 x A5000 $0.36 / hours 685s $3.55 $9.72 $71.4\\nGemma-2-2B 1 x A5000 $0.36 / hours 248s $3.12 $5.35 $27.7\\nQwen2.5-1.5B 1 x A5000 $0.36 / hours 165s $3.04 $4.52 $19.4\\n\\n*The checklist generation process on WILDBENCH consumes 1.38M input tokens and 228k output tokens on GPT-4o.\\n\\ncision required in quantitative and advisory responses. This distribution pattern not only guides the\\nassessment of answer quality but also ensures that responses meet the specific criteria of each ques-\\ntion type, thereby enhancing the overall reliability and applicability of the information conveyed.\\n\\nChecklist Item Reweighting. Adjusting the weights of checklist items is crucial for accurately\\nassessing the effectiveness of responses to the original question, especially when a gold standard is\\navailable. Initially, each checklist item was given equal weight, assuming an equal impact on the\\noverall assessment. However, this does not accurately reflect the true importance of each item in\\nvalidating the response’s accuracy and completeness. As shown in Figure 8, Assigning the value\\nof radial stress in item 1 (0.296) is critical, being a core part of the answer, significantly impacts\\nsubsequent calculations and the overall analysis. In contrast, item 3 (0.069), which calculates the\\ninner radius, is fundamental but simple, less prone to error, and less critical than stress calculations,\\nthus bearing a lower weight. By reweighting, we emphasize the steps that are the most crucial to\\nthe accurate responses, ensuring that the evaluation process is both rigorous and reflective of the\\nactual importance of each component, which leads to more reliable and credible results, ultimately\\nenhancing the overall validity and consistency with the gold standard of the assessment.\\n\\n15%\\n15%\\n\\n9%\\n\\n8%\\n\\n6%\\n5%\\n\\n5% 5% 3%\\n3%\\n\\n3%\\n3%\\n2%\\n\\n18%\\n\\nInclude\\nProvide\\n\\nExplain\\n\\nMention\\n\\nDescribe\\n\\nIdentify\\nAddress Discuss\\n\\nMaintain\\nUse\\nSuggest\\nCalculate\\nSpecify\\n\\nOthers\\n\\nFigure 7: The predicates used in\\nchecklist items on WILDBENCH.\\n\\n1\\n Does  the response  calculate  the radial stress ( r)\\n\\nusing the correct formula \\\\( \\\\sigma_r = -P \\\\) and provide\\nthe correct value of -5 MPa?\\n\\n2\\n Does  the  response  calculate  the tangential stress\\n\\n( t) using the correct formula \\\\( \\\\sigma_t = \\\\frac{Pr}{t}\\n\\\\) and provide the correct value of 375 MPa?\\n\\n3 Does  the response  correctly identify  and use the inner\\nradius as 150 mm (half of the diameter)?\\n\\n4 Does the  response  apply the principle correctly to find\\nthe tangential strain ( t)?\\n\\n5\\n Does the response include the correct Poisson\\'s ratio ( )\\n\\nand modulus of elasticity (E) in relevant strain\\ncalculations?\\n\\n6\\n Does the response mention the necessary calculation for\\n\\nthe initial volume of the cylinder \\\\( V_0 \\\\) in terms of\\nthe unspecified height (h)?\\n\\n7\\n\\n Does the response correctly calculate the volumetric\\nstrain ( v) by  summing individual strains and use that to\\n\\nexpress the change in volume ( V) in reference to the\\ninitial volume?\\n\\n A thin-walled cylinder has a diameter\\nof 300 mm and a wall thickness of 2 mm.\\nThe cylinder is subjected to an internal\\npressure of 5 MPa. The material of the\\ncylinder has a Poisson\\'s ratio (v) of\\n0.3 and a modulus of elasticity of 200\\nGPa. Determine:\\na. The radial stress and tangential\\nstress in the cylinder.\\nb. The volumetric strain and the change\\nin volume of the cylinder due to the\\ninternal pressure\\n\\n0.296\\n\\n0.272\\n\\n0.069\\n\\n0.146\\n\\n0.064\\n\\n0.094\\n\\n0.059\\n\\nFigure 8: Visualization of checklist item reweighting. (De-\\nfault weight is the reciprocal of the number of checklists)\\n\\n5 CONCLUSION\\n\\nIn this paper, we introduce RocketEval, an innovative evaluation framework that uses lightweight\\nLLMs to achieve high efficiency, low cost, interpretability, and alignment with human preferences.\\nBy reframing the evaluation task as a multi-faceted Q&A using instance-specific checklists, we ad-\\ndressed the challenges of high uncertainty and positional bias inherent in lightweight LLMs. Our\\nmethod demonstrated a high correlation with human preferences, achieving a Spearman correla-\\ntion of 0.965 with the Gemma-2-2B model, comparable to GPT-4o, but at a fraction of the cost.\\nThis significant cost reduction makes RocketEval a feasible solution for large-scale evaluation and\\ncomparison scenarios.\\n\\n10\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nACKNOWLEDGMENTS\\n\\nThis work was partially supported by the National Natural Science Foundation of China (Project No.\\n62202122 and No. 62073272), the Shenzhen Science and Technology Program under Grant No.\\nGXWD20231130110308001, and the Guangdong Basic and Applied Basic Research Foundation\\nunder Grant No. 2024A1515011949.\\n\\nREFERENCES\\n\\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng\\nLi, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot\\narena: an open platform for evaluating llms by human preference. In Proceedings of the 41st\\nInternational Conference on Machine Learning, ICML’24. JMLR.org, 2024. URL https://dl.\\nacm.org/doi/abs/10.5555/3692070.3692401.\\n\\nYann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba,\\nCarlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simu-\\nlation framework for methods that learn from human feedback. In A. Oh, T. Nau-\\nmann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neu-\\nral Information Processing Systems, volume 36, pp. 30039–30069. Curran Associates,\\nInc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\\n5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf.\\n\\nClémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open\\nllm leaderboard v2. https://huggingface.co/spaces/open-llm-leaderboard/open_llm_\\nleaderboard, 2024.\\n\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. GPTScore: Evaluate as you desire.\\nIn Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Confer-\\nence of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies (Volume 1: Long Papers), pp. 6556–6576, Mexico City, Mexico, June\\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.365. URL\\nhttps://aclanthology.org/2024.naacl-long.365/.\\n\\nPierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine Learning,\\n63(1):3–42, Apr 2006. ISSN 1573-0565. doi: 10.1007/s10994-006-6226-1. URL https://doi.\\norg/10.1007/s10994-006-6226-1.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\\nSteinhardt. Measuring massive multitask language understanding. In Proceedings of the Interna-\\ntional Conference on Learning Representations (ICLR), 2021.\\n\\nHui Huang, Yingqi Qu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, and Tiejun Zhao. On the\\nlimitations of fine-tuned judge models for llm evaluation, 2024. URL https://arxiv.org/pdf/\\n2403.02839.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gi-\\nanna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-\\nAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le\\nScao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\nMixtral of experts, 2024a. URL https://arxiv.org/abs/2401.04088.\\n\\nDongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. TIGER-\\nScore: Towards building explainable metric for all text generation tasks. Transactions on Ma-\\nchine Learning Research, 2024b. ISSN 2835-8856. URL https://openreview.net/forum?\\nid=EE1CBKC0SZ.\\n\\nSeungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun,\\nSeongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-\\ngrained evaluation capability in language models. In The Twelfth International Conference on\\nLearning Representations, 2024a. URL https://openreview.net/forum?id=8euJaTveKw.\\n\\n11\\n\\nhttps://dl.acm.org/doi/abs/10.5555/3692070.3692401\\nhttps://dl.acm.org/doi/abs/10.5555/3692070.3692401\\nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf\\nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf\\nhttps://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\\nhttps://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\\nhttps://aclanthology.org/2024.naacl-long.365/\\nhttps://doi.org/10.1007/s10994-006-6226-1\\nhttps://doi.org/10.1007/s10994-006-6226-1\\nhttps://arxiv.org/pdf/2403.02839\\nhttps://arxiv.org/pdf/2403.02839\\nhttps://arxiv.org/abs/2401.04088\\nhttps://openreview.net/forum?id=EE1CBKC0SZ\\nhttps://openreview.net/forum?id=EE1CBKC0SZ\\nhttps://openreview.net/forum?id=8euJaTveKw\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nSeungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Gra-\\nham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source\\nlanguage model specialized in evaluating other language models. In Yaser Al-Onaizan, Mohit\\nBansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Meth-\\nods in Natural Language Processing, pp. 4334–4353, Miami, Florida, USA, November 2024b.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.248. URL\\nhttps://aclanthology.org/2024.emnlp-main.248/.\\n\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\\nserving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Princi-\\nples, SOSP ’23, pp. 611–626, New York, NY, USA, 2023. Association for Computing Machinery.\\nISBN 9798400702297. doi: 10.1145/3600006.3613165. URL https://doi.org/10.1145/\\n3600006.3613165.\\n\\nYukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, and Pilsung Kang. Checkeval: Robust\\nevaluation framework using large language model via checklist, 2024. URL https://arxiv.\\norg/abs/2403.18771.\\n\\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gon-\\nzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and\\nbenchbuilder pipeline, 2024a. URL https://arxiv.org/abs/2406.11939.\\n\\nZongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang\\nLiu. Split and merge: Aligning position biases in LLM-based evaluators. In Yaser Al-Onaizan,\\nMohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical\\nMethods in Natural Language Processing, pp. 11084–11108, Miami, Florida, USA, November\\n2024b. Association for Computational Linguistics. URL https://aclanthology.org/2024.\\nemnlp-main.621.\\n\\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\\nmodels. Transactions on Machine Learning Research, 2023. URL https://openreview.net/\\npdf?id=iO4LZibEqW.\\n\\nBill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Abhilasha Ravichander, Valentina Pyatkin, Nouha\\nDziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking LLMs with challenging tasks\\nfrom real users in the wild. In The Thirteenth International Conference on Learning Representa-\\ntions, 2025. URL https://openreview.net/forum?id=MKEHCx25xp.\\n\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG\\nevaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika\\nBali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\\nProcessing, pp. 2511–2522, Singapore, December 2023. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/2023.emnlp-main.153. URL https://aclanthology.org/2023.\\nemnlp-main.153.\\n\\nZihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catan-\\nzaro. ChatQA: Surpassing GPT-4 on conversational QA and RAG. In The Thirty-eighth Annual\\nConference on Neural Information Processing Systems, 2024. URL https://openreview.net/\\nforum?id=bkUvKPKafQ.\\n\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con-\\nduct electricity? a new dataset for open book question answering. In Ellen Riloff, David\\nChiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing, pp. 2381–2391, Brussels, Belgium, oct\\n- nov 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL\\nhttps://aclanthology.org/D18-1260.\\n\\nBhuvanashree Murugadoss, Christian Poelitz, Ian Drosos, Vu Le, Nick McKenna, Carina Suzana\\nNegreanu, Chris Parnin, and Advait Sarkar. Evaluating the evaluator: Measuring llms’ adherence\\nto task evaluation instructions, 2024. URL https://arxiv.org/abs/2408.08781.\\n\\n12\\n\\nhttps://aclanthology.org/2024.emnlp-main.248/\\nhttps://doi.org/10.1145/3600006.3613165\\nhttps://doi.org/10.1145/3600006.3613165\\nhttps://arxiv.org/abs/2403.18771\\nhttps://arxiv.org/abs/2403.18771\\nhttps://arxiv.org/abs/2406.11939\\nhttps://aclanthology.org/2024.emnlp-main.621\\nhttps://aclanthology.org/2024.emnlp-main.621\\nhttps://openreview.net/pdf?id=iO4LZibEqW\\nhttps://openreview.net/pdf?id=iO4LZibEqW\\nhttps://openreview.net/forum?id=MKEHCx25xp\\nhttps://aclanthology.org/2023.emnlp-main.153\\nhttps://aclanthology.org/2023.emnlp-main.153\\nhttps://openreview.net/forum?id=bkUvKPKafQ\\nhttps://openreview.net/forum?id=bkUvKPKafQ\\nhttps://aclanthology.org/D18-1260\\nhttps://arxiv.org/abs/2408.08781\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nJinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and\\nYang You. Mixeval: Deriving wisdom of the crowd from LLM benchmark mixtures. In The\\nThirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https:\\n//openreview.net/forum?id=6A29LUZhfv.\\n\\nZhaopeng Qiu, Xian Wu, and Wei Fan. Automatic distractor generation for multiple choice questions\\nin standard tests. In Proceedings of the 28th International Conference on Computational Linguis-\\ntics, pp. 2096–2106, 2020. URL https://aclanthology.org/2020.coling-main.189/.\\n\\nKai Sun, Yifan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: How knowledgeable\\nare large language models (llms)? aka will llms replace knowledge graphs? In Proceedings\\nof the 2024 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies (Volume 1: Long Papers), pp. 311–325, 2024. URL\\nhttps://aclanthology.org/2024.naacl-long.18/.\\n\\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhu-\\npatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Fer-\\nret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Char-\\nline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin,\\nNikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur,\\nOlivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchi-\\nson, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge,\\nAntonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar,\\nChris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Wein-\\nberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin Herbison, Elisa Bandy, Emma Wang,\\nEric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin,\\nGary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen\\nBatra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway,\\nJetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez,\\nJoost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mo-\\nhamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir\\nSodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leti-\\ncia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Mar-\\ntins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth,\\nMatt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi,\\nMichael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khat-\\nwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Os-\\ncar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko\\nGeorgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana,\\nReza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah\\nPerrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth,\\nSue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Ko-\\ncisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren\\nBarkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao\\nGong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris\\nWarkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine\\nBanks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu,\\nClement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen\\nKenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at a\\npractical size, 2024. URL https://arxiv.org/abs/2408.00118.\\n\\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong,\\nQi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Lun-Wei\\nKu, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 9440–9450, Bangkok,\\nThailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.\\nacl-long.511. URL https://aclanthology.org/2024.acl-long.511.\\n\\nYidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao\\nChen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue\\n\\n13\\n\\nhttps://openreview.net/forum?id=6A29LUZhfv\\nhttps://openreview.net/forum?id=6A29LUZhfv\\nhttps://aclanthology.org/2020.coling-main.189/\\nhttps://aclanthology.org/2024.naacl-long.18/\\nhttps://arxiv.org/abs/2408.00118\\nhttps://aclanthology.org/2024.acl-long.511\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nZhang. PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimiza-\\ntion. In The Twelfth International Conference on Learning Representations, 2024b. URL\\nhttps://openreview.net/forum?id=5Nn2BLV7SB.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V\\nLe, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language mod-\\nels. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Ad-\\nvances in Neural Information Processing Systems, volume 35, pp. 24824–24837. Curran As-\\nsociates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/\\nfile/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.\\n\\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jin-\\ngren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin\\nYang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao,\\nRunji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wen-\\nbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng\\nRen, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,\\nZeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL\\nhttps://arxiv.org/abs/2407.10671.\\n\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text gen-\\neration. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.),\\nAdvances in Neural Information Processing Systems, volume 34, pp. 27263–27277. Curran As-\\nsociates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/\\nfile/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf.\\n\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\\nchine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, 2019.\\n\\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore:\\nEvaluating text generation with bert. In International Conference on Learning Representations,\\n2020. URL https://openreview.net/forum?id=SkeHuCVFDr.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez,\\nand Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In A. Oh,\\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in\\nNeural Information Processing Systems, volume 36, pp. 46595–46623. Curran Associates,\\nInc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\\n91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf.\\n\\nLianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao,\\nChristos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. SGLang: Ef-\\nficient execution of structured language model programs. In The Thirty-eighth Annual Conference\\non Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=\\nVqkAKQibpq.\\n\\nZihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F. Wong, Xiaowei Huang,\\nQiufeng Wang, and Kaizhu Huang. Is your model really a good math reasoner? evaluating\\nmathematical reasoning with checklist, 2024. URL https://arxiv.org/abs/2407.08733.\\n\\nLianghui Zhu, Xinggang Wang, and Xinlong Wang. JudgeLM: Fine-tuned large language models\\nare scalable judges. In The Thirteenth International Conference on Learning Representations,\\n2025. URL https://openreview.net/forum?id=xsELpEPn4A.\\n\\n14\\n\\nhttps://openreview.net/forum?id=5Nn2BLV7SB\\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\\nhttps://arxiv.org/abs/2407.10671\\nhttps://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf\\nhttps://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf\\nhttps://openreview.net/forum?id=SkeHuCVFDr\\nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf\\nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf\\nhttps://openreview.net/forum?id=VqkAKQibpq\\nhttps://openreview.net/forum?id=VqkAKQibpq\\nhttps://arxiv.org/abs/2407.08733\\nhttps://openreview.net/forum?id=xsELpEPn4A\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nA APPENDIX\\n\\nA.1 PROMPTS USED IN ROCKETEVAL\\n\\nPrompts for Checklist Creation. The following prompt is designed to generate an instance-level\\nchecklist.\\n\\nPrompt for Checklist Creation\\n\\n# Instruction\\nYou are an helpful assistant who identifies and summarizes key factors in large\\n\\n↪→ language models (LLMs) evaluation to help humans evaluate LLMs\\n↪→ efficiently.\\n\\nFeed any query into different LLMs, I will get various responses. I need to\\n↪→ know in quick whether these responses follows the instructions and\\n↪→ answers the question in the user query better.\\n\\nI’ll provide you with a user query. Your task is to identify those key factors\\n↪→ that will affect my judgment and summarize them into a list to improve\\n↪→ the efficiency of my evaluation.\\n\\n# Conversation between User and AI\\n<|begin_of_history|>\\n\\n{history}\\n\\n<|end_of_history|>\\n\\n## Current User Query\\n<|begin_of_query|>\\n\\n{user_query}\\n\\n<|end_of_query|>\\n\\n## Reference Response\\n<|begin_of_reference_response|>\\n\\n{reference_response}\\n\\n<|end_of_reference_response|>\\n\\n# Task\\nGiven the above information, I need you to create a binary question list, so\\n\\n↪→ that I can perform an efficient and accurate evaluation through\\n↪→ answering several questions.\\n\\nYour question should be concise and include any necessary key content and\\n↪→ information (such as keywords, formats, correct counts and values) in\\n↪→ the user query or expected to be shown in responses. Your questions\\n↪→ should not only consider evaluating the reference response, but all\\n↪→ possible responses. Avoid creating duplicate, cumbersome or vague\\n↪→ questions. For example, you should ask \"Is this response contain the\\n↪→ correct answer ...\" instead of \"Is this response’s answer correct?\". Ask\\n↪→ fewer questions by aggregating questions with repeated contexts into\\n↪→ one question.\\n\\n## Output Format\\nPlease provide your outputs in the following markdown format by filling in the\\n\\n↪→ placeholders in {{}}:\\n‘‘‘\\n\\n15\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n1. {{question1}}\\n2. {{question2}}\\n...\\n‘‘‘\\n\\nPrompts for Response Judgment. To keep the consistency with the previous work, we adopt the\\nprompt in Lin et al. (2025) and make several modifications to adapt to different settings.\\n\\n• For MT-BENCH, to keep consistent with the original paper which utilizes the reference\\nanswer to guide the judgment, we replace the part of the checklist with the reference answer\\nand modify the prompt correspondingly.\\n\\n• For other benchmarks that utilize pairwise comparison, we use the same prompt as WILD-\\nBENCH, and create the checklist using GPT-4o for each instance in those benchmarks.\\n\\n• When prompting LLM to output the score directly without conducting analysis, we modify\\nthe output format part in the prompt and change the score range from 1-10 to 0-9. This\\naims to avoid the ambiguity of scoring 1 or 10 when only the first token is captured.\\n\\nThe prompts with different versions of the modification are listed as follows.\\n\\nPrompts for Response Judgment\\n\\n# Instruction\\nYou are an expert evaluator. Your task is to evaluate the quality of the\\n\\n↪→ responses generated by AI models.\\nWe will provide you with the user query and an AI-generated responses.\\nYou should first read the user query and the conversation history carefully for\\n\\n↪→ analyzing the task, and then evaluate the quality of the responses\\n↪→ based on and rules provided below.\\n\\n# Conversation between User and AI\\n## History\\n<|begin_of_history|>\\n{history}\\n<|end_of_history|>\\n## Current User Query\\n<|begin_of_query|>\\n{user_query}\\n<|end_of_query|>\\n\\nChecklist disabled\\n\\n## Reference Response\\n<|begin_of_reference_response|>\\n{ref_answer}\\n<|end_of_reference_response|>\\n\\n## AI Response\\n<|begin_of_response|>\\n{model_output}\\n<|end_of_response|>\\n\\n# Evaluation\\n\\n16\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nChecklist disabled\\n\\n## Rules\\nYou should first compare the AI response and reference response based on\\n\\n↪→ your analysis of the user queries and the conversation history,\\n↪→ and then provide your assessment by scoring the AI response.\\n\\nChecklist enabled\\n\\n## Checklist\\n<|begin_of_checklist|>\\n{checklist}\\n<|end_of_checklist|>\\nPlease use this checklist to guide your evaluation, but do not limit your\\n\\n↪→ assessment to the checklist.\\n## Rules\\nYou should compare the above response based on your analysis of the user\\n\\n↪→ queries and the conversation history.\\nYou should first write down your analysis and the checklist that you used\\n\\n↪→ for the evaluation, and then provide your assessment according to\\n↪→ the checklist.\\n\\nCoT enabled\\n\\nThe scores are in the range of 1~10, where 1 means the response is very\\n↪→ poor and 10 means the response is perfect.\\n\\nHere are more detailed criteria for the scores:\\n\\n- Score 1~2: The response is very poor and does not make sense at all.\\n- Score 3~4: The response is poor and does help user solve the problem in\\n\\n↪→ a meaningful way.\\n- Score 5~6: The response is fair but has some issues (e.g., factual\\n\\n↪→ errors, hallucinations, missing key information).\\n- Score 7~8: The response is good enough but could be improved in some\\n\\n↪→ ways.\\n- Score 9~10: The response is perfect and provides helpful information\\n\\n↪→ that can help user solve the problem.\\n\\n## Output Format\\nFirst, please output your analysis for the model response, and then\\n\\n↪→ summarize your assessment to two aspects: \"strengths\" and \"\\n↪→ weaknesses\"; Finally, please write down your rating for the\\n↪→ assessment.\\n\\nPlease provide your evaluation results in the following json format by\\n↪→ filling in the placeholders in []:\\n\\n‘‘‘\\n{\\n\\n\"strengths\": \"[analysis for the strengths of the response]\",\\n\"weaknesses\": \"[analysis for the weaknesses of the response]\",\\n\"score\": \"[1~10]\"\\n\\n}\\n‘‘‘\\n\\n17\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nCoT disabled\\n\\nThe scores are in the range of 0~9, where 0 means the response is very\\n↪→ poor and 9 means the response is perfect.\\n\\nHere are more detailed criteria for the scores:\\n\\n- Score 0~1: The response is very poor and does not make sense at all.\\n- Score 2~3: The response is poor and does help user solve the problem in\\n\\n↪→ a meaningful way.\\n- Score 4~5: The response is fair but has some issues (e.g., factual\\n\\n↪→ errors, hallucinations, missing key information).\\n- Score 6~7: The response is good enough but could be improved in some\\n\\n↪→ ways.\\n- Score 8~9: The response is perfect and provides helpful information\\n\\n↪→ that can help user solve the problem.\\n\\n## Output Format\\nPlease output the score directly as a digit from 0-9. Do not output other\\n\\n↪→ text.\\nYour score:\\n\\nPrompts for Checklist Grading. The prompt for grading checklist items is shown below.\\n\\nPrompts for Response Judgment\\n\\n# Instruction\\n\\nYou are an expert evaluator. Your task is to evaluate the quality of the\\n↪→ responses generated by AI models.\\n\\nWe will provide you with the user query and an AI-generated responses.\\nYou should first read the user query and the conversation history carefully for\\n\\n↪→ analyzing the task, and then evaluate the quality of the responses by\\n↪→ answer the question provided below.\\n\\n# Conversation between User and AI\\n\\n## History\\n<|begin_of_history|>\\n\\n{history}\\n\\n<|end_of_history|>\\n\\n## Current User Query\\n<|begin_of_query|>\\n\\n{user_query}\\n\\n<|end_of_query|>\\n\\n## AI Response\\n<|begin_of_response|>\\n\\n{$model_output}\\n\\n<|end_of_response|>\\n\\n# Evaluation\\n\\n18\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n## Question\\n<|begin_of_question|>\\n\\n{checklist_item}\\n\\n<|end_of_question|>\\n\\nPlease answer the given question based on the conversation history and the AI\\n↪→ response. You can only answer ’Yes’ or ’No’.\\n\\nYour answer (Yes/No):\\n\\nA.2 DATASETS AND BASELINES\\n\\nIn addition to the results reported by Zheng et al. (2023) on instance-level agreement and Lin et al.\\n(2025) on list-level correlation, we add more baselines with the following setup:\\n\\n• GPT-4o: We replace GPT-4 with GPT-4o as judge and rerun the experiments using the\\npublicly available code provided by Zheng et al. (2023). For WILDBENCH, we use the\\njudgment of GPT-4o provided by Lin et al. (2025) as the baseline results. All baseline\\nresults are produced following the template of scoring from WILDBENCH to keep the con-\\nsistency.\\n\\n• Prometheus-2: We add the state-of-the-art fine-tuned judge model Prometheus-7B-v2.0\\n(Kim et al., 2024b) as the baseline. This model introduces custom scoring criteria and\\nrubrics to conduct the evaluation. For simplicity, we use the rubrics in WILDBENCH (Lin\\net al., 2025), and set the criteria as \"The response is in high quality and provides correct,\\nrelevant, and helpful information that focuses on the user query.\". The complete prompt is\\nshown below.\\n\\nPrompts for Prometheus-2 judge\\n\\n###Task Description:\\nAn instruction (might include an Input inside it), a response to evaluate, a\\n\\n↪→ reference answer that gets a score of 5, and a score rubric representing\\n↪→ a evaluation criteria are given.\\n\\n1. Write a detailed feedback that assess the quality of the response strictly\\n↪→ based on the given score rubric, not evaluating in general.\\n\\n2. After writing a feedback, write a score that is an integer between 1 and 5.\\n↪→ You should refer to the score rubric.\\n\\n3. The output format should look as follows: \"Feedback: (write a feedback for\\n↪→ criteria) [RESULT] (an integer number between 1 and 5)\"\\n\\n4. Please do not generate any other opening, closing, and explanations.\\n\\n###The instruction to evaluate:\\n{user_query}\\n\\n###Response to evaluate:\\n{model_output}\\n\\n###Reference Answer (Score 5):\\n{ref_answer}\\n\\n###Score Rubrics:\\n[The response is in high quality and provides correct, relevant, and helpful\\n\\n↪→ information that focuses on the user query.]\\nScore 1: The response is very poor and does not make sense at all.\\n\\n19\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nScore 2: The response is poor and does help user solve the problem in a\\n↪→ meaningful way.\\n\\nScore 3: The response is fair but has some issues (e.g., factual errors,\\n↪→ hallucinations, missing key information).\\n\\nScore 4: The response is good enough but could be improved in some ways.\\nScore 5: The response is perfect and provides helpful information that can help\\n\\n↪→ user solve the problem.\\n\\n###Feedback:\\n\\nA.3 DETAILS OF EXPERIMENTS ON LIST-LEVEL CORRELATION\\n\\nTo comprehensively assess the performance of RocketEval, we conduct the experiments by adding\\ntwo additional benchmark datasets, ALPACAEVAL (Dubois et al., 2023) and ARENA-HARD (Li et al.,\\n2024a). The statistics of all benchmarks are listed in Table 5.\\n\\nTable 5: Statistics of benchmark datasets.\\n\\nDataset #Instances Turns QueryLen PromptLen\\n\\nMT-BENCH (Zheng et al., 2023) 160* 1-2 202.2 1123.4\\nWILDBENCH (Lin et al., 2025) 1024 1-5 978.5 3402.1\\nALPACAEVAL (Dubois et al., 2023) 805 1 164.9 164.9\\nARENA-HARD (Li et al., 2024a) 500 1 406.4 406.4\\n\\n*Here we treat each 2-turn dialogues as 2 instances.\\n\\nA.3.1 EXPERIMENTAL SETUP\\n\\nWe draw inspiration from the study by Lin et al. (2025), where 12 models 4 were selected for a\\ncorrelation analysis between their rankings in the CHATBOT ARENA ELO RATING (Hard prompts-\\nEnglish) and WILDBENCH. However, upon closer inspection, we notice that some of the chosen\\nmodels have overlapping elo rating confidence intervals, which may compromise the reliability of\\nthe correlation results. To address this issue, we revise the model selection to ensure that there is\\nno overlap in the 95% CI. Similarly, when selecting test models for other benchmarks, we take into\\naccount the availability of model responses in the official released data and the elo ratings of the test\\nmodels. The elo ratings of the selected models are presented in Figure 9.\\n\\nA.3.2 RESULTS\\n\\nScore Distribution. We visualize the distribution of the scores from all instances in a single bench-\\nmark. Figure 10 shows when lightweight LLMs are employed as the judge and using CoT to grade\\nthe responses, the distribution of scores are highly skewed, which indicates the poor ability of such\\nmodels in distinguish the responses with different qualities. Meanwhile, the scores graded by the\\nsame judge under RocketEval are highly distinguishable and close to the distribution of GPT-4o.\\nAlso, the supervised scorer shows the strong ability to reduce the score deviation, making the score\\ndistribution closer to the ideal distribution like GPT-4o. This suggests the proposed method can\\nprovide valuable context information via checklist and guide the LLM to give differential judgment.\\n\\nElo Ratings. Given that the scores assigned by the same LLM judge can be utilized to establish\\npairwise comparisons between different test model responses, we follow the approach of (Chiang\\net al., 2024) by introducing a match simulator and importing all pairwise comparison results from\\nan LLM judge. Specifically, we convert the scores of two responses to determine the winner, with\\nconsidering differences in scores smaller than 0.1 as ties. For each pair of test models, we derive the\\n\\n4The selected models are gpt-4-turbo-2024-04-09, claude-3-opus-20240229, Meta-Llama-3-70B-Instruct,\\nQwen1.5-72B-Chat, claude-3-sonnet-20240229, mistral-large-2402, dbrx-instruct@together, Mixtral-8x7B-\\nInstruct-v0.1, Meta-Llama-3-8B-Instruct, tulu-2-dpo-70b, Llama-2-70b-chat-hf, Llama-2-7b-chat-hf, gemma-\\n7b-it and gemma-2b-it.\\n\\n20\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n1000\\n\\n1050\\n\\n1100\\n\\n1150\\n\\n1200\\n\\n1250\\n\\nCh\\nat\\n\\nbo\\nt A\\n\\nre\\nna\\n\\n E\\nLO\\n\\n R\\nat\\n\\nin\\ng \\n\\n(H\\nar\\n\\nd-\\nEn\\n\\n)\\n\\nGPT-4o Mini (20240718)\\n\\nYi Large Preview\\n\\nLlama-3-70B-Instruct\\n\\nGPT-4 (0314)\\n\\nYi-1.5-34B-Chat\\nLlama-3-8B-Instruct\\n\\nClaude v1\\nClaude Instant v1\\n\\nVicuna-33B-v1.3\\n\\nVicuna-13B-v1.2\\n\\nMT-Bench\\n\\nGPT-4 Turbo (20240409)\\n\\nClaude 3 Opus (20240229)\\nLlama-3-70B-Instruct\\n\\nClaude 3 Sonnet (20240229)\\n\\nMistral Large (2402)\\nLlama-3-8B-Instruct\\n\\nDBRX Instruct Preview\\n\\nGPT-3.5-Turbo (1106)\\n\\nLlama-2-70B-Chat\\n\\nGemma-7B-It\\n\\nLlama-2-7B-Chat\\n\\nGemma-2B-It\\n\\nAlpacaEval\\n\\nGPT-4 Turbo (20240409)\\n\\nClaude 3 Opus (20240229)\\nLlama-3-70B-Instruct\\n\\nClaude 3 Sonnet (20240229)\\n\\nClaude 3 Haiku (20240307)\\nMistral Large (2402)\\nLlama-3-8B-Instruct\\n\\nDBRX Instruct Preview\\n\\nLlama-2-70B-Chat\\n\\nGemma-7B-It\\n\\nGemma-1.1-2B-It\\n\\nGemma-2B-It\\n\\nArena Hard\\n\\nGPT-4 Turbo (20240409)\\n\\nClaude 3 Opus (20240229)\\nLlama-3-70B-Instruct\\n\\nClaude 3 Sonnet (20240229)\\nReka Core (20240501)\\n\\nMistral Large (2402)\\nLlama-3-8B-Instruct\\n\\nDBRX Instruct Preview\\n\\nLlama-2-70B-Chat\\n\\nGemma-7B-It\\n\\nLlama-2-7B-Chat\\n\\nGemma-2B-It\\n\\nWildBench\\n\\nFigure 9: The scores of selected test models on CHATBOT ARENA ELO RATING (Hard prompts\\nEnglish, 2024-09-17).\\n\\nGPT-4o\\n\\nMistral-Nemo\\n\\nLlama-3-8B\\n\\nQwen2-7B\\n\\nPhi-3-mini-4k\\n\\nGemma-2-2B\\n\\n1 3 5 7 9 1 3 5 7 9\\n\\nQwen2-1.5B\\n\\n1 3 5 7 9\\n\\nMethod\\nCoT RocketEval (Unsup.) RocketEval (Sup.)\\n\\nFigure 10: Distribution of scores on WILDBENCH dataset.\\n\\npairwise comparison results as the match data. Subsequently, the Elo rating is calculated based on all\\nmatch results. We employ the same Bradley-Terry model-based Maximum Likelihood Estimation\\n\\n21\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nTable 6: Correlation of ranking with CHATBOT ARENA ELO RATING (Hard prompts-English) on\\ndifferent benchmarks.\\n\\nMT-BENCH\\n\\nMethod CoT Direct Ours (Unsup.) Ours (Sup.)\\n\\nCoefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\\n\\nGPT-4o Kendall’s Tau (Kend.): 1.0 : Spearman (Spea.): 1.0\\n\\nLlama-3-70B 1.000 1.000 0.944 0.985 - - - -\\nQwen2-72B 1.000 1.000 0.956 0.988 - - - -\\n\\nMistral-Nemo 0.911 0.976 0.867 0.964 0.867 0.952 0.911 0.976\\nLlama-3-8B 0.867 0.952 0.867 0.964 0.822 0.927 0.911 0.976\\nQwen2-7B 0.867 0.952 0.867 0.952 0.778 0.927 0.911 0.976\\nMistral-7B-v0.3 0.733 0.903 0.822 0.927 0.822 0.939 0.822 0.939\\nPhi-3-mini-4k 0.778 0.915 0.822 0.939 0.867 0.952 0.956 0.988\\nQwen2.5-3B 0.764 0.912 0.644 0.867 0.822 0.927 0.822 0.927\\nLlama-3.2-3B 0.867 0.952 0.422 0.600 0.822 0.927 0.956 0.988\\nGemma-2-2B 0.556 0.721 0.629 0.796 0.778 0.915 0.822 0.927\\nInternLM2.5-1.8B 0.378 0.430 -0.296 -0.451 0.644 0.794 0.689 0.855\\nQwen2.5-1.5B 0.556 0.745 -0.068 -0.128 0.778 0.915 0.911 0.976\\nQwen2-1.5B 0.511 0.745 0.523 0.665 0.822 0.927 0.822 0.939\\nLlama-3.2-1B 0.511 0.697 0.114 0.146 -0.067 -0.200 0.733 0.879\\nQwen2.5-0.5B 0.333 0.394 0.422 0.479 0.600 0.733 0.600 0.806\\n\\nALPACAEVAL\\n\\nMethod CoT Direct Ours (Unsup.) Ours (Sup.)\\n\\nCoefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\\n\\nGPT-4o Kendall’s Tau (Kend.): 0.879 : Spearman (Spea.): 0.972\\n\\nMistral-Nemo 0.848 0.958 0.779 0.900 0.848 0.951 0.848 0.951\\nLlama-3-8B 0.758 0.895 0.818 0.923 0.758 0.916 0.758 0.916\\nQwen2-7B 0.667 0.825 0.606 0.734 0.788 0.923 0.818 0.937\\nMistral-7B-v0.3 0.636 0.804 0.515 0.692 0.727 0.881 0.788 0.923\\nPhi-3-mini-4k 0.606 0.762 0.565 0.708 0.818 0.937 0.818 0.944\\nQwen2.5-3B 0.697 0.811 0.667 0.783 0.727 0.888 0.788 0.923\\nLlama-3.2-3B 0.606 0.804 0.515 0.699 0.909 0.979 0.909 0.979\\nGemma-2-2B 0.636 0.818 0.848 0.951 0.939 0.986 0.909 0.979\\nInternLM2.5-1.8B 0.182 0.371 0.455 0.587 0.636 0.804 0.667 0.818\\nQwen2.5-1.5B 0.303 0.483 0.333 0.469 0.879 0.965 0.879 0.965\\nQwen2-1.5B 0.273 0.420 0.303 0.406 0.545 0.706 0.818 0.909\\nLlama-3.2-1B 0.000 -0.063 -0.061 -0.007 0.515 0.720 0.727 0.867\\nQwen2.5-0.5B 0.182 0.287 0.485 0.636 0.606 0.769 0.636 0.818\\n\\nARENA-HARD\\n\\nMethod CoT Direct Ours (Unsup.) Ours (Sup.)\\n\\nCoefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\\n\\nGPT-4o Kendall’s Tau (Kend.): 0.939 : Spearman (Spea.): 0.986\\n\\nMistral-Nemo 1.000 1.000 0.939 0.986 1.000 1.000 0.970 0.993\\nLlama-3-8B 0.909 0.972 0.939 0.986 1.000 1.000 0.970 0.993\\nQwen2-7B 0.901 0.974 0.879 0.965 0.970 0.993 0.970 0.993\\nMistral-7B-v0.3 0.818 0.937 0.818 0.930 0.970 0.993 0.939 0.986\\n\\nPhi-3-mini-4k 0.879 0.958 0.758 0.881 1.000 1.000 1.000 1.000\\nQwen2.5-3B 0.962 0.988 0.939 0.979 0.970 0.993 0.970 0.993\\nLlama-3.2-3B 0.818 0.937 0.333 0.392 0.970 0.993 1.000 1.000\\nGemma-2-2B 0.545 0.692 0.879 0.958 0.939 0.986 0.970 0.993\\nInternLM2.5-1.8B 0.424 0.559 0.504 0.666 0.818 0.937 0.909 0.972\\nQwen2.5-1.5B 0.394 0.476 0.515 0.545 0.970 0.993 0.939 0.986\\nQwen2-1.5B 0.121 0.224 0.636 0.748 0.879 0.965 0.939 0.979\\nLlama-3.2-1B -0.121 -0.147 0.321 0.522 -0.606 -0.727 0.909 0.979\\nQwen2.5-0.5B 0.091 0.098 0.769 0.902 0.576 0.811 0.939 0.986\\n\\n(MLE), as used by (Chiang et al., 2024), to fit the Elo rating and use bootstrap to estimate confidence\\nintervals.\\n\\nFigure 11 illustrates the elo rating from CHATBOT ARENA, ARENA-HARD (Li et al., 2024a) and the\\nresults of judges Gemma-2-2B, Llama-3.2-3B, Qwen2.5-3B, and the ensemble result on the ARENA-\\nHARD benchmark dataset. We utilize the average score and the elo ratings derived by combining all\\nmatches from the three judges. We conduct an experiment on all available responses from Li et al.\\n(2024a) and exclude 10 models used to fit the predictor in RocketEval and the GPT-4o-2024-05-13,\\nwhich is used as the judge to produce labels, resulting in 50 test models. It is evident that the scores\\nand elo ratings produced by the LLM judge follow similar trends and are more closely aligned with\\nthe results derived from human preferences. Meanwhile, we notice that the test models that exhibit\\na large deviation from human judgments belong to the same Llama series, indicating the potential\\nbias on different patterns of responses.\\n\\n22\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n5 15 25 35 45\\nModel Rank in Chatbot Arena (Hard-En)\\n\\n800\\n\\n900\\n\\n1000\\n\\n1100\\n\\n1200\\n\\n1300\\n\\n1400\\n\\nEL\\nO\\n\\n R\\nat\\n\\nin\\ng\\n\\nChatbot Arena Elo (Hard-En)\\nLlama-3.1-405B-Instruct-FP8\\n\\nLlama-3.1-70B-Instruct\\n\\nLlama-3.1-8B-Instruct\\nRocketEval Elo (Combined)\\nArena-Hard-Auto Elo\\nRocketEval Score (Average)\\nCoT Score (Average) 3.0\\n\\n3.5\\n\\n4.0\\n\\n4.5\\n\\n5.0\\n\\n5.5\\n\\n6.0\\n\\nSc\\nor\\n\\ne\\n\\nFigure 11: Scores and elo ratings on ARENA-HARD benchmark dataset. We use the CHATBOT\\nARENA ELO RATING (Hard-En, 2024-09-27) and reproduce the result of ARENA-HARD-AUTO\\nusing the official implementation with the default setting (Judge: GPT-4-1106-preview, Baseline:\\nGPT-4-0314). ARENA-HARD-AUTO Elo, RocketEval elo rating and CoT score are adjusted by\\nscaling and adding a constant value for better visualization.\\n\\nTable 7: Ablation study on instance-level agreement with MT-BENCH HUMAN JUDGMENTS.\\n\\nMethod RocketEval (Unsup.) w/o Norm Score w/o Indep. Judgment w/o Weight Factor RocketEval (Sup.)\\n\\nMistral-Nemo 63.2% 62.6% 63.0% 63.0% 64.2%\\nLlama-3-8B 63.8% 57.5% 60.8% 59.9% 62.9%\\nQwen2-7B 58.6% 49.3% 59.0% 57.8% 59.8%\\nMistral-7B-v0.3 58.8% 47.4% 57.5% 52.9% 58.3%\\n\\nPhi-3-mini-4k 61.2% 55.5% 60.5% 57.4% 60.9%\\nQwen2.5-3B 57.4% 51.3% 59.2% 55.7% 58.7%\\nLlama-3.2-3B 58.6% 48.7% 57.6% 56.5% 58.8%\\nGemma-2-2B 57.9% 49.6% 55.9% 56.5% 57.3%\\nInternLM2.5-1.8B 51.7% 39.7% 42.3% 37.4% 48.4%\\n\\nQwen2.5-1.5B 60.7% 46.3% 55.7% 57.5% 60.2%\\nQwen2-1.5B 56.2% 31.2% 54.3% 52.7% 55.6%\\nLlama-3.2-1B 33.6% 23.4% 40.9% 42.0% 41.9%\\nQwen2.5-0.5B 54.3% 33.4% 49.4% 47.2% 50.9%\\n\\nA.4 ABLATION STUDY\\n\\nTo validate the effectiveness of strategies adopted in RocketEval, we conduct ablation study by\\ntesting the performance on the following variants:\\n\\n• w/o Norm Score: It removes the conditional normalized score and simply use the decoding\\nresult as the judgment.\\n\\n• w/o Indep. Judgment: It inputs the checklist into the LLM judge in a multi-turn format, so\\nthat the LLM can see its previous judgment result when judging on the current checklist\\nitem.\\n\\n• w/o Weight Factor: It sets the weight factor αr to the constant 1.\\n\\nThe results, presented in Tables 7 and 8, demonstrate that incorporating conditional normalized\\nscore consistently enhances the performance of LLM judges, particularly for smaller-sized LLMs.\\nThis observation confirms the high uncertainty associated with lightweight LLMs and supports the\\ninference that introducing conditional normalized score can increase their reliability when serving\\nas judges. Simultaneously, setting the weight factor αr to 1 causes the final score to be entirely\\ndetermined by the supervised predictor. Predictors trained on a limited number of annotations may\\nstruggle to provide accurate scoring results but exhibit superior performance in aligning with human\\n\\n23\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nTable 8: Ablation study on list-level correlation with CHATBOT ARENA ELO RATING (Hard-En) on\\nWILDBENCH dataset.\\n\\nMethod RocketEval (Unsup.) w/o Norm Score w/o Indep. Judgment w/o Weight Factor RocketEval (Sup.)\\n\\nCoefficient Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea. Kend. Spea.\\n\\nMistral-Nemo 0.939 0.986 0.909 0.979 0.879 0.965 0.939 0.986 0.939 0.986\\nLlama-3-8B 0.909 0.979 0.909 0.979 0.848 0.958 0.939 0.986 0.909 0.979\\nQwen2-7B 0.758 0.895 0.758 0.895 0.788 0.916 0.909 0.979 0.818 0.930\\nMistral-7B-v0.3 0.758 0.874 0.758 0.874 0.788 0.881 0.879 0.965 0.818 0.930\\n\\nPhi-3-mini-4k 0.788 0.916 0.758 0.902 0.818 0.930 0.909 0.979 0.848 0.958\\nQwen2.5-3B 0.848 0.944 0.848 0.944 0.848 0.944 0.909 0.979 0.848 0.944\\nLlama-3.2-3B 0.848 0.944 0.848 0.944 0.818 0.930 0.939 0.979 0.848 0.944\\nGemma-2-2B 0.879 0.965 0.879 0.965 0.879 0.965 0.939 0.986 0.879 0.965\\nInternLM2.5-1.8B 0.576 0.748 0.576 0.748 0.545 0.700 0.636 0.790 0.606 0.769\\n\\nQwen2.5-1.5B 0.818 0.923 0.788 0.916 0.697 0.867 0.879 0.951 0.848 0.944\\nQwen2-1.5B 0.455 0.622 0.515 0.643 0.636 0.804 0.758 0.874 0.667 0.825\\nLlama-3.2-1B -0.273 -0.357 -0.212 -0.231 0.606 0.755 0.848 0.923 0.697 0.846\\nQwen2.5-0.5B 0.667 0.811 0.667 0.811 0.697 0.839 0.848 0.951 0.758 0.895\\n\\npreferences at the list level. In such scenarios, the weight factor αr proves to be effective in miti-\\ngating the negative influences of biased annotations, thereby achieving strong performance in both\\ninstance-level agreement and list-level ranking correlation. Meanwhile, although there is a perfor-\\nmance drop in the variant without independent checklist item judgment, the drop is not significant.\\nThis may be due to the fact that the position bias exists in all tests and is further alleviated in sub-\\nsequent score prediction stage. Although the position bias has limited impact on the final prediction\\nresults, the form of multi-round dialogue prevents batch processing during LLM inference, thereby\\nreducing efficiency. In conclusion, we believe that independent checklist judgment in RocketEval\\nremains an optimal choice.\\n\\nA.5 CHECKLIST ANALYSIS\\n\\nAs mentioned in Section 4.3 , we undertake a more detailed examination of the checklists generated\\nby RocketEval on the WILDBENCH. This benchmark can be categorized into five major task types:\\nMath & Data Analysis, Coding & Debugging, Creative Tasks, Information/Advice Seeking, and\\nPlanning & Reasoning.\\n\\nIn this section, we focus on extracting knowledge graph relationships from all the checklists and\\nconducting a comprehensive analysis of these relationships. Furthermore, we investigate instances\\nof checklist item reweighting across a broader spectrum of tasks to provide a more extensive under-\\nstanding of the underlying dynamics.\\n\\nSubject Distribution. As shown in Figure 12, the distribution of subject keywords in validat-\\ning original question responses ensures a universal, compatible, and effective checklist for various\\ntasks. High-frequency keywords like \"explanation\", and \"code\" are crucial for Coding & Debug-\\nging, aiding in verifying code functionality and clarity. Keywords like \"essay\", \"example\", and\\n\"story\" are vital for Creative Tasks and Information/Advice seeking, ensuring creativity, relevance,\\nand clarity. For Planning & Reasoning, keywords such as \"strategy\", \"method\" and \"process\" en-\\nsure comprehensive and practical solutions. In Math & Data Analysis, keywords like \"calculation\",\\n\"algorithm\", and \"solution\" validate mathematical logic and data analysis. Universally applicable\\nkeywords like \"response\", \"explanation\", and \"example\" consistently evaluate clarity, relevance,\\nand accuracy across all tasks. This multifaceted approach ensures a robust, flexible, and thorough\\nevaluation process, enhancing the overall effectiveness and reliability of the responses.\\n\\nTask-Predicate Relationship. We categorized the checklist items according to the types of tasks\\nand conducted a statistical analysis of the corresponding predicates. As shown in Figure 13, the\\ndistribution of predicate keywords within checklists mirrors the distinct demands inherent to dif-\\nferent task categories, such as Coding & Debugging, Planning & Reasoning, Mathematical & Data\\nAnalysis, Information/Advice Seeking, and Creative Tasks. Predicate keywords that are exclusive\\nto certain domains, such as \"handle\" within the context of Coding & Debugging or \"calculate\" in\\nMathematical & Data Analysis, denote actions that are specific and pertinent to those respective\\n\\n24\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\nFigure 12: Distribution of Subjects in checklist items which generated from WILDBENCH.\\n\\nCreative Tasks\\n\\ninclude\\nPlanning & Reasoning\\n\\nprovide\\nCoding & Debugging\\n\\nInformation/Advice seeking mention\\n\\nexplain\\n\\ndiscuss\\n\\ndescribe\\nMath & Data Analysis\\n\\ncalculate\\n\\nmaintain\\n\\naddress\\n\\nidentify\\n\\nhandleoutline\\n\\nuse\\n\\nsuggest\\n\\napply\\n\\nreflect\\n\\nimplement\\n\\nacknowledge\\n\\nstate\\n\\navoid\\n\\ndetail\\n\\ncontain\\nincorporate\\n\\nconsider\\n\\nhave\\n\\nanalyze specify\\n\\nhighlight\\npresent\\n\\nintroduce\\n\\nlist\\n\\nintegrate emphasize\\n\\nFigure 13: Visualization of Task-Predicate Relationship.\\n\\nfields. Conversely, common predicates like \"explain\" and \"describe\" are universally applicable,\\nserving general verification objectives across all question types.\\n\\nThese predicate keywords augment the verification process by imparting explicit and goal-oriented\\ndirectives. Specifically, exclusive keywords concentrate on criteria that are specific to each task\\ntype, ensuring a detailed and contextually relevant assessment. Meanwhile, common keywords\\nensure uniformity and exhaustiveness in the evaluation of responses. This dual strategy ensures\\n\\n25\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n1 Does  the  response  correctly import necessary PyTorch components such as `torch`,\\n`nn`, `optim`, `DataLoader`, and other relevant libraries?\\n\\n2 Is there a GRU-based Encoder class with the `__init__`, `forward`, and `init_hidden`\\nmethods implemented?\\n\\n3 Is  there a GRU-based  Decoder class with `__init__`, `forward`, and an output layer\\nusing `LogSoftmax` or equivalent?\\n\\n4 Does the response set up a comprehensive character-to-index mapping and word pair\\nhandling, specifically through a custom dataset class?\\n\\n5 Is there  an   explanation or  implementation showing  how padding and batching are\\nmanaged in mini-batch training?\\n\\n6 Does the response show a complete training function that includes initializing hidden\\nstates, zeroing gradients, and executing forward and backward passes?\\n\\n7 Are  start  and  end of sequence token indices correctly implemented and used within\\ninput handling in the training function?\\n\\n8 Does  the  response  correctly set  up the training infrastructure including the dataset,\\nDataLoader, model instantiation, and the optimizer?\\n\\n9 Are there clear details of a training loop with epoch iterations, batch processing from\\na DataLoader, and intermediate loss outputs?\\n\\n10 Is the loss  function for  sequence-to-sequence learning in NLP, like CrossEntropy or\\nNLLLoss, clearly explained or demonstrated in the training loop?\\n\\nusing pytorch, implement a character\\nlevel sequence to sequence encoder-\\ndecoder model with GRU . Trained the\\nmodel on a dataset of word pairs with\\nminibatch training.\\n\\n0.050\\n\\n0.228\\n\\n0.212\\n\\n0.085\\n\\n0.050\\n\\n0.130\\n\\n0.050\\n\\n0.050\\n\\n0.093\\n\\n0.050\\n\\nFigure 14: Visualization of checklist item reweighting in Coding & Debugging.\\n\\nthat each answer is evaluated comprehensively and from multiple perspectives, thereby significantly\\nenhancing the efficacy of the verification process.\\n\\nChecklist Item Reweighting Analysis. Here we present more reweighted case examples under a\\nvariety of tasks. As shown in Figure 14, the checklist generated for Coding & Debugging tasks\\nensures the complete generation of the code for the target problem. After reweighting, there is a\\ngreater emphasis on critical steps within the code, such as the structures of the forward, backward,\\nand loss function in a neural network. For Planning & Reasoning tasks, reweighting makes key\\nreasoning items more prominent. For example, As shown in Figure 17, based on the symptoms\\ndescribed in the problem, the response needs to deduce diabetic ketoacidosis (DKA) to provide a\\ncorrect and reasonable treatment in subsequent answers. For other Creative Tasks, see Figure 15,\\nand for Information/Advice seeking tasks, see Figure 16. By reweighting checklist items, we can\\nreasonably focus on critical steps or results across different types of tasks, ensuring effectiveness\\nwhen using lightweight LLM as judges.\\n\\n1 Does the response include dialogue that effectively develops the story and character\\ninteractions?\\n\\n2 Is   the  setting,  Luther\\'s old residence, described to contribute an eerie atmosphere\\nappropriate for a mystery story?\\n\\n3 Is    the   central   mystery   (Luther\\'s murder by suffocation) introduced clearly and\\nmaintained as the focus of the story?\\n\\n4 Do  all suspects  have  distinct  roles or characteristics that contribute to the story\\'s\\ndepth?\\n\\n5 Are the  series  of  murders integrated into the story in a way that escalates tension\\nand drives the narrative forward?\\n\\n6 Does the narrative  convey  an increasing sense of urgency to solve the mystery as\\nthe story progresses?\\n\\n7 Does  the  resolution  of  the  mystery   logically  follow from the clues and events\\ndescribed in the story?\\n\\n8 Is the murderer\\'s identity revealed in a manner consistent with the development of\\nthe plot and character motivations?\\n\\n9 Is  emotional  tension  and  suspicion  among the suspects effectively conveyed to\\nengage the reader?\\n\\n10 Are clues  and  red herrings  integrated within the narrative to challenge the reader\\nwhile keeping the solution attainable and fair?\\n\\nWrite an intricate mystery story,\\nincluding dialogue, about the murder of\\nLuther Watson. All five suspects,\\nChristian Vaughn, Selina Morano, Ted\\nMosley, Blii Paxrott and Samntha\\nWilliams, are gathered at Luther\\'s old\\nresidence. One of them is the murderer\\nand they have to find out who it is. The\\nonly thing they know, is that Luther was\\nsuffocated. Each night one of the\\nsuspects gets killed by the murderer and\\nthe pressure to solve the case\\nincreases.\\n\\n0.058\\n\\n0.083\\n\\n0.123\\n\\n0.046\\n\\n0.062\\n\\n0.074\\n\\n0.176\\n\\n0.046\\n\\n0.102\\n\\n0.229\\n\\nFigure 15: Visualization of checklist item reweighting in Creative Tasks.\\n\\n26\\n\\n\\n\\nPublished as a conference paper at ICLR 2025\\n\\n1\\n Does the response provide a  clear and detailed explanation of how mining\\n\\nengineering can connect to renewable energy resources, including future\\npotential?\\n\\n2 Does the response mention the importance of renewable energy for climate\\nchange mitigation and reducing dependence on fossil fuels?\\n\\n3\\n Does the response specify essential skills for mining engineering\\n\\ngraduates to integrate  with renewable energy fields, such as understanding\\nof raw materials, sustainable practices, and technical skills?\\n\\n4 Does the response  describe  some academic or career positions relevant to\\nmining engineering and renewable energy?\\n\\n5 Does the response include examples of renewable energy technologies like\\nwind turbines, solar panels, and energy storage systems?\\n\\n6\\n Is the response structured  in a clear and well-organized manner, breaking\\ndown different topics such as connection, importance, required skills, and\\n\\ncareer opportunities?\\n\\n how to connect mining engineering to\\nrenewable energy resources and future of\\nthis connection, importance of renewable\\nenergy and its future and what skills of\\na graduate student in mining engineering\\ndoes need to this connection and carrier\\njob and academic positions\\n\\n0.118\\n\\n0.233\\n\\n0.118\\n\\n0.118\\n\\n0.297\\n\\n0.118\\n\\nFigure 16: Visualization of checklist item reweighting in Information/Advice seeking.\\n\\n1 Does   the   response correctly  identify  the  most   likely\\ndiagnosis as diabetic ketoacidosis (DKA)?\\n\\n2\\n Does the response  mention a relevant  test  for\\n\\nproving DKA, specifically mentioning blood ketone levels\\nor beta-hydroxybutyrate?\\n\\n3 Does the response correctly calculate the anion gap using\\nthe  formula: Na+ - (Cl- + HCO3-) and provide the value?\\n\\n4\\n Does the  response  outline  appropriate lines of treatment\\n\\nfor DKA covering fluid resuscitation, insulin therapy,\\nelectrolyte management, and acidosis correction?\\n\\n5\\n Does the response address the patient\\'s infected foot ulcer\\nas a possible secondary infection and suggest appropriate\\n\\ninterventions like antibiotics?\\n\\n6\\n Does the response  mention additional  patient\\n\\neducation on diabetes management and foot care as part of\\nthe treatment plan or prevention of future episodes?\\n\\n7 Is the  response  clear, concise, and appropriately detailed\\nin addressing each part of the original query?\\n\\nA 25-year-old man with type-1 diabetes\\nmellitus on insulin presents to ED with\\nfever,cough, vomiting and abdominal\\npain. Examination reveals tachypnia, dry\\nmucosa,decreased skin turgor, a\\ntemperature of 37.8 °C and infected\\nulcer in the left foot.Investigations\\nshow RPG of 350 mg/dl, Na+ 130 mmol/l,\\nK+ 5.7 mmol/l, Bicarbonate\\n12 mmol/L, Chloride 105 mmol/L, urea 60\\nmg/dl.\\nA. What is the most likely diagnosis?\\nB. Mention one test to prove the\\ndiagnosis.\\nC. What is the value of anion gap in\\nthis patient?\\nD. What are the lines of treatment?\\n\\n0.637\\n\\n0.052\\n\\n0.094\\n\\n0.053\\n\\n0.052\\n\\n0.052\\n\\n0.061\\n\\nFigure 17: Visualization of checklist item reweighting in Planning & Reasoning.\\n\\n27\\n\\n\\n\\tIntroduction\\n\\tHow lightweight LLMs perform as a judge?\\n\\tSetup\\n\\tHow do lightweight LLM judges perform?\\n\\tWhere do lightweight LLMs underperform as a judge?\\n\\tWhy do lightweight LLMs not good at judging?\\n\\n\\tMethodology\\n\\tChecklist Creation\\n\\tChecklist Grading\\n\\tScore Prediction\\n\\n\\tExperiments\\n\\tHuman Agreement on the Evaluation\\n\\tEvaluation Cost Estimation\\n\\tQualitative Analysis\\n\\n\\tConclusion\\n\\tAppendix\\n\\tPrompts used in RocketEval\\n\\tDatasets and Baselines\\n\\tDetails of Experiments on List-level Correlation\\n\\tExperimental Setup\\n\\tResults\\n\\n\\tAblation Study\\n\\tChecklist Analysis\\n\\n\\n.\\n                    \\n                    Extract the following information from the research paper:\\n                    \\n                    - Problem/Research Question: What is the research trying to address or investigate? \\n                    - Methods/Approach: How did the researchers conduct their study? \\n                    - Results/Findings: What did the research uncover? \\n                    - Conclusions/Implications: What are the main takeaways and significance of the research? \\n                    - Limitations and Future Directions: What\\n                    \\n                    Generate a high-engagement Linked In post about the challenges and solutions in the research paper.\\n                    \\n                    Extract keywords and use bing search to explain the keywords in the research paper.                    \\n                    The tone should be authoritative yet engaging. \\n                    Follow the Hook → Context → Insights → Engagement Trigger structure. End with a question to spark discussion. Add 3-5 relevant hashtags.\\n    ', type='TextMessage'), ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=28709, completion_tokens=214), metadata={}, content=[FunctionCall(id='call_tlomk9A0GsqWpUR2MLugyY39', arguments='{\"query\": \"Problem/Research Question of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent'), FunctionCall(id='call_R29Sdqjd21lRzxDtiBtSKQrj', arguments='{\"query\": \"Methods/Approach of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent'), FunctionCall(id='call_wl20TARio3Z6BszejZUgLyfB', arguments='{\"query\": \"Results/Findings of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent'), FunctionCall(id='call_B3X1dwhE4TajPLEVBrRvdtK8', arguments='{\"query\": \"Conclusions/Implications of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent'), FunctionCall(id='call_gZwgF6BE7O3ksMTFUgiI1eGc', arguments='{\"query\": \"Limitations and Future Directions of ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST\"}', name='web_ai_agent')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='The core research problem addressed by \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" is the cost and limitations associated with human evaluations of large language models (LLMs). The paper proposes using a lightweight LLM as a judge through a grading checklist, which addresses performance disparities and improves judgment accuracy. This method aims to provide a straightforward, replicable, and cost-effective way to automate LLM evaluations, achieving high correlation with human preferences while significantly reducing expenses【0†source】【1†source】.', name='web_ai_agent', call_id='call_tlomk9A0GsqWpUR2MLugyY39', is_error=False), FunctionExecutionResult(content='The RocketEval approach involves three primary steps:\\n\\n1. **Checklist Creation**: Building a comprehensive, instance-specific checklist for the scenarios and questions under consideration.\\n2. **Checklist Grading**: Lightweight LLMs, such as Gemma-2-2B, grade the checklists.\\n3. **Reweighting**: Reweighting the checklist items to align with supervised annotations, ensuring the evaluation reflects human preferences accurately【0†source】【1†source】.', name='web_ai_agent', call_id='call_R29Sdqjd21lRzxDtiBtSKQrj', is_error=False), FunctionExecutionResult(content='RocketEval leverages a lightweight LLM as a judge to achieve efficient automated evaluations. By framing evaluations as multi-faceted Q&A tasks with a checklist, it manages to reduce costs and maintain high correlation with human preferences. Specifically, RocketEval, using Gemma-2-2B as the judge, shows a correlation of 0.965 with human preferences, comparable to GPT-4, and offers over 50-fold cost savings for large-scale scenarios【5†source】.', name='web_ai_agent', call_id='call_wl20TARio3Z6BszejZUgLyfB', is_error=False), FunctionExecutionResult(content='The conclusions and implications of the study \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" reveal that RocketEval, using lightweight LLMs for grading through an instance-specific checklist, achieves a high correlation with human preferences (0.965) comparable to more powerful models like GPT-4, while reducing evaluation costs by over 50-fold. This approach addresses challenges such as high expenses, privacy, security, and reproducibility issues associated with human or powerful LLM-based evaluations【0†source】【1†source】.', name='web_ai_agent', call_id='call_B3X1dwhE4TajPLEVBrRvdtK8', is_error=False), FunctionExecutionResult(content='RocketEval addresses the challenge of evaluating large language models (LLMs) with reduced costs by using lightweight LLMs and a checklist grading approach. However, it faces limitations such as reduced judgment accuracy due to high uncertainty and positional bias of lightweight LLMs. Future directions include improving adaptability to various scenarios, further reducing biases, and enhancing reproducibility in different contexts【9†source】【3†source】.', name='web_ai_agent', call_id='call_gZwgF6BE7O3ksMTFUgiI1eGc', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant', models_usage=None, metadata={}, content='The core research problem addressed by \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" is the cost and limitations associated with human evaluations of large language models (LLMs). The paper proposes using a lightweight LLM as a judge through a grading checklist, which addresses performance disparities and improves judgment accuracy. This method aims to provide a straightforward, replicable, and cost-effective way to automate LLM evaluations, achieving high correlation with human preferences while significantly reducing expenses【0†source】【1†source】.\\nThe RocketEval approach involves three primary steps:\\n\\n1. **Checklist Creation**: Building a comprehensive, instance-specific checklist for the scenarios and questions under consideration.\\n2. **Checklist Grading**: Lightweight LLMs, such as Gemma-2-2B, grade the checklists.\\n3. **Reweighting**: Reweighting the checklist items to align with supervised annotations, ensuring the evaluation reflects human preferences accurately【0†source】【1†source】.\\nRocketEval leverages a lightweight LLM as a judge to achieve efficient automated evaluations. By framing evaluations as multi-faceted Q&A tasks with a checklist, it manages to reduce costs and maintain high correlation with human preferences. Specifically, RocketEval, using Gemma-2-2B as the judge, shows a correlation of 0.965 with human preferences, comparable to GPT-4, and offers over 50-fold cost savings for large-scale scenarios【5†source】.\\nThe conclusions and implications of the study \"RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\" reveal that RocketEval, using lightweight LLMs for grading through an instance-specific checklist, achieves a high correlation with human preferences (0.965) comparable to more powerful models like GPT-4, while reducing evaluation costs by over 50-fold. This approach addresses challenges such as high expenses, privacy, security, and reproducibility issues associated with human or powerful LLM-based evaluations【0†source】【1†source】.\\nRocketEval addresses the challenge of evaluating large language models (LLMs) with reduced costs by using lightweight LLMs and a checklist grading approach. However, it faces limitations such as reduced judgment accuracy due to high uncertainty and positional bias of lightweight LLMs. Future directions include improving adaptability to various scenarios, further reducing biases, and enhancing reproducibility in different contexts【9†source】【3†source】.', type='ToolCallSummaryMessage'), TextMessage(source='write_agent', models_usage=RequestUsage(prompt_tokens=29188, completion_tokens=540), metadata={}, content='🚀 **Revolutionizing LLM Evaluation: The Rise of RocketEval** 🚀\\n\\nIn an era where large language models (LLMs) are rapidly evolving, the efficacy of their evaluation has become an urgent necessity. Traditional methods often involve costly human evaluations that can compromise privacy and reproducibility. **Enter \"RocketEval.\"**\\n\\n**Context:** This groundbreaking research presented at ICLR 2025 explores how lightweight LLMs can serve as highly-efficient evaluators through a structured checklist approach. By transitioning the evaluation process from subjective human judgments to a systematic, automated framework, researchers have opened new avenues for cost-effective, scalable assessments that align closely with human preferences.\\n\\n**Insights:** \\n- **Key Findings**: The RocketEval framework demonstrates a remarkable correlation of **0.965** with human evaluations using a lightweight model (Gemma-2-2B), achieving results comparable to powerful models like GPT-4, but at **over a 50-fold** cost reduction! \\n- **Innovative Approach**: By reframing evaluations into multi-faceted checklists, the study addresses common pitfalls in LLM assessments: high uncertainty and positional bias. This systematic approach ensures that critical aspects of a model\\'s response are thoroughly evaluated, leading to more reliable outputs.\\n\\n**Engagement Trigger:** As the landscape of AI continues to shift, how can we ensure that our evaluation methods keep pace with the technology? Do you believe automated systems can fully replace human evaluators, or is there still value in the human touch in this process?\\n\\n💬 *Let\\'s discuss the future of LLM evaluations!*\\n\\n#LLM #Automation #AIResearch #DataScience #MachineLearning \\n\\n---\\n\\n**Keywords Explained**:\\n\\n1. **Large Language Models (LLMs)**: These are complex AI models designed to understand and generate human language. They are foundational in various applications, from chatbots to content generation.\\n   \\n2. **Automated Evaluation**: This refers to the use of technology, especially AI, to assess the performance of models. It aims to provide faster, scalable, and cost-effective assessments compared to traditional methods.\\n\\n3. **Checklist Grading**: A systematic approach where specific criteria are outlined (checklists) to guide the evaluation process, ensuring comprehensive and consistent assessments.\\n\\n4. **High Correlation with Human Preferences**: A metric indicating how closely the automated evaluation results match human evaluations, highlighting the effectiveness of the evaluation method.\\n\\n5. **Positional Bias**: A cognitive bias that affects decision-making based on the order of presented information. Addressing positional bias is crucial in evaluations to enhance fairness and accuracy. \\n\\nFeel free to ask if you have more specific questions or need additional information!', type='TextMessage'), ToolCallRequestEvent(source='save_post_content_agent', models_usage=RequestUsage(prompt_tokens=29762, completion_tokens=589), metadata={}, content=[FunctionCall(id='call_Ec2ws0LMS1vKHPDEwSO7YS5G', arguments='{\"blog_content\":\"🚀 **Revolutionizing LLM Evaluation: The Rise of RocketEval** 🚀\\\\n\\\\nIn an era where large language models (LLMs) are rapidly evolving, the efficacy of their evaluation has become an urgent necessity. Traditional methods often involve costly human evaluations that can compromise privacy and reproducibility. **Enter \\\\\"RocketEval.\\\\\"\\\\n**\\\\n**Context:** This groundbreaking research presented at ICLR 2025 explores how lightweight LLMs can serve as highly-efficient evaluators through a structured checklist approach. By transitioning the evaluation process from subjective human judgments to a systematic, automated framework, researchers have opened new avenues for cost-effective, scalable assessments that align closely with human preferences.\\\\n\\\\n**Insights:** \\\\n- **Key Findings**: The RocketEval framework demonstrates a remarkable correlation of **0.965** with human evaluations using a lightweight model (Gemma-2-2B), achieving results comparable to powerful models like GPT-4, but at **over a 50-fold** cost reduction! \\\\n- **Innovative Approach**: By reframing evaluations into multi-faceted checklists, the study addresses common pitfalls in LLM assessments: high uncertainty and positional bias. This systematic approach ensures that critical aspects of a model\\'s response are thoroughly evaluated, leading to more reliable outputs.\\\\n\\\\n**Engagement Trigger:** As the landscape of AI continues to shift, how can we ensure that our evaluation methods keep pace with the technology? Do you believe automated systems can fully replace human evaluators, or is there still value in the human touch in this process?\\\\n\\\\n💬 *Let\\'s discuss the future of LLM evaluations!*\\\\n\\\\n#LLM #Automation #AIResearch #DataScience #MachineLearning \\\\n\\\\n---\\\\n\\\\n**Keywords Explained**:\\\\n\\\\n1. **Large Language Models (LLMs)**: These are complex AI models designed to understand and generate human language. They are foundational in various applications, from chatbots to content generation.\\\\n   \\\\n2. **Automated Evaluation**: This refers to the use of technology, especially AI, to assess the performance of models. It aims to provide faster, scalable, and cost-effective assessments compared to traditional methods.\\\\n\\\\n3. **Checklist Grading**: A systematic approach where specific criteria are outlined (checklists) to guide the evaluation process, ensuring comprehensive and consistent assessments.\\\\n\\\\n4. **High Correlation with Human Preferences**: A metric indicating how closely the automated evaluation results match human evaluations, highlighting the effectiveness of the evaluation method.\\\\n\\\\n5. **Positional Bias**: A cognitive bias that affects decision-making based on the order of presented information. Addressing positional bias is crucial in evaluations to enhance fairness and accuracy. \\\\n\\\\nFeel free to ask if you have more specific questions or need additional information!\"}', name='save_blog_agent')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='save_post_content_agent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='Saved', name='save_blog_agent', call_id='call_Ec2ws0LMS1vKHPDEwSO7YS5G', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='save_post_content_agent', models_usage=None, metadata={}, content='Saved', type='ToolCallSummaryMessage')], stop_reason=\"Text 'Saved' mentioned\")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Console(\n",
    "    reflection_team.run_stream(task=f\"\"\"\n",
    "                    I'm writing a linked in post about a research paper. The content of the research paper is {content}.\n",
    "                    \n",
    "                    Extract the following information from the research paper:\n",
    "                    \n",
    "                    - Problem/Research Question: What is the research trying to address or investigate? \n",
    "                    - Methods/Approach: How did the researchers conduct their study? \n",
    "                    - Results/Findings: What did the research uncover? \n",
    "                    - Conclusions/Implications: What are the main takeaways and significance of the research? \n",
    "                    - Limitations and Future Directions: What\n",
    "                    \n",
    "                    Generate a high-engagement Linked In post about the challenges and solutions in the research paper.\n",
    "                    \n",
    "                    Extract keywords and use bing search to explain the keywords in the research paper.                    \n",
    "                    The tone should be authoritative yet engaging. \n",
    "                    Follow the Hook → Context → Insights → Engagement Trigger structure. End with a question to spark discussion. Add 3-5 relevant hashtags.\n",
    "    \"\"\")\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
