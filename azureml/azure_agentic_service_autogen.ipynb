{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent orchestrator using Azure AI Agentic Service\n",
    "\n",
    "In this example we will summarize a research paper into LinkedIn post using, Azure Agentic Service, Bing & LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-otlp-proto-grpc 1.26.0 requires opentelemetry-sdk~=1.26.0, but you have opentelemetry-sdk 1.31.0 which is incompatible.\n",
      "opentelemetry-proto 1.26.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.29.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## Project PIP requirements\n",
    "# %pip install -q azure-ai-projects azure-identity azure-ai-ml azure-search-documents tika \"autogen-agentchat\" \"autogen-ext[openai]\"\n",
    "# %pip install -q -U \"autogen-agentchat\" \"autogen-ext[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import CodeInterpreterTool\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Research paper path\n",
    "research_paper_path =  \"https://arxiv.org/pdf/2503.05142\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(), conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=15, completion_tokens=7) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    azure_endpoint=os.environ[\"AOAI_ENDPOINT\"], # Azure OpenAI endpoint.\n",
    "    api_key=os.environ[\"AOAI_KEY\"], # For key-based authentication.\n",
    ")\n",
    "\n",
    "# Test the Azure OpenAI model client\n",
    "result = await az_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agents\n",
    "\n",
    "**Pre-requisites**\n",
    "1. Create Azure AI Search and configure as connector to the agent. [Link](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/azure-ai-search?tabs=pythonsdk%2Cpython&pivots=code-examples#setup-create-an-agent-that-can-use-an-existing-azure-ai-search-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and ingest the file to vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research paper already downloaded.\n",
      "research-paper-index created\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tika import parser \n",
    "# create azure search index\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex\n",
    ")\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "file_path = \"./data/research_paper.pdf\"\n",
    "\n",
    "# Download research paper\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading research paper...\")\n",
    "    response = requests.get(research_paper_path)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "       f.write(response.content)\n",
    "else:\n",
    "    print(\"Research paper already downloaded.\")\n",
    "    \n",
    "index_name = \"research-paper-index\"\n",
    "index_client = SearchIndexClient(os.environ[\"SEARCH_ENDPOINT\"],AzureKeyCredential(os.environ[\"SEARCH_KEY\"]))\n",
    "fields = [\n",
    "    SearchField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchField(name=\"content\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
    "]\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(name=\"myHnsw\"),\n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "        )\n",
    "    ]\n",
    ")  \n",
    "  \n",
    "# Create the search index\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\")\n",
    "raw = parser.from_file(file_path)\n",
    "content = raw['content']\n",
    "\n",
    "search_client = SearchClient(os.environ[\"SEARCH_ENDPOINT\"], index_name, AzureKeyCredential(os.environ[\"SEARCH_KEY\"]))\n",
    "# Upload the research paper content to the search index\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"content\": content\n",
    "    }\n",
    "]\n",
    "result = search_client.upload_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure AI Search Agent for Custom Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection ID: /subscriptions/6a01260f-39d6-415f-a6c9-cf4fd479cbec/resourceGroups/rg-vism-7896_ai/providers/Microsoft.MachineLearningServices/workspaces/agentic-service/connections/vectorsearchagentic\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects.models import AzureAISearchTool\n",
    "\n",
    "# Upload research paper to Azure AI Search\n",
    "conn_list = project_client.connections._list_connections()[\"value\"]\n",
    "conn_id = \"\"\n",
    "\n",
    "# Search in the metadata field of each connection in the list for the azure_ai_search type and get the id value to establish the variable\n",
    "for conn in conn_list:\n",
    "    metadata = conn[\"properties\"].get(\"metadata\", {})\n",
    "    if metadata.get(\"type\", \"\").upper() == \"AZURE_AI_SEARCH\":\n",
    "        conn_id = conn[\"id\"]\n",
    "        break\n",
    "\n",
    "print(f\"Connection ID: {conn_id}\")\n",
    "\n",
    "async def ai_search_tool(query: str):\n",
    "\n",
    "    ai_search = AzureAISearchTool(index_connection_id=conn_id, index_name=index_name)\n",
    "    with project_client:\n",
    "        print(query)\n",
    "        agent = project_client.agents.create_agent(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            name=\"research-doc-assistant\",\n",
    "            instructions=\"\"\"\n",
    "            You are a research doc search assistant.\n",
    "            You can search for information in the research paper provided.\n",
    "            You can also answer questions about the research paper.\n",
    "            You can also summarize the research paper.\n",
    "            \"\"\",\n",
    "            tools=ai_search.definitions,\n",
    "            headers={\"x-ms-enable-preview\": \"true\"},\n",
    "            tool_resources = ai_search.resources,\n",
    "        )\n",
    "        \n",
    "        thread = project_client.agents.create_thread()\n",
    "        \n",
    "        print(f\"Created thread, ID: {thread.id}\")\n",
    "        \n",
    "        # Create message to thread\n",
    "        message = project_client.agents.create_message(\n",
    "             thread_id=thread.id,\n",
    "             role=\"user\",\n",
    "             content=query,\n",
    "        )\n",
    "        \n",
    "        print(f\"SMS: {message}\")\n",
    "        # Create and process agent run in thread with tools\n",
    "        run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "        print(f\"Run finished with status: {run.status}\")\n",
    "        if run.status == \"failed\":\n",
    "            print(f\"Run failed: {run.last_error}\")\n",
    "        # Delete the assistant when done\n",
    "        project_client.agents.delete_agent(agent.id)\n",
    "        print(\"Deleted agent\")\n",
    "        # Fetch and log all messages\n",
    "        messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "        print(\"Messages:\"+ messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"])\n",
    "    return messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Search Agent created\n"
     ]
    }
   ],
   "source": [
    "ai_search_agent = AssistantAgent(\n",
    "    name=\"ai_search_agent\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[ai_search_tool],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    ")\n",
    "\n",
    "print(\"AI Search Agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bing Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import BingGroundingTool\n",
    "\n",
    "async def web_ai_agent(query: str) -> str:\n",
    "    print(\"This is Bing for Azure AI Agent Service .......\")\n",
    "    bing = BingGroundingTool(connection_id=conn_id)\n",
    "    with project_client:\n",
    "        agent = project_client.agents.create_agent(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            name=\"bing-search-assistant\",\n",
    "            instructions=\"\"\"        \n",
    "                You are a web search agent.\n",
    "                Your only tool is search_tool - use it to find information.\n",
    "                You make only one search call at a time.\n",
    "                Once you have the results, you never do calculations based on them.\n",
    "            \"\"\",\n",
    "            tools=bing.definitions,\n",
    "            headers={\"x-ms-enable-preview\": \"true\"}\n",
    "        )\n",
    "        \n",
    "        print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "        # Create thread for communication\n",
    "        thread = project_client.agents.create_thread()\n",
    "        print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "        # Create message to thread\n",
    "        message = project_client.agents.create_message(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=query,\n",
    "        )\n",
    "        print(f\"SMS: {message}\")\n",
    "        # Create and process agent run in thread with tools\n",
    "        run = project_client.agents.create_and_process_run(thread_id=thread.id, assistant_id=agent.id)\n",
    "        print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "        if run.status == \"failed\":\n",
    "            print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "        # Delete the assistant when done\n",
    "        project_client.agents.delete_agent(agent.id)\n",
    "        print(\"Deleted agent\")\n",
    "\n",
    "        # Fetch and log all messages\n",
    "        messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "        print(\"Messages:\"+ messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"])\n",
    "    return messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bing Search Agent created\n"
     ]
    }
   ],
   "source": [
    "bing_search_agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[web_ai_agent],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    ")\n",
    "\n",
    "print(\"Bing Search Agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save_blog_agent(blog_content: str) -> str:\n",
    "\n",
    "    print(\"This is Code Interpreter for Azure AI Agent Service .......\")\n",
    "\n",
    "\n",
    "    code_interpreter = CodeInterpreterTool()\n",
    "        \n",
    "    agent = project_client.agents.create_agent(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            name=\"my-agent\",\n",
    "            instructions=\"You are helpful agent\",\n",
    "            tools=code_interpreter.definitions,\n",
    "            # tool_resources=code_interpreter.resources,\n",
    "    )\n",
    "\n",
    "    thread = project_client.agents.create_thread()\n",
    "\n",
    "    message = project_client.agents.create_message(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=\"\"\"\n",
    "        \n",
    "                    You are my Python programming assistant. Generate code,save \"\"\"+ blog_content +\n",
    "                    \n",
    "                \"\"\"    \n",
    "                    and execute it according to the following requirements\n",
    "\n",
    "                    1. Save blog content to blog-{YYMMDDHHMMSS}.md\n",
    "\n",
    "                    2. give me the download this file link\n",
    "                \"\"\",\n",
    "    )\n",
    "    # create and execute a run\n",
    "    run = project_client.agents.create_and_process_run(thread_id=thread.id, assistant_id=agent.id)\n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    if run.status == \"failed\":\n",
    "            # Check if you got \"Rate limit is exceeded.\", then you want to get more quota\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "        # # delete the original file from the agent to free up space (note: this does not delete your version of the file)\n",
    "        # project_client.agents.delete_file(file.id)\n",
    "        # print(\"Deleted file\")\n",
    "\n",
    "        # print the messages from the agent\n",
    "    messages = project_client.agents.get_messages(thread_id=thread.id)\n",
    "    print(f\"Messages: {messages}\")\n",
    "\n",
    "        # get the most recent message from the assistant\n",
    "    last_msg = messages.get_last_text_message_by_sender(\"assistant\")\n",
    "    if last_msg:\n",
    "        print(f\"Last Message: {last_msg.text.value}\")\n",
    "\n",
    "        # print(f\"File: {messages.file_path_annotations}\")\n",
    "\n",
    "\n",
    "    for file_path_annotation in messages.file_path_annotations:\n",
    "\n",
    "        file_name = os.path.basename(file_path_annotation.text)\n",
    "\n",
    "        project_client.agents.save_file(file_id=file_path_annotation.file_path.file_id, file_name=file_name,target_dir=\"./blog\")\n",
    "        \n",
    "\n",
    "    project_client.agents.delete_agent(agent.id)\n",
    "    print(\"Deleted agent\")\n",
    "\n",
    "    # project_client.close()\n",
    "\n",
    "    return \"Saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_blog_content_agent = AssistantAgent(\n",
    "    name=\"save_post_content_agent\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[save_blog_agent],\n",
    "    system_message=\"\"\"\n",
    "        Save post content. Respond with 'Saved' to when your post are saved.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_agent = AssistantAgent(\n",
    "    name=\"write_agent\",\n",
    "    model_client=az_model_client,\n",
    "    system_message=\"\"\"\n",
    "        You are a linked in post writer, please help me write a linked post based on research paper and bing search content.\"\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_termination = TextMentionTermination(\"Saved\")\n",
    "# Define a termination condition that stops the task after 5 messages.\n",
    "max_message_termination = MaxMessageTermination(10)\n",
    "# Combine the termination conditions using the `|`` operator so that the\n",
    "# task stops when either condition is met.\n",
    "termination = text_termination | max_message_termination\n",
    "reflection_team = RoundRobinGroupChat([ai_search_agent], termination_condition=termination)\n",
    "# reflection_team = RoundRobinGroupChat([ai_search_agent, bing_search_agent, write_agent,save_blog_content_agent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "\n",
      "                    I'm writing a linked in post about a research paper on llm evaluations.\n",
      "                    The research paper is stored in the Azure AI Search index.\n",
      "                    Query the research paper using the AI Search agent. \n",
      "                    Generate a high-engagement Linked In post about the challenges and solutions in the research paper.\n",
      "                    Identify Key Components:\n",
      "                    - Problem/Research Question: What is the research trying to address or investigate? \n",
      "                    - Methods/Approach: How did the researchers conduct their study? \n",
      "                    - Results/Findings: What did the research uncover? \n",
      "                    - Conclusions/Implications: What are the main takeaways and significance of the research? \n",
      "                    - Limitations and Future Directions: What\n",
      "                    Extract keywords and use bing search to explain the keywords in the research paper.                    \n",
      "                    The tone should be authoritative yet engaging. \n",
      "                    Follow the Hook → Context → Insights → Engagement Trigger structure. End with a question to spark discussion. Add 3-5 relevant hashtags.\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for ai_search_agent_b15dbb5d-a9e3-4b14-bfab-f1bf174ce877/b15dbb5d-a9e3-4b14-bfab-f1bf174ce877\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 394, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 99, in handle_async_request\n",
      "    raise exc\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 76, in handle_async_request\n",
      "    stream = await self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 122, in _connect\n",
      "    stream = await self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpcore\\_backends\\auto.py\", line 30, in connect_tcp\n",
      "    return await self._backend.connect_tcp(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 114, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectTimeout\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1500, in _request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1629, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1657, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1694, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1730, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 393, in handle_async_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectTimeout\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 510, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 113, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 69, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 748, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 870, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 523, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2000, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1767, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1461, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1509, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1509, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in _request\n",
      "    raise APITimeoutError(request=request) from err\n",
      "openai.APITimeoutError: Request timed out.\n"
     ]
    },
    {
     "ename": "APITimeoutError",
     "evalue": "Request timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPITimeoutError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m Console(\n\u001b[0;32m      2\u001b[0m     reflection_team\u001b[38;5;241m.\u001b[39mrun_stream(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m                    I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm writing a linked in post about a research paper on llm evaluations.\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m                    The research paper is stored in the Azure AI Search index.\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m                    Query the research paper using the AI Search agent. \u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m                    Generate a high-engagement Linked In post about the challenges and solutions in the research paper.\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m                    Identify Key Components:\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m                    - Problem/Research Question: What is the research trying to address or investigate? \u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m                    - Methods/Approach: How did the researchers conduct their study? \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m                    - Results/Findings: What did the research uncover? \u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m                    - Conclusions/Implications: What are the main takeaways and significance of the research? \u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m                    - Limitations and Future Directions: What\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m                    Extract keywords and use bing search to explain the keywords in the research paper.                    \u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m                    The tone should be authoritative yet engaging. \u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m                    Follow the Hook → Context → Insights → Engagement Trigger structure. End with a question to spark discussion. Add 3-5 relevant hashtags.\u001b[39m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m )  \n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\ui\\_console.py:117\u001b[0m, in \u001b[0;36mConsole\u001b[1;34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m last_processed: Optional[T] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    115\u001b[0m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[0;32m    119\u001b[0m         duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:482\u001b[0m, in \u001b[0;36mBaseGroupChat.run_stream\u001b[1;34m(self, task, cancellation_token)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shutdown_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;66;03m# Wait for the shutdown task to finish.\u001b[39;00m\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;66;03m# This will propagate any exceptions raised.\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m shutdown_task\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;66;03m# Clear the output message queue.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_message_queue\u001b[38;5;241m.\u001b[39mempty():\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:426\u001b[0m, in \u001b[0;36mBaseGroupChat.run_stream.<locals>.stop_runtime\u001b[1;34m()\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runtime, SingleThreadedAgentRuntime)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# This will propagate any exceptions raised.\u001b[39;00m\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runtime\u001b[38;5;241m.\u001b[39mstop_when_idle()\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# Stop the consumption of messages and end the stream.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# NOTE: we also need to put a GroupChatTermination event here because when the group chat\u001b[39;00m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;66;03m# has an exception, the group chat manager may not be able to put a GroupChatTermination event in the queue.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_message_queue\u001b[38;5;241m.\u001b[39mput(\n\u001b[0;32m    432\u001b[0m         GroupChatTermination(\n\u001b[0;32m    433\u001b[0m             message\u001b[38;5;241m=\u001b[39mStopMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException occurred.\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group_chat_manager_name)\n\u001b[0;32m    434\u001b[0m         )\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py:746\u001b[0m, in \u001b[0;36mSingleThreadedAgentRuntime.stop_when_idle\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntime is not started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_context\u001b[38;5;241m.\u001b[39mstop_when_idle()\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py:120\u001b[0m, in \u001b[0;36mRunContext.stop_when_idle\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopped\u001b[38;5;241m.\u001b[39mset()\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runtime\u001b[38;5;241m.\u001b[39m_message_queue\u001b[38;5;241m.\u001b[39mshutdown(immediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_task\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py:109\u001b[0m, in \u001b[0;36mRunContext._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopped\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runtime\u001b[38;5;241m.\u001b[39m_process_next()\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py:581\u001b[0m, in \u001b[0;36mSingleThreadedAgentRuntime._process_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    579\u001b[0m         e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_background_exception\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_background_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m message_envelope:\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py:528\u001b[0m, in \u001b[0;36mSingleThreadedAgentRuntime._process_publish\u001b[1;34m(self, message_envelope)\u001b[0m\n\u001b[0;32m    525\u001b[0m         future \u001b[38;5;241m=\u001b[39m _on_message(agent, message_context)\n\u001b[0;32m    526\u001b[0m         responses\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[1;32m--> 528\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mresponses)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_unhandled_handler_exceptions:\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py:523\u001b[0m, in \u001b[0;36mSingleThreadedAgentRuntime._process_publish.<locals>._on_message\u001b[1;34m(agent, message_context)\u001b[0m\n\u001b[0;32m    515\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing publish message for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    516\u001b[0m event_logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    517\u001b[0m     MessageHandlerExceptionEvent(\n\u001b[0;32m    518\u001b[0m         payload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_serialize(message_envelope\u001b[38;5;241m.\u001b[39mmessage),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m     )\n\u001b[0;32m    522\u001b[0m )\n\u001b[1;32m--> 523\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py:510\u001b[0m, in \u001b[0;36mSingleThreadedAgentRuntime._process_publish.<locals>._on_message\u001b[1;34m(agent, message_context)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m MessageHandlerContext\u001b[38;5;241m.\u001b[39mpopulate_context(agent\u001b[38;5;241m.\u001b[39mid):\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 510\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mon_message(\n\u001b[0;32m    511\u001b[0m             message_envelope\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[0;32m    512\u001b[0m             ctx\u001b[38;5;241m=\u001b[39mmessage_context,\n\u001b[0;32m    513\u001b[0m         )\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    515\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing publish message for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py:113\u001b[0m, in \u001b[0;36mBaseAgent.on_message\u001b[1;34m(self, message, ctx)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: Any, ctx: MessageContext) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_message_impl(message, ctx)\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py:67\u001b[0m, in \u001b[0;36mSequentialRoutedAgent.on_message_impl\u001b[1;34m(self, message, ctx)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fifo_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mon_message_impl(message, ctx)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Release the FIFO lock to allow the next message to be processed.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fifo_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py:485\u001b[0m, in \u001b[0;36mRoutedAgent.on_message_impl\u001b[1;34m(self, message, ctx)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m h\u001b[38;5;241m.\u001b[39mrouter(message, ctx):\n\u001b[1;32m--> 485\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m h(\u001b[38;5;28mself\u001b[39m, message, ctx)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_unhandled_message(message, ctx)\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py:268\u001b[0m, in \u001b[0;36mevent.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, message, ctx)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in target types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 268\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, message, ctx)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py:69\u001b[0m, in \u001b[0;36mChatAgentContainer.handle_request\u001b[1;34m(self, message, ctx)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Pass the messages in the buffer to the delegate agent.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m response: Response \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent\u001b[38;5;241m.\u001b[39mon_messages_stream(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_buffer, ctx\u001b[38;5;241m.\u001b[39mcancellation_token):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(msg, Response):\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;66;03m# Log the response.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpublish_message(\n\u001b[0;32m     73\u001b[0m             GroupChatMessage(message\u001b[38;5;241m=\u001b[39mmsg\u001b[38;5;241m.\u001b[39mchat_message),\n\u001b[0;32m     74\u001b[0m             topic_id\u001b[38;5;241m=\u001b[39mDefaultTopicId(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_topic_type),\n\u001b[0;32m     75\u001b[0m         )\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:748\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages_stream\u001b[1;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;66;03m# STEP 3: Run the first inference\u001b[39;00m\n\u001b[0;32m    747\u001b[0m model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m inference_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_llm(\n\u001b[0;32m    749\u001b[0m     model_client\u001b[38;5;241m=\u001b[39mmodel_client,\n\u001b[0;32m    750\u001b[0m     model_client_stream\u001b[38;5;241m=\u001b[39mmodel_client_stream,\n\u001b[0;32m    751\u001b[0m     system_messages\u001b[38;5;241m=\u001b[39msystem_messages,\n\u001b[0;32m    752\u001b[0m     model_context\u001b[38;5;241m=\u001b[39mmodel_context,\n\u001b[0;32m    753\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    754\u001b[0m     handoff_tools\u001b[38;5;241m=\u001b[39mhandoff_tools,\n\u001b[0;32m    755\u001b[0m     agent_name\u001b[38;5;241m=\u001b[39magent_name,\n\u001b[0;32m    756\u001b[0m     cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token,\n\u001b[0;32m    757\u001b[0m ):\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inference_output, CreateResult):\n\u001b[0;32m    759\u001b[0m         model_result \u001b[38;5;241m=\u001b[39m inference_output\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:870\u001b[0m, in \u001b[0;36mAssistantAgent._call_llm\u001b[1;34m(cls, model_client, model_client_stream, system_messages, model_context, tools, handoff_tools, agent_name, cancellation_token)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 870\u001b[0m     model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model_client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    871\u001b[0m         llm_messages, tools\u001b[38;5;241m=\u001b[39mall_tools, cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token\n\u001b[0;32m    872\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:523\u001b[0m, in \u001b[0;36mBaseOpenAIChatCompletionClient.create\u001b[1;34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    522\u001b[0m     cancellation_token\u001b[38;5;241m.\u001b[39mlink_future(future)\n\u001b[1;32m--> 523\u001b[0m result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_beta_client:\n\u001b[0;32m    525\u001b[0m     result \u001b[38;5;241m=\u001b[39m cast(ParsedChatCompletion[Any], result)\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2000\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1959\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1997\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m   1999\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 2000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   2001\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2002\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   2003\u001b[0m             {\n\u001b[0;32m   2004\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   2005\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   2006\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   2007\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   2008\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   2009\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   2010\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   2011\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   2012\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   2013\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   2014\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   2015\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   2016\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   2017\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   2018\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   2019\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   2020\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m   2021\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   2022\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   2023\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   2024\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   2025\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   2026\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   2027\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   2028\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   2029\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   2030\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   2031\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   2032\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   2033\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   2034\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m   2035\u001b[0m             },\n\u001b[0;32m   2036\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m   2037\u001b[0m         ),\n\u001b[0;32m   2038\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   2039\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   2040\u001b[0m         ),\n\u001b[0;32m   2041\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   2042\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2043\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   2044\u001b[0m     )\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1767\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1755\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1762\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1763\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m   1764\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1765\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1766\u001b[0m     )\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1461\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1459\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1462\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1463\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1464\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1465\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1466\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1467\u001b[0m )\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1509\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1506\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1510\u001b[0m         input_options,\n\u001b[0;32m   1511\u001b[0m         cast_to,\n\u001b[0;32m   1512\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1513\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1514\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1515\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1516\u001b[0m     )\n\u001b[0;32m   1518\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1509\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1506\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1510\u001b[0m         input_options,\n\u001b[0;32m   1511\u001b[0m         cast_to,\n\u001b[0;32m   1512\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1513\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1514\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1515\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1516\u001b[0m     )\n\u001b[0;32m   1518\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32mc:\\code\\sriksml\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1519\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1509\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1510\u001b[0m             input_options,\n\u001b[0;32m   1511\u001b[0m             cast_to,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1515\u001b[0m             response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1516\u001b[0m         )\n\u001b[0;32m   1518\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1520\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1521\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAPITimeoutError\u001b[0m: Request timed out."
     ]
    }
   ],
   "source": [
    "await Console(\n",
    "    reflection_team.run_stream(task=\"\"\"\n",
    "                    I'm writing a linked in post about a research paper on llm evaluations.\n",
    "                    The research paper is stored in the Azure AI Search index.\n",
    "                    Query the research paper using the AI Search agent. \n",
    "                    Generate a high-engagement Linked In post about the challenges and solutions in the research paper.\n",
    "                    Identify Key Components:\n",
    "                    - Problem/Research Question: What is the research trying to address or investigate? \n",
    "                    - Methods/Approach: How did the researchers conduct their study? \n",
    "                    - Results/Findings: What did the research uncover? \n",
    "                    - Conclusions/Implications: What are the main takeaways and significance of the research? \n",
    "                    - Limitations and Future Directions: What\n",
    "                    Extract keywords and use bing search to explain the keywords in the research paper.                    \n",
    "                    The tone should be authoritative yet engaging. \n",
    "                    Follow the Hook → Context → Insights → Engagement Trigger structure. End with a question to spark discussion. Add 3-5 relevant hashtags.\n",
    "\n",
    "    \"\"\")\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
