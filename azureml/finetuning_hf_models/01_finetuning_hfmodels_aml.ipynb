{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Huggingface models on Azure ML\n",
    "\n",
    "This notebook explains how to create an end-to-end lineage for hugging face models on Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, AzureCliCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.entities import (\n",
    "    VsCodeJobService,\n",
    "    TensorBoardJobService,\n",
    "    JupyterLabJobService,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create MLClient\n",
    "def create_ml_client(credential=None):     \n",
    "    if credential is None:\n",
    "        try:\n",
    "            credential = DefaultAzureCredential()\n",
    "            # Check if given credential can get token successfully.\n",
    "            credential.get_token(\"https://management.azure.com/.default\")\n",
    "        except Exception as ex:\n",
    "            # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "            credential = AzureCliCredential()\n",
    "\n",
    "    return MLClient.from_config(credential=credential)\n",
    "\n",
    "# Function to create AML Compute Cluster\n",
    "def create_aml_cluster(ml_client, compute_cluster_name = \"AmlComputeCluster\", vm_size = \"Standard_NC4as_T4_v3\", min_nodes = 1, max_nodes = 2):\n",
    "    # If you already have a gpu cluster, mention it here. Else will create a new one    \n",
    "    try:\n",
    "        compute = ml_client.compute.get(compute_cluster_name)\n",
    "        print(\"successfully fetched compute:\", compute.name)\n",
    "    except Exception as ex:\n",
    "        print(\"failed to fetch compute:\", compute_cluster_name)\n",
    "        print(f\"creating new {vm_size} compute\")\n",
    "        compute = AmlCompute(\n",
    "            name=compute_cluster_name,\n",
    "            size=vm_size,\n",
    "            min_instances=min_nodes,\n",
    "            max_instances=max_nodes,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "        print(\"successfully created compute:\", compute.name)\n",
    "    return compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Environment\n",
    "def create_environment(ml_client, env_Name = \"finetune_hf_lora\"):\n",
    "    try:\n",
    "        env = ml_client.environments.get(env_Name)\n",
    "        print(\"successfully fetched environment:\", env.name)\n",
    "    except Exception as ex:\n",
    "        print(\"failed to fetch environment:\", env_Name)\n",
    "        print(f\"creating new environment {env_Name}\")\n",
    "        env_docker_context = Environment(\n",
    "            build=BuildContext(path=\"env\"),\n",
    "            name=env_Name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "        )\n",
    "        ml_client.environments.create_or_update(env_docker_context)\n",
    "        print(\"successfully created environment:\", env_docker_context.name)\n",
    "    return env_docker_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ./config.json\n"
     ]
    }
   ],
   "source": [
    "# Create Azure ML dataset from local file\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "VERSION = \"1.0\"\n",
    "NAME = \"squad_dev_v2\"\n",
    "def create_dataset(ml_client):\n",
    "    # Create a dataset from local file\n",
    "    dataset = Data(\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        path=\"./data/dev-v2.0.json\",\n",
    "        description=\"SquAD v2.0 dev dataset\",\n",
    "        name=NAME,\n",
    "        version=VERSION,\n",
    "    )\n",
    "    return ml_client.data.create_or_update(dataset)\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "if ml_client.data.get(NAME, VERSION) == None:\n",
    "    create_dataset(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "\n",
    "\n",
    "# Set the input for the job:\n",
    "data_asset = ml_client.data.get(NAME, version=VERSION)\n",
    "inputs = Input(path=data_asset.id, mode=InputOutputModes.MOUNT, type=AssetTypes.URI_FILE, destination_path_on_compute=\"data\")\n",
    "\n",
    "# Function to create Job\n",
    "def create_job(compute_cluster, script_file = \"finetune_hf_models.py\", job_name = \"hf_finetuning\", env_name = \"finetune_hf_lora\"):\n",
    "    job = command(\n",
    "        code=\".\",\n",
    "        command=f\"python src/{script_file} \\\n",
    "            --model_name google/flan-t5-small \\\n",
    "            --num_epochs 1 \\\n",
    "            --data_path ${{inputs.data}} \\\n",
    "            --target_input_length=512 \\\n",
    "            --target_max_length=100 \\\n",
    "            --train_size=1000\",\n",
    "    compute=compute_cluster,\n",
    "    inputs={\n",
    "      \"data\": inputs\n",
    "    },\n",
    "    services={\n",
    "      \"My_jupyterlab\": JupyterLabJobService(\n",
    "        nodes=\"all\" # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\n",
    "      ),\n",
    "      \"My_vscode\": VsCodeJobService(\n",
    "        nodes=\"all\"\n",
    "      ),\n",
    "      \"My_tensorboard\": TensorBoardJobService(\n",
    "        nodes=\"all\",\n",
    "        log_dir=\"outputs/runs\"  # relative path of Tensorboard logs (same as in your training script)         \n",
    "      ),\n",
    "    },\n",
    "    environment=f\"{env_name}@latest\",\n",
    "    instance_count=1,  \n",
    "    display_name=job_name)\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ./config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully fetched compute: AMLComputeCluster\n",
      "failed to fetch environment: finetune_hf_lora\n",
      "creating new environment finetune_hf_lora\n",
      "successfully created environment: finetune_hf_lora\n",
      "Creating job hf_finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading finetuning_hf_models (4.43 MBs): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4433310/4433310 [00:00<00:00, 14167438.63it/s]\n",
      "\n",
      "\n",
      "Use of {} for parameters is deprecated, instead use ${{}}.\n",
      "Readonly attribute status will be ignored in class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.JobService'>\n",
      "Readonly attribute status will be ignored in class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.JobService'>\n",
      "Readonly attribute status will be ignored in class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.JobService'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job created successfully\n",
      "https://ml.azure.com/runs/funny_vulture_c7tm1ngmxw?wsid=/subscriptions/6a01260f-39d6-415f-a6c9-cf4fd479cbec/resourcegroups/sriks-ml-rg/workspaces/sriks-ml-sea&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n"
     ]
    }
   ],
   "source": [
    "compute_cluster_name = \"AmlComputeCluster\"\n",
    "ml_client = create_ml_client()\n",
    "compute_cluster = create_aml_cluster(ml_client, compute_cluster_name = compute_cluster_name)\n",
    "env_docker_context = create_environment(ml_client)\n",
    "job = create_job(compute_cluster_name)\n",
    "print(f\"Creating job {job.display_name}\")\n",
    "# Submit the job and wait for completion\n",
    "job = ml_client.jobs.create_or_update(job).wait()\n",
    "print(\"Job created successfully\")\n",
    "print(job.studio_url)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
