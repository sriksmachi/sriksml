{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, AzureCliCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.entities import (\n",
    "    VsCodeJobService,\n",
    "    TensorBoardJobService,\n",
    "    JupyterLabJobService,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create MLClient\n",
    "def create_ml_client(credential=None):     \n",
    "    if credential is None:\n",
    "        try:\n",
    "            credential = DefaultAzureCredential()\n",
    "            # Check if given credential can get token successfully.\n",
    "            credential.get_token(\"https://management.azure.com/.default\")\n",
    "        except Exception as ex:\n",
    "            # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "            credential = AzureCliCredential()\n",
    "\n",
    "    return MLClient.from_config(credential=credential)\n",
    "\n",
    "# Function to create AML Compute Cluster\n",
    "def create_aml_cluster(ml_client, compute_cluster_name = \"AmlComputeCluster\", vm_size = \"Standard_NC4as_T4_v3\", min_nodes = 1, max_nodes = 2):\n",
    "    # If you already have a gpu cluster, mention it here. Else will create a new one    \n",
    "    try:\n",
    "        compute = ml_client.compute.get(compute_cluster_name)\n",
    "        print(\"successfully fetched compute:\", compute.name)\n",
    "    except Exception as ex:\n",
    "        print(\"failed to fetch compute:\", compute_cluster_name)\n",
    "        print(f\"creating new {vm_size} compute\")\n",
    "        compute = AmlCompute(\n",
    "            name=compute_cluster_name,\n",
    "            size=vm_size,\n",
    "            min_instances=min_nodes,\n",
    "            max_instances=max_nodes,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "        print(\"successfully created compute:\", compute.name)\n",
    "    return compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Environment\n",
    "def create_environment(ml_client, env_Name = \"finetune_hf_lora\"):\n",
    "    try:\n",
    "        env = ml_client.environments.get(env_Name)\n",
    "        print(\"successfully fetched environment:\", env.name)\n",
    "    except Exception as ex:\n",
    "        print(\"failed to fetch environment:\", env_Name)\n",
    "        print(f\"creating new environment {env_Name}\")\n",
    "        env_docker_context = Environment(\n",
    "            build=BuildContext(path=\"env\"),\n",
    "            name=env_Name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "        )\n",
    "        ml_client.environments.create_or_update(env_docker_context)\n",
    "        print(\"successfully created environment:\", env_docker_context.name)\n",
    "    return env_docker_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Job\n",
    "def create_job(compute_cluster, script_file = \"finetune_hf_models.py\", job_name = \"hf_finetuning\", env_name = \"finetune_hf_lora\"):\n",
    "    job = command(\n",
    "        code=\".\",\n",
    "        command=f\"python src/{script_file} \\\n",
    "            --model_name google/flan-t5-small \\\n",
    "            --dataset squad \\\n",
    "            --num_epochs 1 \\\n",
    "            --target_input_length=512 \\\n",
    "            --target_max_length=100 \\\n",
    "            --train_size=1000\",            \n",
    "    compute=compute_cluster,\n",
    "    services={\n",
    "      \"My_jupyterlab\": JupyterLabJobService(\n",
    "        nodes=\"all\" # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\n",
    "      ),\n",
    "      \"My_vscode\": VsCodeJobService(\n",
    "        nodes=\"all\"\n",
    "      ),\n",
    "      \"My_tensorboard\": TensorBoardJobService(\n",
    "        nodes=\"all\",\n",
    "        log_dir=\"outputs/runs\"  # relative path of Tensorboard logs (same as in your training script)         \n",
    "      ),\n",
    "    },\n",
    "    environment=f\"{env_name}@latest\",\n",
    "    instance_count=1,  \n",
    "    display_name=job_name)\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ./config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully fetched compute: AMLComputeCluster\n",
      "failed to fetch environment: finetune_hf_lora\n",
      "creating new environment finetune_hf_lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading env (0.0 MBs): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1230/1230 [00:00<00:00, 15150.96it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created environment: finetune_hf_lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating job : type: command\n",
      "environment: azureml:finetune_hf_lora@latest\n",
      "resources:\n",
      "  instance_count: 1\n",
      "component:\n",
      "  name: azureml_anonymous\n",
      "  version: '1'\n",
      "  display_name: hf_finetuning\n",
      "  type: command\n",
      "  command: python src/finetune_hf_models.py             --model_name google/flan-t5-small             --dataset\n",
      "    squad             --num_epochs 1             --target_input_length=512             --target_max_length=100             --train_size=1000\n",
      "  environment: azureml:finetune_hf_lora@latest\n",
      "  code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/vism-cpu-4c/code/Users/vism/sriksml/azureml/finetuning_hf_models\n",
      "  resources:\n",
      "    instance_count: 1\n",
      "  is_deterministic: true\n",
      "compute: azureml:AmlComputeCluster\n",
      "services:\n",
      "  My_jupyterlab:\n",
      "    nodes: all\n",
      "    type: jupyter_lab\n",
      "  My_vscode:\n",
      "    nodes: all\n",
      "    type: vs_code\n",
      "  My_tensorboard:\n",
      "    nodes: all\n",
      "    type: tensor_board\n",
      "    log_dir: outputs/runs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading finetuning_hf_models (0.06 MBs): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55790/55790 [00:00<00:00, 284761.18it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n",
      "Readonly attribute status will be ignored in class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.JobService'>\n",
      "Readonly attribute status will be ignored in class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.JobService'>\n",
      "Readonly attribute status will be ignored in class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.JobService'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job created successfully\n",
      "https://ml.azure.com/runs/joyful_garlic_dlggv2dzhv?wsid=/subscriptions/6a01260f-39d6-415f-a6c9-cf4fd479cbec/resourcegroups/sriks-ml-rg/workspaces/sriks-ml-sea&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n"
     ]
    }
   ],
   "source": [
    "compute_cluster_name = \"AmlComputeCluster\"\n",
    "ml_client = create_ml_client()\n",
    "compute_cluster = create_aml_cluster(ml_client, compute_cluster_name = compute_cluster_name)\n",
    "env_docker_context = create_environment(ml_client)\n",
    "job = create_job(compute_cluster_name)\n",
    "print(f\"Creating job : {job.name}\")\n",
    "job = ml_client.jobs.create_or_update(job)\n",
    "print(\"Job created successfully\")\n",
    "print(job.studio_url)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Preparing\n",
      "Job status: Queued\n",
      "Job status: Queued\n",
      "Job status: Running\n",
      "Job status: Running\n",
      "Job status: Running\n",
      "Job status: Running\n",
      "Job status: Running\n",
      "Job status: Running\n",
      "Job status: Running\n",
      "Job status: Running\n"
     ]
    }
   ],
   "source": [
    "# Print status of the azure ml job\n",
    "import time\n",
    "while not job.is_complete:\n",
    "    job = ml_client.jobs.get(job.name)    \n",
    "    print(f\"Job status: {job.status}\") \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
