{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Huggingface models on Azure ML\n",
    "\n",
    "This notebook explains how to create an end-to-end lineage for hugging face models on Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, AzureCliCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import AmlCompute, ComputeInstance\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create MLClient\n",
    "def create_ml_client(credential=None):     \n",
    "    if credential is None:\n",
    "        try:\n",
    "            credential = DefaultAzureCredential()\n",
    "            # Check if given credential can get token successfully.\n",
    "            credential.get_token(\"https://management.azure.com/.default\")\n",
    "        except Exception as ex:\n",
    "            # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "            credential = AzureCliCredential()\n",
    "\n",
    "    return MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Environment\n",
    "def create_environment(ml_client, env_Name = \"finetune_hf_lora\"):\n",
    "    try:\n",
    "        env = ml_client.environments.get(env_Name, version=\"latest\")\n",
    "        print(\"successfully fetched environment:\", env.name)\n",
    "    except Exception as ex:\n",
    "        print(\"failed to fetch environment:\", env_Name)\n",
    "        print(f\"creating new environment {env_Name}\")\n",
    "        env_docker_context = Environment(\n",
    "            build=BuildContext(path=\"src/env\"),\n",
    "            name=env_Name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "        )\n",
    "        ml_client.environments.create_or_update(env_docker_context)\n",
    "        print(\"successfully created environment:\", env_docker_context.name)\n",
    "    return env_docker_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: .\\config.json\n"
     ]
    }
   ],
   "source": [
    "# Create Azure ML dataset from local file\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "VERSION = \"1.0\"\n",
    "NAME = \"squad_dev_v1\"\n",
    "def create_dataset(ml_client):\n",
    "    # Create a dataset from local file\n",
    "    dataset = Data(\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        path=\"./data/squad.json\",\n",
    "        description=\"SquAD v1.0 dev dataset\",\n",
    "        name=NAME,\n",
    "        version=VERSION,\n",
    "    )\n",
    "    return ml_client.data.create_or_update(dataset)\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "try:\n",
    "   ml_client.data.get(NAME, VERSION)\n",
    "except:\n",
    "   create_dataset(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: .\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to fetch environment: finetune_hf_lora\n",
      "creating new environment finetune_hf_lora\n",
      "successfully created environment: finetune_hf_lora\n"
     ]
    }
   ],
   "source": [
    "gpu_cluster_name = \"AmlComputeCluster\"\n",
    "cpu_compute_name = \"vism-cpu-8c\"\n",
    "env_name = \"finetune_hf_lora\"\n",
    "data_asset = ml_client.data.get(NAME, version=VERSION)\n",
    "ml_client = create_ml_client()\n",
    "mlflow_tracking_uri = ml_client.workspaces.get(ml_client.workspace_name).mlflow_tracking_uri\n",
    "env_docker_context = create_environment(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading src (0.01 MBs): 100%|##########| 12239/12239 [00:00<00:00, 43549.04it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component train_hf_qna_squad_dev_v1 with Version 2024-08-14-14-12-09-1577996 is registered\n"
     ]
    }
   ],
   "source": [
    "training_component = command(\n",
    "    name=\"train_hf_qna_squad_dev_v1\",\n",
    "    display_name=\"Fine tune HF model\",\n",
    "    description=\"Fine tune HF model on Squad dataset for QnA task\",\n",
    "    inputs={\n",
    "        \"data\": Input(mode=InputOutputModes.MOUNT, type=AssetTypes.URI_FILE, destination_path_on_compute=\"data\"),\n",
    "        \"mlflow_tracking_uri\": \"\"\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_output=Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code='./src',\n",
    "    command=\"\"\"python train_component.py \\\n",
    "            --data_path ${{inputs.data}} \\\n",
    "            --target_input_length=512 \\\n",
    "            --model_name \"microsoft/Phi-3-mini-4k-instruct\" \\\n",
    "            --num_epochs 1 \\\n",
    "            --mlflow_tracking_uri \"${{inputs.mlflow_tracking_uri}}\" \\\n",
    "            --target_max_length=100 \\\n",
    "            --train_size=1000\"\"\",\n",
    "    environment=f\"{env_name}@latest\",\n",
    ")\n",
    "\n",
    "training_component = ml_client.create_or_update(training_component.component)\n",
    "print(f\"Component {training_component.name} with Version {training_component.version} is registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(path=data_asset.id, mode=InputOutputModes.MOUNT, type=AssetTypes.URI_FILE, destination_path_on_compute=\"data\")\n",
    "\n",
    "@pipeline(\n",
    "    default_compute=\"serverless\", \n",
    "    name=\"Phi3 training pipeline\", \n",
    "    description=\"Pipeline to train a phi3 model from hugging face.\"\n",
    ")\n",
    "def finetune_hfmodels_azureml_pipeline(pipeline_input_data):\n",
    "    \"\"\"E2E Hugging face Q and A model using huggingface, peft, azureml and python sdk.\"\"\"\n",
    "    train_node = training_component(data=pipeline_input_data, mlflow_tracking_uri=mlflow_tracking_uri)\n",
    "    train_node.compute = cpu_compute_name\n",
    "\n",
    "# create a pipeline\n",
    "pipeline_job = finetune_hfmodels_azureml_pipeline(pipeline_input_data=inputs)\n",
    "pipeline_job = ml_client.jobs.create_or_update(pipeline_job, experiment_name=\"finetuning_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline shy_garden_6f36cff62y created\n",
      "Pipeline shy_garden_6f36cff62y status: Running\n",
      "Pipeline shy_garden_6f36cff62y status: Running\n",
      "Pipeline shy_garden_6f36cff62y status: Running\n",
      "Pipeline shy_garden_6f36cff62y status: Failed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(f\"Pipeline {pipeline_job.name} created\")\n",
    "job_status = ml_client.jobs.get(pipeline_job.name)\n",
    "print(f\"Pipeline {pipeline_job.name} status: {job_status.status}\")\n",
    "while job_status.status not in [\"Completed\", \"Failed\", \"Canceled\"]:\n",
    "    job_status = ml_client.jobs.get(pipeline_job.name)\n",
    "    print(f\"Pipeline {pipeline_job.name} status: {job_status.status}\")\n",
    "    time.sleep(30)\n",
    "\n",
    "# ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
