{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Huggingface models on Azure ML\n",
    "\n",
    "This notebook explains how to create an end-to-end lineage for hugging face models on Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential, AzureCliCredential\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLClient\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mentities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AmlCompute, ComputeInstance\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential, AzureCliCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import AmlCompute, ComputeInstance\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azure.ai.ml import command, Input, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create MLClient\n",
    "def create_ml_client(credential=None):     \n",
    "    if credential is None:\n",
    "        try:\n",
    "            credential = DefaultAzureCredential()\n",
    "            # Check if given credential can get token successfully.\n",
    "            credential.get_token(\"https://management.azure.com/.default\")\n",
    "        except Exception as ex:\n",
    "            # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "            credential = AzureCliCredential()\n",
    "\n",
    "    return MLClient.from_config(credential=credential)\n",
    "\n",
    "# Function to create AML Compute Cluster\n",
    "def create_aml_cluster(ml_client, gpu_cluster_name = \"AmlComputeCluster\", vm_size = \"Standard_NC4as_T4_v3\", min_nodes = 1, max_nodes = 2):\n",
    "    # If you already have a gpu cluster, mention it here. Else will create a new one    \n",
    "    try:\n",
    "        compute = ml_client.compute.get(gpu_cluster_name)\n",
    "        print(\"successfully fetched compute:\", compute.name)\n",
    "    except Exception as ex:\n",
    "        print(\"failed to fetch compute:\", gpu_cluster_name)\n",
    "        print(f\"creating new {vm_size} compute\")\n",
    "        compute = AmlCompute(\n",
    "            name=gpu_cluster_name,\n",
    "            size=vm_size,\n",
    "            min_instances=min_nodes,\n",
    "            max_instances=max_nodes,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "        print(\"successfully created compute:\", compute.name)\n",
    "    return compute\n",
    "\n",
    "def create_aml_node(ml_client, node_name = \"vism-cpu-4c\", vm_size = \"Standard_E4ds_v4\"):\n",
    "    try:\n",
    "        compute = ml_client.compute.get(node_name)\n",
    "        print(\"successfully fetched compute:\", compute.name)\n",
    "    except Exception as ex:\n",
    "        print(\"failed to fetch compute:\", node_name)\n",
    "        print(f\"creating new {vm_size} compute\")\n",
    "        compute = ComputeInstance(\n",
    "            name=node_name,\n",
    "            size=vm_size\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "        print(\"successfully created compute:\", compute.name)\n",
    "    return compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Environment\n",
    "def create_environment(ml_client, env_Name = \"finetune_hf_lora\"):\n",
    "    try:\n",
    "        env = ml_client.environments.get(env_Name, version=\"latest\")\n",
    "        print(\"successfully fetched environment:\", env.name)\n",
    "    except Exception as ex:\n",
    "        print(\"failed to fetch environment:\", env_Name)\n",
    "        print(f\"creating new environment {env_Name}\")\n",
    "        env_docker_context = Environment(\n",
    "            build=BuildContext(path=\"src/env\"),\n",
    "            name=env_Name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "        )\n",
    "        ml_client.environments.create_or_update(env_docker_context)\n",
    "        print(\"successfully created environment:\", env_docker_context.name)\n",
    "    return env_docker_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ./config.json\n"
     ]
    }
   ],
   "source": [
    "# Create Azure ML dataset from local file\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "VERSION = \"1.0\"\n",
    "NAME = \"squad_dev_v1\"\n",
    "def create_dataset(ml_client):\n",
    "    # Create a dataset from local file\n",
    "    dataset = Data(\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        path=\"./data/squad.json\",\n",
    "        description=\"SquAD v1.0 dev dataset\",\n",
    "        name=NAME,\n",
    "        version=VERSION,\n",
    "    )\n",
    "    return ml_client.data.create_or_update(dataset)\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "try:\n",
    "   ml_client.data.get(NAME, VERSION)\n",
    "except:\n",
    "   create_dataset(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ml_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AssetTypes, InputOutputModes\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Set the input for the job:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data_asset \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget(NAME, version\u001b[38;5;241m=\u001b[39mVERSION)\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m Input(path\u001b[38;5;241m=\u001b[39mdata_asset\u001b[38;5;241m.\u001b[39mid, mode\u001b[38;5;241m=\u001b[39mInputOutputModes\u001b[38;5;241m.\u001b[39mMOUNT, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mAssetTypes\u001b[38;5;241m.\u001b[39mURI_FILE, destination_path_on_compute\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m mlflow_tracking_uri \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mworkspaces\u001b[38;5;241m.\u001b[39mget(ml_client\u001b[38;5;241m.\u001b[39mworkspace_name)\u001b[38;5;241m.\u001b[39mmlflow_tracking_uri\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ml_client' is not defined"
     ]
    }
   ],
   "source": [
    "# from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "# # Set the input for the job:\n",
    "# data_asset = ml_client.data.get(NAME, version=VERSION)\n",
    "# inputs = Input(path=data_asset.id, mode=InputOutputModes.MOUNT, type=AssetTypes.URI_FILE, destination_path_on_compute=\"data\")\n",
    "# mlflow_tracking_uri = ml_client.workspaces.get(ml_client.workspace_name).mlflow_tracking_uri\n",
    "\n",
    "# # Function to create Job\n",
    "# def create_job(compute_cluster, script_file = \"finetune_hf_models.py\", job_name = \"hf_finetuning\", env_name = \"finetune_hf_lora\"):\n",
    "#     job = command(\n",
    "#         code=\".\",\n",
    "#         command=f\"python src/{script_file} \\\n",
    "#             --model_name google/flan-t5-small \\\n",
    "#             --num_epochs 1 \\\n",
    "#             --mlflow_tracking_uri {mlflow_tracking_uri} \\\n",
    "#             --data_path ${{inputs.data}} \\\n",
    "#             --target_input_length=512 \\\n",
    "#             --target_max_length=100 \\\n",
    "#             --train_size=1000\",\n",
    "#     compute=compute_cluster,\n",
    "#     inputs={\n",
    "#       \"data\": inputs\n",
    "#     },\n",
    "#     services={\n",
    "#       \"My_jupyterlab\": JupyterLabJobService(\n",
    "#         nodes=\"all\" # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\n",
    "#       ),\n",
    "#       \"My_vscode\": VsCodeJobService(\n",
    "#         nodes=\"all\"\n",
    "#       ),\n",
    "#       \"My_tensorboard\": TensorBoardJobService(\n",
    "#         nodes=\"all\",\n",
    "#         log_dir=\"outputs/runs\"  # relative path of Tensorboard logs (same as in your training script)         \n",
    "#       ),\n",
    "#     },\n",
    "#     environment=f\"{env_name}@latest\",\n",
    "#     instance_count=1,  \n",
    "#     display_name=job_name)\n",
    "#     return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ./config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully fetched compute: AMLComputeCluster\n",
      "failed to fetch environment: finetune_hf_lora\n",
      "creating new environment finetune_hf_lora\n",
      "successfully created environment: finetune_hf_lora\n",
      "Creating job hf_finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading finetuning_hf_models (85.26 MBs): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 85262513/85262513 [00:00<00:00, 115392488.33it/s]\n",
      "\n",
      "\n",
      "Use of {} for parameters is deprecated, instead use ${{}}.\n",
      "Readonly attribute status will be ignored in class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.JobService'>\n",
      "Readonly attribute status will be ignored in class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.JobService'>\n",
      "Readonly attribute status will be ignored in class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.JobService'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job created successfully\n",
      "https://ml.azure.com/runs/willing_avocado_h5g2k7n8gc?wsid=/subscriptions/6a01260f-39d6-415f-a6c9-cf4fd479cbec/resourcegroups/sriks-ml-rg/workspaces/sriks-ml-sea&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n"
     ]
    }
   ],
   "source": [
    "# gpu_cluster_name = \"AmlComputeCluster\"\n",
    "# ml_client = create_ml_client()\n",
    "# compute_cluster = create_aml_cluster(ml_client, gpu_cluster_name = gpu_cluster_name)\n",
    "# env_docker_context = create_environment(ml_client)\n",
    "# job = create_job(gpu_cluster_name)\n",
    "# print(f\"Creating job {job.display_name}\")\n",
    "# # Submit the job and wait for completion\n",
    "# job = ml_client.jobs.create_or_update(job, show_output = True)\n",
    "# print(\"Job created successfully\")\n",
    "# print(job.studio_url)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 06:01:23.672648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-10 06:01:26.220176: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-10 06:01:26.930770: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/\n",
      "2024-04-10 06:01:26.930803: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-04-10 06:01:32.833427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/\n",
      "2024-04-10 06:01:32.833630: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/\n",
      "2024-04-10 06:01:32.833645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function prepare_data_component in module src.prep.prepare_component:\n",
      "\n",
      "prepare_data_component()\n",
      "\n",
      "Help on function training_component in module src.train.train_component:\n",
      "\n",
      "training_component(input_data: <mldesigner._input_output.Input object at 0x7f844d4ac7f0>, output_model: <mldesigner._input_output.Output object at 0x7f844d4acbb0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Users.vism.sriksml.azureml.finetuning_hf_models.src.prepare_component import prepare_data_component\n",
    "from Users.vism.sriksml.azureml.finetuning_hf_models.src.train_component import training_component\n",
    "\n",
    "help(prepare_data_component)\n",
    "help(training_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ./config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully fetched compute: AMLComputeCluster\n",
      "successfully fetched compute: vism-cpu-4c\n",
      "failed to fetch environment: finetune_hf_lora\n",
      "creating new environment finetune_hf_lora\n",
      "successfully created environment: finetune_hf_lora\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "\n",
    "gpu_cluster_name = \"AmlComputeCluster\"\n",
    "cpu_compute_name = \"vism-cpu-4c\"\n",
    "env_name = \"finetune_hf_lora\"\n",
    "data_asset = ml_client.data.get(NAME, version=VERSION)\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "mlflow_tracking_uri = ml_client.workspaces.get(ml_client.workspace_name).mlflow_tracking_uri\n",
    "compute_cluster = create_aml_cluster(ml_client, gpu_cluster_name = gpu_cluster_name)\n",
    "compute_instance = create_aml_node(ml_client, node_name = cpu_compute_name)\n",
    "env_docker_context = create_environment(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component data_prep_qna_squad_dev_v1 with Version 2024-04-10-07-12-34-5797646 is registered\n"
     ]
    }
   ],
   "source": [
    "# Define the data preparation component using python SDK\n",
    "# Alternatively we can declare components using YAML\n",
    "# The component is defined using the command function\n",
    "data_prep_component = command(\n",
    "    name=\"data_prep_qna_squad_dev_v1\",\n",
    "    display_name=\"Squad dataset preparation component\",\n",
    "    description=\"Reads the SQuAD v1.0 dev dataset and prepares it for fine tuning HF models\",\n",
    "    inputs={\n",
    "        \"data\": Input(mode=InputOutputModes.MOUNT, type=AssetTypes.URI_FILE, destination_path_on_compute=\"data\")\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code='./src',\n",
    "    command=\"\"\"python prepare_component.py \\\n",
    "            --data_path ${{inputs.data}} \\\n",
    "            --target_input_length=512 \\\n",
    "            --target_max_length=100 \\\n",
    "            --train_size=1000\"\"\",\n",
    "    environment=f\"{env_name}@latest\",\n",
    ")\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
    "print(f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component train_hf_qna_squad_dev_v1 with Version 1 is registered\n"
     ]
    }
   ],
   "source": [
    "training_component = command(\n",
    "    name=\"train_hf_qna_squad_dev_v1\",\n",
    "    display_name=\"Fine tune HF model\",\n",
    "    description=\"Fine tune HF model on Squad dataset for QnA task\",\n",
    "    inputs={\n",
    "        \"train_data\": Input(mode=InputOutputModes.MOUNT, type=\"uri_folder\")\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_output=Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code='./src',\n",
    "    command=\"\"python prepare_component.py \\\n",
    "            --data_path ${{inputs.train_data}} \\\n",
    "            --target_input_length=512 \\\n",
    "            --model_name \"google/flan-t5-small\" \\\n",
    "            --num_epochs 1 \\\n",
    "            --mlflow_tracking_uri {mlflow_tracking_uri} \\\n",
    "            --target_max_length=100 \\\n",
    "            --train_size=1000\"\",\n",
    "    environment=f\"{env_name}@latest\",\n",
    ")\n",
    "\n",
    "training_component = ml_client.create_or_update(training_component.component)\n",
    "print(f\"Component {training_component.name} with Version {training_component.version} is registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: boring_yam_gd699rprr1\n",
      "Web View: https://ml.azure.com/runs/boring_yam_gd699rprr1?wsid=/subscriptions/6a01260f-39d6-415f-a6c9-cf4fd479cbec/resourcegroups/sriks-ml-rg/workspaces/sriks-ml-sea\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-04-10 07:14:28Z] Submitting 1 runs, first five are: 251421c1:24b5eb0f-de3a-4e5c-ad50-42b453675075\n",
      "[2024-04-10 07:20:53Z] Completing processing run id 24b5eb0f-de3a-4e5c-ad50-42b453675075.\n",
      "[2024-04-10 07:20:54Z] Submitting 1 runs, first five are: 993c760a:f693c5fd-4d03-4b09-8fbd-17f6c82c082a\n",
      "[2024-04-10 07:21:48Z] Execution of experiment failed, update experiment status and cancel running nodes.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: boring_yam_gd699rprr1\n",
      "Web View: https://ml.azure.com/runs/boring_yam_gd699rprr1?wsid=/subscriptions/6a01260f-39d6-415f-a6c9-cf4fd479cbec/resourcegroups/sriks-ml-rg/workspaces/sriks-ml-sea\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /train_node. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"southeastasia\",\n    \"location\": \"southeastasia\",\n    \"time\": \"2024-04-10T07:21:48.302878Z\",\n    \"component_name\": \"\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m pipeline_job \u001b[38;5;241m=\u001b[39m finetune_hfmodels_azureml_pipeline(pipeline_input_data\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m     16\u001b[0m pipeline_job \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate_or_update(pipeline_job, experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuning_hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azure/ai/ml/_telemetry/activity.py:275\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[0;32m--> 275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azure/ai/ml/operations/_job_operations.py:800\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m--> 800\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azure/ai/ml/operations/_job_ops_helper.py:332\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    330\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[1;32m    333\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)),\n\u001b[1;32m    334\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m    335\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException raised on failed job.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    336\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[1;32m    337\u001b[0m         )\n\u001b[1;32m    339\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    340\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /train_node. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"southeastasia\",\n    \"location\": \"southeastasia\",\n    \"time\": \"2024-04-10T07:21:48.302878Z\",\n    \"component_name\": \"\"\n} "
     ]
    }
   ],
   "source": [
    "inputs = Input(path=data_asset.id, mode=InputOutputModes.MOUNT, type=AssetTypes.URI_FILE, destination_path_on_compute=\"data\")\n",
    "\n",
    "\n",
    "@pipeline(\n",
    "    default_compute=gpu_cluster_name,\n",
    ")\n",
    "def finetune_hfmodels_azureml_pipeline(pipeline_input_data):\n",
    "    \"\"\"E2E Hugging face Q and A model using huggingface, peft, azureml and python sdk.\"\"\"\n",
    "    prepare_data_node = data_prep_component(data=pipeline_input_data)\n",
    "    prepare_data_node.compute = cpu_compute_name\n",
    "    train_node = training_component(train_data=prepare_data_node.outputs.train_data)\n",
    "    train_node.compute = gpu_cluster_name\n",
    "\n",
    "# create a pipeline\n",
    "pipeline_job = finetune_hfmodels_azureml_pipeline(pipeline_input_data=inputs)\n",
    "pipeline_job = ml_client.jobs.create_or_update(pipeline_job, experiment_name=\"finetuning_hf\")\n",
    "ml_client.jobs.stream(pipeline_job.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
