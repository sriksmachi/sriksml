{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install langchain langchain_community\n",
    "# %pip install neo4j\n",
    "# %pip install langchain_openai\n",
    "# %pip install python-dotenv\n",
    "# %pip install chromadb\n",
    "# %pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEO4J_URI'] = 'bolt://localhost:7690'\n",
    "os.environ['NEO4J_USERNAME'] = 'neo4j'\n",
    "os.environ['NEO4J_PASSWORD'] = 'Password@123'\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://sriks-openai.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-05-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"] = \"gpt-4o\"\n",
    "os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"] = \"text-embedding-ada-002\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"],\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What is the root cause request id 8ff8696695aa73588ac454809741e2ea\",\n",
    "        \"query\": \"MATCH (n)-[r:DEPENDS_ON]->(m) where n.id <> 'ROOT' OR m.id <> 'ROOT' and r.operationId = 8ff8696695aa73588ac454809741e2ea RETURN n.id, r.duration, r.operation_Name, m.id ORDER BY r.duration DESC LIMIT 3\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is the longest running operation\",\n",
    "        \"query\": \"MATCH (n)-[r:DEPENDS_ON]->(m) where n.id <> 'ROOT' OR m.id <> 'ROOT' RETURN n.id, r.duration, r.operation_Name, m.id ORDER BY r.duration DESC LIMIT 3\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector, MaxMarginalRelevanceExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.graph_qa.cypher import construct_schema\n",
    "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# load human intervention tools\n",
    "tools = load_tools(\n",
    "    [\"human\"], llm\n",
    ")\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "QA_GENERATION_TEMPLATE = \"\"\"\n",
    "       Task: answer the question you are given based on the context provided.\n",
    "       Instructions:\n",
    "        You are an assistant that helps to form nice and human understandable answers. \n",
    "        Use the context information provided to generate a well organized and comprehensve answer to the user's question. \n",
    "        When the provided information contains multiple elements, structure your answer as a bulleted or numbered list to enhance clarity and readability.\n",
    "        You must use the information to construct your answer. \n",
    "        The provided information is authoritative; do not doubt it or try to use your internal knowledge to correct it. \n",
    "        Make the answer sound like a response to the question without mentioning that you based the result on the given information. \n",
    "        If there is no information provided, say that the knowledge base returned empty results.\n",
    "\n",
    "        Here's the information:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "            \"\"\"\n",
    "EXAMPLES_PROMPT_TEMPLATE = \"\"\"   \n",
    "                Input: {question},\n",
    "                Output: {query}\n",
    "            \"\"\"\n",
    "\n",
    "qaPrompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=QA_GENERATION_TEMPLATE)\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"query\"], template=EXAMPLES_PROMPT_TEMPLATE)\n",
    "\n",
    "cypherPromptTemplate = \"\"\"\n",
    "You are an expert Neo4j Developer translating user questions into Cypher to answer questions.\n",
    "Convert the user's question based on the schema.\n",
    "Instructions: Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "\n",
    "Important: In the generated Cypher query, the RETURN statement must explicitly include the property values used in the query's filtering condition, alongside the main information requested from the original question.\n",
    "\n",
    "Question: {question}\n",
    "input:\n",
    "\"\"\"\n",
    "\n",
    "similaritySelector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples, \n",
    "    embeddings=embeddings, \n",
    "    k=1,\n",
    "    vectorstore_cls=Chroma\n",
    ")\n",
    "\n",
    "cypherPrompt = FewShotPromptTemplate(\n",
    "    example_selector=similaritySelector,\n",
    "    example_prompt=example_prompt,\n",
    "    input_variables=[\"question\", \"schema\"], \n",
    "    prefix=cypherPromptTemplate,\n",
    "    suffix=\"the question is:{question}\",\n",
    ")\n",
    "\n",
    "cypherqachain = GraphCypherQAChain.from_llm(\n",
    "    llm = llm,\n",
    "    return_intermediate_steps=True,\n",
    "    validate_cypher=True,\n",
    "    graph=graph, \n",
    "    verbose=True,\n",
    "    k=3,\n",
    "    use_function_response=True,\n",
    "    cypherPrompt=cypherPrompt,\n",
    "    return_direct=True,\n",
    "    output_key=\"input\",\n",
    ") \n",
    "\n",
    "schema  = construct_schema(graph.get_structured_schema, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (n:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(m:SQL)\n",
      "RETURN n.id AS cloudRoleName, COUNT(r) AS azureDependencies\n",
      "ORDER BY azureDependencies DESC\n",
      "LIMIT 1\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'which nodes has max azure dependencies',\n",
       " 'input': [{'cloudRoleName': 'people-api', 'azureDependencies': 23}],\n",
       " 'intermediate_steps': [{'query': 'cypher\\nMATCH (n:cloudRoleName)-[r:AZURE_DEPENDS_ON]->(m:SQL)\\nRETURN n.id AS cloudRoleName, COUNT(r) AS azureDependencies\\nORDER BY azureDependencies DESC\\nLIMIT 1\\n'}]}"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test cypher chain invocation\n",
    "cypherqachain.invoke({\"query\": \"which nodes has max azure dependencies\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code, creates a confidence chain that will be used to ask LLM how confident it is query generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain.invoke({\"query\": \"What is the root cause request id 8ff8696695aa73588ac454809741e2ea\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Work In Progress]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc_df = pd.read_csv('pc.csv')\n",
    "# pc_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc_df.iloc[0]['InstanceName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parseServiceName(x):\n",
    "#     splits = x.split('/')\n",
    "#     servicename = splits[-1]\n",
    "#     return servicename\n",
    "# def parseInstanceId(x):\n",
    "#     splits = x.split('/')\n",
    "#     instanceid = splits[-2]\n",
    "#     return instanceid\n",
    "# pc_df['ServiceName'] = pc_df['InstanceName'].apply(lambda x: parseServiceName(x))\n",
    "# pc_df['InstanceId'] = pc_df['InstanceName'].apply(lambda x: parseInstanceId(x))\n",
    "# # pc_df.drop(columns=['InstanceName'], inplace=True)\n",
    "# pc_df.rename(columns={'TimeGenerated [UTC]': 'timestamp'}, inplace=True)\n",
    "# pc_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pivot the table to get the service names as columns\n",
    "# pc_df = pc_df.pivot(index='timestamp', columns='ServiceName', values='avg_CounterValue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
