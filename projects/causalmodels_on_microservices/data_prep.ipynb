{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in c:\\code\\sriksml\\.venv\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas\n",
    "# %pip install langchain langchain_community\n",
    "# %pip install neo4j\n",
    "# %pip install langchain_openai\n",
    "# %pip install python-dotenv\n",
    "# %pip install chromadb\n",
    "# %pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEO4J_URI'] = 'bolt://localhost:7690'\n",
    "os.environ['NEO4J_USERNAME'] = 'neo4j'\n",
    "os.environ['NEO4J_PASSWORD'] = 'Password@123'\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://sriks-openai.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-05-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"] = \"gpt-4o\"\n",
    "os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"] = \"text-embedding-ada-002\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"],\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What is the root cause request id 8ff8696695aa73588ac454809741e2ea\",\n",
    "        \"query\": \"MATCH (n)-[r:DEPENDS_ON]->(m) where n.id <> 'ROOT' OR m.id <> 'ROOT' and r.operationId = 8ff8696695aa73588ac454809741e2ea RETURN n.id, r.duration, r.operation_Name, m.id ORDER BY r.duration DESC LIMIT 3\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is the longest running operation\",\n",
    "        \"query\": \"MATCH (n)-[r:DEPENDS_ON]->(m) where n.id <> 'ROOT' OR m.id <> 'ROOT' RETURN n.id, r.duration, r.operation_Name, m.id ORDER BY r.duration DESC LIMIT 3\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "QA_GENERATION_TEMPLATE = \"\"\"\n",
    "       Task: answer the question you are given based on the context provided.\n",
    "       Instructions:\n",
    "        You are an assistant that helps to form nice and human understandable answers. \n",
    "        Use the context information provided to generate a well organized and comprehensve answer to the user's question. \n",
    "        When the provided information contains multiple elements, structure your answer as a bulleted or numbered list to enhance clarity and readability.\n",
    "        You must use the information to construct your answer. \n",
    "        The provided information is authoritative; do not doubt it or try to use your internal knowledge to correct it. \n",
    "        Make the answer sound like a response to the question without mentioning that you based the result on the given information. \n",
    "        If there is no information provided, say that the knowledge base returned empty results.\n",
    "\n",
    "        Here's the information:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "            \"\"\"\n",
    "EXAMPLES_PROMPT_TEMPLATE = \"\"\"   \n",
    "                Input: {question},\n",
    "                Output: {query}\n",
    "            \"\"\"\n",
    "qaPrompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=QA_GENERATION_TEMPLATE)\n",
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"query\"], template=EXAMPLES_PROMPT_TEMPLATE)\n",
    "\n",
    "cypherPromptTemplate = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "\n",
    "The question is:\n",
    "{question} \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "similaritySelector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples, \n",
    "    embeddings=embeddings, \n",
    "    k=1,\n",
    "    vectorstore_cls=Chroma\n",
    ")\n",
    "\n",
    "cypherPrompt = FewShotPromptTemplate(\n",
    "    example_selector=similaritySelector,\n",
    "    example_prompt=example_prompt,\n",
    "    # examples=examples, # either examples or example_selector can be used at a time\n",
    "    input_variables=[\"schema\", \"question\"], \n",
    "    prefix=cypherPromptTemplate,\n",
    "    suffix=\"The question is:\\n{question}\",\n",
    ")\n",
    "\n",
    "similar_prompt = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm = llm,\n",
    "    qa_llm = llm, \n",
    "    return_intermediate_steps=True,\n",
    "    validate_cypher=True,\n",
    "    graph=graph, \n",
    "    verbose=True,\n",
    "    qa_llm_kwargs={\"prompt\":qaPrompt},\n",
    "    cypher_llm_kwargs={ \"prompt\": cypherPrompt},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:Generate Cypher statement to query a graph database.\n",
      "Instructions:\n",
      "Use only the provided relationship types and properties in the schema.\n",
      "Do not use any other relationship types or properties that are not provided.\n",
      "Schema:\n",
      "Node properties:\n",
      "cloudRoleName {id: STRING}\n",
      "SQL {id: STRING}\n",
      "Relationship properties:\n",
      "DEPENDS_ON {counter: INTEGER, operation_Name: STRING, duration: FLOAT, itemType: STRING, operationId: STRING, type: STRING, id: STRING}\n",
      "INPROC_TT {operation_Name: STRING, duration: FLOAT, itemType: STRING, id: STRING, counter: INTEGER, operationId: STRING, type: STRING}\n",
      "AZURE_DEPENDS_ON {itemType: STRING, operation_Name: STRING, id: STRING, operationId: STRING, type: STRING, duration: FLOAT, counter: INTEGER}\n",
      "The relationships:\n",
      "(:cloudRoleName)-[:DEPENDS_ON]->(:cloudRoleName)\n",
      "(:cloudRoleName)-[:INPROC_TT]->(:cloudRoleName)\n",
      "(:cloudRoleName)-[:AZURE_DEPENDS_ON]->(:SQL)\n",
      "\n",
      "Note: Do not include any explanations or apologies in your responses.\n",
      "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
      "Do not include any text except the generated Cypher statement.\n",
      "\n",
      "The question is:\n",
      "What is the root cause request id 8ff8696695aa73588ac454809741e2ea \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "                Input: What is the root cause request id 8ff8696695aa73588ac454809741e2ea,\n",
      "                Output: MATCH (n)-[r:DEPENDS_ON]->(m) where n.id <> 'ROOT' OR m.id <> 'ROOT' and r.operationId = 8ff8696695aa73588ac454809741e2ea RETURN n.id, r.duration, r.operation_Name, m.id ORDER BY r.duration DESC LIMIT 3\n",
      "            \n",
      "\n",
      "The question is:\n",
      "What is the root cause request id 8ff8696695aa73588ac454809741e2ea\n"
     ]
    }
   ],
   "source": [
    "print(cypherPrompt.format(schema=graph.schema, question=\"What is the root cause request id 8ff8696695aa73588ac454809741e2ea\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (n)-[r]->(m) \n",
      "WHERE r.operationId = \"8ff8696695aa73588ac454809741e2ea\"\n",
      "RETURN r.operation_Name, r.duration, r.operationId \n",
      "ORDER BY r.duration DESC \n",
      "LIMIT 1\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'r.operation_Name': 'POST Timesheet/SubmitShiftEntry', 'r.duration': 1868.0681, 'r.operationId': '8ff8696695aa73588ac454809741e2ea'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "- The longest operation in request ID 8ff8696695aa73588ac454809741e2ea is \"POST Timesheet/SubmitShiftEntry\" with a duration of 1868.0681 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "result = cypher_chain({\"query\": \"What is the longest operation in request id 8ff8696695aa73588ac454809741e2ea\"})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Work In Progress]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc_df = pd.read_csv('pc.csv')\n",
    "# pc_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc_df.iloc[0]['InstanceName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parseServiceName(x):\n",
    "#     splits = x.split('/')\n",
    "#     servicename = splits[-1]\n",
    "#     return servicename\n",
    "# def parseInstanceId(x):\n",
    "#     splits = x.split('/')\n",
    "#     instanceid = splits[-2]\n",
    "#     return instanceid\n",
    "# pc_df['ServiceName'] = pc_df['InstanceName'].apply(lambda x: parseServiceName(x))\n",
    "# pc_df['InstanceId'] = pc_df['InstanceName'].apply(lambda x: parseInstanceId(x))\n",
    "# # pc_df.drop(columns=['InstanceName'], inplace=True)\n",
    "# pc_df.rename(columns={'TimeGenerated [UTC]': 'timestamp'}, inplace=True)\n",
    "# pc_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pivot the table to get the service names as columns\n",
    "# pc_df = pc_df.pivot(index='timestamp', columns='ServiceName', values='avg_CounterValue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
