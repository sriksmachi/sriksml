{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e6398c",
   "metadata": {},
   "source": [
    "# Multimodal RAG with embeddings of Text, Image and Table Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2893d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet azure-ai-documentintelligence \"python-dotenv\" \"pandas\" \"langchain-openai\" \"azure-ai-vision-imageanalysis\" \"azure-search-documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeOutputOption\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "import pandas as pd\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "import uuid\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "di_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "di_credential = AzureKeyCredential(os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\"))\n",
    "chat_model = os.getenv(\"AOAI_CHAT_DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "aoai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "vision_model_endpoint = os.getenv(\"AZURE_VISION_MODEL_ENDPOINT\")\n",
    "vision_model_key = os.getenv(\"AZURE_VISION_MODEL_KEY\")\n",
    "search_endpoint = os.getenv(\"AZURE_AI_SEARCH_SERVICE_NAME\")\n",
    "search_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\")\n",
    "embeddings_model = os.getenv(\"AOAI_EMBEDDINGS_DEPLOYMENT_NAME\", \"text-embedding-3-small\")\n",
    "search_index_name = os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\", \"langchain-vector-demo\")\n",
    "\n",
    "# Create a Document Intelligence client for synchronous operations,\n",
    "document_intelligence_client = DocumentIntelligenceClient(di_endpoint, di_credential)\n",
    "\n",
    "# Create an Image Analysis client for synchronous operations,\n",
    "# using API key authentication\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=vision_model_endpoint,\n",
    "    credential=AzureKeyCredential(vision_model_key)\n",
    ")\n",
    "\n",
    "# Summary chain\n",
    "llm_client = AzureChatOpenAI(\n",
    "    azure_deployment=chat_model,\n",
    "    api_version=\"2023-05-15\",\n",
    "    temperature=0.3,\n",
    "    model_name=chat_model,\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    ")\n",
    "\n",
    "embeddings_model_client = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    "    azure_deployment=embeddings_model,\n",
    "    \n",
    ")\n",
    "\n",
    "# Download link: \n",
    "path_to_sample_documents = \"azure_ref_architecture.pdf\"\n",
    "with open(path_to_sample_documents, \"rb\") as f:\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\",\n",
    "        body=f,\n",
    "        output=[AnalyzeOutputOption.FIGURES],\n",
    "    )\n",
    "result = poller.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e726f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 31 documents with headings and text.\n",
      "Table 0 found on page 6.\n",
      "Table 1 found on page 28.\n",
      "Table 2 found on page 29.\n",
      "Extracted 3 tables from the document.\n",
      "Figure 2.1 extracted from page 2.\n",
      "Figure 12.1 extracted from page 12.\n",
      "Figure 26.1 extracted from page 26.\n",
      "Figure 27.1 extracted from page 27.\n",
      "Extracted 4 figures from the document.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract Text and Headings\n",
    "pages = []\n",
    "\n",
    "# Structure the documents as Pages -> Paragraphs -> Text\n",
    "for paragraph in result.paragraphs:\n",
    "    page = {}\n",
    "    page_number = paragraph[\"boundingRegions\"][0][\"pageNumber\"] if paragraph[\"boundingRegions\"] else None\n",
    "    role = paragraph[\"role\"] if \"role\" in paragraph else \"paragraph\"\n",
    "    content = paragraph[\"content\"] if \"content\" in paragraph else \"\"\n",
    "    \n",
    "    # check if page already exists\n",
    "    if page_number and any(p[\"page_number\"] == page_number for p in pages):\n",
    "        page = next(p for p in pages if p[\"page_number\"] == page_number)\n",
    "        page[\"content\"] = page.get(\"content\", \"\") + \" \" + content\n",
    "        if role in [\"heading\", \"sectionHeading\", \"title\"]:\n",
    "            page[\"sections\"] = page.get(\"sections\", \"\") + \",\" + content\n",
    "        continue\n",
    "    else:\n",
    "        page = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"content\": content,\n",
    "            \"sections\": content if role in [\"heading\", \"sectionHeading\", \"title\"] else \"\",\n",
    "            \"page_number\": page_number,\n",
    "            \"page_content\": [] # Initialize to fill tables, figures later\n",
    "        }\n",
    "    pages.append(page)\n",
    "print(f\"Extracted {len(pages)} documents with headings and text.\")\n",
    "\n",
    "\n",
    "# Extract Tables and add to pages\n",
    "table_elements = []\n",
    "if result.tables:\n",
    "    for idx, table in enumerate(result.tables):\n",
    "        page_number = table[\"boundingRegions\"][0][\"pageNumber\"] if table[\"boundingRegions\"] else None\n",
    "        page = next((p for p in pages if p[\"page_number\"] == page_number), None)\n",
    "        if not page:\n",
    "            print(f\"Page {page_number} not found for table {idx}. Skipping.\")\n",
    "            continue\n",
    "        print(f\"Table {idx} found on page {page_number}.\")\n",
    "        t_documents = []\n",
    "        for cell in table.cells:\n",
    "            t_document = {}\n",
    "            t_document[\"row\"] = cell.row_index\n",
    "            t_document[\"column\"] = cell.column_index\n",
    "            t_document[\"row_content\"] = cell.content\n",
    "            t_documents.append(t_document)\n",
    "        page[\"page_content\"].append({\n",
    "            \"type\": \"table\",\n",
    "            \"content\": str(t_documents),\n",
    "            \"page_number\": page_number,\n",
    "            \"id\": str(uuid.uuid4())\n",
    "        })\n",
    "    print(f\"Extracted {len(result.tables)} tables from the document.\")    \n",
    "\t\n",
    "# # Extract Figures and add to pages later\n",
    "if result.figures:\n",
    "    operation_id = poller.details[\"operation_id\"]\n",
    "    os.makedirs(\"extracted_images\", exist_ok=True)\n",
    "    for figure in result.figures:\n",
    "        page_number = figure[\"boundingRegions\"][0][\"pageNumber\"] if figure[\"boundingRegions\"] else None\n",
    "        page = next((p for p in pages if p[\"page_number\"] == page_number), None)\n",
    "        if not page:\n",
    "            print(f\"Page {page_number} not found for figure {figure.id}. Skipping.\")\n",
    "            continue\n",
    "        if figure.id:\n",
    "            response = document_intelligence_client.get_analyze_result_figure(\n",
    "                model_id=result.model_id, result_id=operation_id, figure_id=figure.id\n",
    "            )\n",
    "            print(f\"Figure {figure.id} extracted from page {page_number}.\")\n",
    "            page['page_content'].append({\n",
    "                \"type\": \"figure\",\n",
    "                \"content\": \"\", # this will be filled with the image summary later\n",
    "                \"image_url\": f\"extracted_images/{figure.id}.png\", # Save the public URL for the image in real world scenarios\n",
    "                \"page_number\": page_number,\n",
    "                \"id\": figure.id\n",
    "            })\n",
    "            with open(f\"extracted_images/{figure.id}.png\", \"wb\") as writer:\n",
    "                writer.writelines(response)\n",
    "else:\n",
    "    print(\"No figures found.\")\n",
    "print(f\"Extracted {len(result.figures)} figures from the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df9059b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Content with Tables\n",
      "{'id': '336b083f-0221-4c92-9dcd-e57e30ca1cd5', 'content': \"Identity and access management The following guidance expands on the identity and access management guidance in the App Service baseline architecture. The chat UI uses its managed identity to authenticate the chat UI API code to Foundry Agent Service by using the Azure AI Persistent Agents SDK. The Azure AI Foundry project also has a managed identity. This identity authenticates to services such as AI Search through connection definitions. The project makes those connections available to Foundry Agent Service. An Azure AI Foundry account can contain multiple Azure AI Foundry projects. Each project should use its own system-assigned managed identity. If different workload components require isolated access to connected data sources, create separate Azure AI Foundry projects within the same account and avoid sharing connections across them. If your workload doesn't require isolation, use a single project. Role-based access roles You're responsible for creating the required role assignments for the system-assigned managed identities. The following table summarizes the role assignment that you must add to App Service, the Azure AI Foundry project, and individuals who use the portal: [ Expand table Resource Role Scope App Service Azure AI User Azure AI Foundry account Azure AI Foundry project Search Index Data Reader AI Search Portal user (for each individual) Azure AI Developer Azure AI Foundry account Network security To simplify the learning experience for building an end-to-end chat solution, this architecture doesn't implement network security. It uses identity as its perimeter and uses public cloud constructs. Services such as AI Search, Azure AI Foundry, and App Service are reachable from the internet. This setup increases the attack surface of the architecture.\", 'sections': 'Identity and access management,Role-based access roles,Network security', 'page_number': 6, 'page_content': [{'type': 'table', 'content': \"[{'row': 0, 'column': 0, 'row_content': 'Resource'}, {'row': 0, 'column': 1, 'row_content': 'Role'}, {'row': 0, 'column': 2, 'row_content': 'Scope'}, {'row': 1, 'column': 0, 'row_content': 'App Service'}, {'row': 1, 'column': 1, 'row_content': 'Azure AI User'}, {'row': 1, 'column': 2, 'row_content': 'Azure AI Foundry account'}, {'row': 2, 'column': 0, 'row_content': 'Azure AI Foundry project'}, {'row': 2, 'column': 1, 'row_content': 'Search Index Data Reader'}, {'row': 2, 'column': 2, 'row_content': 'AI Search'}, {'row': 3, 'column': 0, 'row_content': 'Portal user (for each individual)'}, {'row': 3, 'column': 1, 'row_content': 'Azure AI Developer'}, {'row': 3, 'column': 2, 'row_content': 'Azure AI Foundry account'}]\", 'page_number': 6, 'id': '5cdb6756-863d-49e5-bc61-2917e00ad4c6'}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample Content with Figures\n",
      "{'id': '2969d79e-3931-4cde-be5e-60e62f291212', 'content': 'Azure AI Foundry Azure AI Foundry account 2 A App Service built- in authentication (Easy Auth) Managed identity App Service 1 https://domainname.azurewebsites.net 3 App Service instance Foundry Agent Service Managed identities User Azure AI Foundry project 6 5 4 Application Insights Azure Monitor Azure OpenAI model Monitoring Microsoft Azure Azure AI Search + Download a Visio filet of this architecture. Workflow The following workflow corresponds to the previous diagram: 1. An application user interacts with a web application that contains chat functionality. They issue an HTTPS request to the App Service default domain on azurewebsites. net . This domain automatically points to the App Service built-in public IP address. The Transport Layer Security connection is established from the client directly to App Service. Azure fully manages the certificate. 2. The App Service feature called Easy Auth ensures that the user who accesses the website is authenticated via Microsoft Entra ID. 3. The application code deployed to App Service handles the request and renders a chat UI for the application user. The chat UI code connects to APIs that are also hosted in that same App Service instance. The API code connects to an Azure AI agent in Azure AI Foundry by using the Azure AI Persistent Agents SDK.', 'sections': ',Workflow', 'page_number': 2, 'page_content': [{'type': 'figure', 'content': '', 'image_url': 'extracted_images/2.1.png', 'page_number': 2, 'id': '2.1'}]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Content with Tables\")\n",
    "print(pages[5])\n",
    "print('--'*50)\n",
    "print(\"Sample Content with Figures\")\n",
    "print(pages[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9802f0d",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cfd5560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:33,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "### Embedding and Vector Store Creation\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "import tqdm\n",
    "\n",
    "# Text Summarization \n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | llm_client | StrOutputParser()\n",
    "\n",
    "# Image Summarization\n",
    "def encode_image(image_path):\n",
    "    '''Getting the base64 string'''\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def image_summarize(image_path):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    prompt_text = \"You are an assistant tasked with summarizing images. Extract text, image from the input and Give a concise summary.\"\n",
    "    img_base64 = encode_image(image_path)\n",
    "    msg = llm_client.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt_text},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "# Summarize content of pages\n",
    "texts = [i[\"content\"] for i in pages if i[\"content\"] != \"\"]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "for i, page in tqdm.tqdm(enumerate(pages)):\n",
    "    page[\"summary\"] = text_summaries[i] if i < len(text_summaries) else \"No summary available\" \n",
    "    # page[\"content_vector\"] = embeddings_model_client.embed_documents(page[\"content\"])\n",
    "    for content in page[\"page_content\"]:\n",
    "        if content[\"type\"] == \"table\":\n",
    "            content[\"summary\"] = summarize_chain.invoke(content[\"content\"])\n",
    "            # content[\"content_vector\"] = embeddings_model_client.embed_documents(content[\"summary\"] + str(content[\"text\"])) # we remove the content vector for tables if it is too large\n",
    "        elif content[\"type\"] == \"figure\":\n",
    "            content[\"summary\"] = image_summarize(content['image_url'])\n",
    "            # content[\"content_vector\"] = embeddings_model_client.embed_documents(content[\"summary\"])\n",
    "        else:\n",
    "            content[\"summary\"] = \"No summary available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99a07f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc91ea90>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc91f650>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc91e490>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc91f990>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc813050>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc8121d0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc813590>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc810e10>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc811090>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc812f10>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc810650>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc812550>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc813350>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc8103d0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc811450>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc813090>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc811d50>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc811ad0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc8137d0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc811c10>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc810a50>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc812dd0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc8102d0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc812490>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc813010>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc812c90>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc8130d0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc813810>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc8123d0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc810910>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x198cc812c50>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=search_index_name,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "search_client.upload_documents(\n",
    "    documents=pages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca576254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import AzureAISearchRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Initialize user memory to store conversation context\n",
    "conversation_memory = {}\n",
    "\n",
    "retriever = AzureAISearchRetriever(\n",
    "    top_k=3, \n",
    "    service_name=search_endpoint,\n",
    "    api_key=search_key,\n",
    "    content_key=\"content\",  # This is the field name for the vector embeddings\n",
    "    index_name=search_index_name,\n",
    "    azure_ad_token=\"True\" # Bug in langchain-azure-ai-search, need to set this to True to use API Key\n",
    ")\n",
    "\n",
    "def splitDocs(docs, config):\n",
    "    imgs = []\n",
    "    table_summaries = []\n",
    "    image_summaries = []\n",
    "    image_urls = []\n",
    "    conversation_id = config['metadata']['conversation_id']\n",
    "    for doc in docs:\n",
    "        content = doc.page_content + \"\\n\" + \"\\n\".join([p[\"summary\"] for p in doc.metadata[\"page_content\"]])\n",
    "        for page_content in doc.metadata[\"page_content\"]:\n",
    "            if page_content[\"type\"] == \"table\":\n",
    "                table_summaries.append(page_content[\"summary\"])\n",
    "            elif page_content[\"type\"] == \"figure\":\n",
    "                image_summaries.append(page_content[\"summary\"])\n",
    "                image_urls.append(page_content[\"image_url\"])\n",
    "                base64_image = encode_image(page_content[\"image_url\"])\n",
    "                imgs.append(base64_image)\n",
    "    conversation_memory[conversation_id] = {'image_urls': image_urls}\n",
    "    return { 'content' : content, 'images' : imgs, 'table_summaries': table_summaries, 'image_summary': image_summaries, \"image_urls\": image_urls }\n",
    "\n",
    "def prompt_func(dict):\n",
    "    format_texts = \"\\n\".join(dict['context'][\"content\"])\n",
    "    table_summaries = \"\\n\".join(dict['context'][\"table_summaries\"])\n",
    "    image_summaries = \"\\n\".join(dict['context'][\"image_summary\"])\n",
    "    message = [\n",
    "        SystemMessage(\n",
    "                content=\"You are a helpful assistant! Your name is Bob. \\\n",
    "                    You are given context that contains document content along with tables and images. A summary of the content along with tables and images is provided. \\\n",
    "                    You are an expert in answering questions using text, tables, and images.\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \n",
    "                 \"text\": f\"\"\"\n",
    "                 Answer the question based only on the following context, which can include text, tables, and the below image: Question: {dict[\"question\"]} \\\n",
    "                    Text {format_texts}, table summaries: {table_summaries}, image summaries: {image_summaries} \"\"\"},\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    if dict['context']['images']:\n",
    "        for i in range(len(dict['context']['images'])):\n",
    "            message.append(\n",
    "                HumanMessage(\n",
    "                    content=[\n",
    "                        {\"type\": \"image_url\", \n",
    "                         \"image_url\": {\"url\": f\"data:image/jpeg;base64,{dict['context']['images'][i]}\"}}\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        print(\"No images found in the context.\")\n",
    "    return message\n",
    "\n",
    "conversation_id = str(uuid.uuid4())\n",
    "chain = ( {\"context\": retriever | RunnableLambda(splitDocs), \"question\": RunnablePassthrough()} | RunnableLambda(prompt_func) | llm_client | StrOutputParser() )\n",
    "answer = chain.invoke({\"question\": \"As a cloud solution architect Summarize the Azure AI Foundry chat reference architecture\"}, config={\"conversation_id\": conversation_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2447288f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Azure AI Foundry chat reference architecture is designed to provide secure, scalable, and efficient integration of Azure services for enterprise-grade AI solutions. The architecture emphasizes security, compliance, and control while leveraging Azure AI services for advanced functionality. Below is a summary of the architecture, supported by the provided diagram:\n",
       "\n",
       "### Key Components:\n",
       "1. **User Access**:\n",
       "   - Users interact with the system through an **Application Gateway** integrated with **Azure Web Application Firewall (WAF)** for enhanced security.\n",
       "   - DNS zones and DDoS protection ensure secure and reliable access.\n",
       "\n",
       "2. **Private Endpoints**:\n",
       "   - Critical services such as **App Service**, **Key Vault**, **Azure Storage**, **Azure AI Foundry**, and others are securely accessed via private endpoints within a virtual network.\n",
       "   - This ensures data traffic remains isolated within the workload's virtual network.\n",
       "\n",
       "3. **Azure AI Foundry**:\n",
       "   - Managed identities facilitate secure connections to the **Azure AI Foundry account** and its associated projects, including **OpenAI models**.\n",
       "\n",
       "4. **Build Agents**:\n",
       "   - A dedicated subnet is allocated for build agents, with optional **jump-box access** for secure management.\n",
       "   - This supports development and deployment processes.\n",
       "\n",
       "5. **Azure Firewall**:\n",
       "   - Outbound traffic from the agents is routed through the **Azure Firewall**, enforcing egress rules and ensuring secure integration with internet sources.\n",
       "\n",
       "6. **Monitoring**:\n",
       "   - **Application Insights** and **Azure Monitor** provide observability and diagnostics for the deployed applications.\n",
       "\n",
       "7. **Dependencies**:\n",
       "   - Foundry Agent Service dependencies include **Azure Cosmos DB**, **Azure Storage**, and **Azure Cognitive Search** for robust data management and search capabilities.\n",
       "\n",
       "### Workflow:\n",
       "1. **User Interaction**:\n",
       "   - Application users interact with a chat UI, with requests routed through the **Application Gateway** and inspected by **Azure WAF**.\n",
       "   - The requests are forwarded to the back-end **App Service**.\n",
       "\n",
       "2. **Secure Communication**:\n",
       "   - All communication between application components and Azure services occurs over private endpoints, ensuring data traffic remains within the virtual network.\n",
       "\n",
       "3. **Integration with AI Services**:\n",
       "   - The architecture integrates with **Azure AI Foundry** and **OpenAI models** for advanced AI functionalities.\n",
       "\n",
       "### Diagram Highlights:\n",
       "The diagram visually represents the architecture, showcasing:\n",
       "- **Subnets** for various components (e.g., Application Gateway, App Service, Build Agents, Azure Firewall).\n",
       "- **Private Endpoints** for secure access to Azure services.\n",
       "- **Azure AI Foundry** integration with managed identities.\n",
       "- **Monitoring** and **dependencies** for observability and data management.\n",
       "\n",
       "This architecture serves as a baseline for developing custom AI solutions while ensuring enterprise-grade security and scalability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](extracted_images/12.1.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "display(Markdown(answer))\n",
    "\n",
    "######## Display images \n",
    "for image_url in conversation_memory[conversation_id]['image_urls']:\n",
    "    display(Markdown(f\"![Image]({image_url})\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
