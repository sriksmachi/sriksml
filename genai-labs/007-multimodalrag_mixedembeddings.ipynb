{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e6398c",
   "metadata": {},
   "source": [
    "# ðŸ¤– Multimodal RAG with Embeddings of Text, Image, and Table Summaries\n",
    "#### A cookbook for deriving more value from multi-modal data in Enterprise RAG Applications\n",
    "\n",
    "### ðŸ—‚ï¸ Introduction\n",
    "\n",
    "Why Most Enterprise RAG Applications Fall Short: The Overlooked Importance of Context Engineering ?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) applications are rapidly gaining traction across enterprises, offering powerful capabilities for knowledge retrieval and generative insights. However, the true effectiveness of these solutions hinges on a critical but often underestimated componentâ€”**data extraction and context engineering**. This notebook deals with 3 most-common problems seen in Enterprise RAG Applications.\n",
    "\n",
    "1. **Multi-Modal data extraction**: Many RAG implementations fail to deliver tangible value because they rely solely on extracting plain text from source documents. This narrow approach overlooks the rich, structured, and visual data often embedded in enterprise contentâ€”such as **images, charts, and tables**. For instance, financial reports and research papers frequently convey essential information through non-textual formats that are crucial for accurate understanding and meaningful generation. Neglecting this rich content leads to incomplete context, ultimately resulting in **subpar performance and poor-quality outputs**. To build truly effective RAG solutions, enterprises must adopt a more comprehensive content extraction strategyâ€”one that intelligently incorporates all relevant modalities, not just text.\n",
    "2. **Text only inputs to LLM in Generation Phase** : Another common pitfall in enterprise RAG applications is the exclusive reliance on text when interacting with large language models (LLMs). When only textual data is passed to the model, valuable information embedded in images, tables, and other visual elementsâ€”which are often used to clarify, quantify, or summarize insightsâ€”is lost. This significantly weakens the modelâ€™s ability to generate accurate, complete responses.\n",
    "3. **Cohesive Search Documents**: Equally important is the cohesion of extracted content. In most documents, information is presented in a tightly interlinked mannerâ€”text, visuals, and structured data are intentionally designed to work together to convey meaning. Extracting and using only fragments of that information disrupts this contextual integrity. For example, referencing a table without its corresponding explanatory textâ€”or vice versaâ€”can lead to misleading or incomplete answers. To achieve high-quality, reliable outputs, RAG applications must preserve this semantic cohesion during retrieval and context construction. This involves not only multimodal extraction but also thoughtful chunking, ranking, and packaging of information before itâ€™s passed to the LLM.In essence, effective RAG isnâ€™t just about finding relevant documentsâ€”itâ€™s about preserving the full fidelity and structure of information within them. Overlooking this principle is a key reason why many enterprise RAG deployments underperform.\n",
    "\n",
    "\n",
    "This notebook demonstrates how to leverage Azure AI services to build a robust multimodal RAG pipeline, enabling efficient data extraction, summarization, and **multi-vector** fields for downstream tasks.\n",
    "The following section defines key learning objectives in each section. \n",
    "\n",
    "### ðŸŽ¯ Key learning Objectives\n",
    "\n",
    "1. ðŸ“„ **Data Extraction**: In this notebook we will learn to extract all types of data from rich PDFs using Azure Document Intelligence. \n",
    "2. ðŸ“ **Summarization**: Implement text, table, and image summarization using Azure OpenAI models. At this stage we will also organize the data into a data-structure that supports **multi-vector** indexes.\n",
    "3. ðŸ§  **Embedding & Search**: Create vector embeddings for multimodal data (text, images) and store them in Azure AI Search for efficient retrieval. Reshape search results to optimize context for text generation using both text and image data. Finally, demonstrate how relevant retrieved images enhance the generated response.\n",
    "\n",
    "### ðŸ Conclusion\n",
    "\n",
    "By the end of this notebook, users will have a clear understanding of how to build a multimodal RAG pipeline that integrates text, image, and table data for advanced AI applications. This approach enhances the ability to generate meaningful insights from diverse data sources, paving the way for innovative solutions in enterprise and research domains. In short, robust context engineering is not optionalâ€”itâ€™s foundational. Without it, even the most advanced RAG systems are at risk of missing the mark.ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2893d6f",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "This cell outlines the installation of required Python packages and the configuration of environment variables necessary for running the notebook Multimodal RAG with embeddings of Text, Image, and Table Summaries.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Installation**\n",
    "The following Python packages are installed to enable multimodal retrieval-augmented generation (RAG) capabilities:\n",
    "```python\n",
    "%pip install --quiet azure-ai-documentintelligence \"python-dotenv\" \"langchain-openai\" \"azure-search-documents\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Environment Variables**\n",
    "The `.env` file must include the following configurations for secure credential management:\n",
    "\n",
    "| **Variable Name**                  | **Description**                                                                 |\n",
    "|------------------------------------|---------------------------------------------------------------------------------|\n",
    "| `AZURE_OPENAI_ENDPOINT`            | Azure OpenAI endpoint URL                                                      |\n",
    "| `AZURE_OPENAI_API_KEY`             | API key for Azure OpenAI                                                       |\n",
    "| `AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT` | Endpoint for Azure Document Intelligence                                       |\n",
    "| `AZURE_DOCUMENT_INTELLIGENCE_KEY`  | API key for Azure Document Intelligence                                        |\n",
    "| `AZURE_VISION_MODEL_ENDPOINT`      | Endpoint for Azure Vision Model                                                |\n",
    "| `AZURE_VISION_MODEL_KEY`           | API key for Azure Vision Model                                                 |\n",
    "| `AZURE_AI_SEARCH_SERVICE_NAME`     | Name of the Azure AI Search service                                            |\n",
    "| `AZURE_AI_SEARCH_API_KEY`          | API key for Azure AI Search service                                            |\n",
    "| `AOAI_EMBEDDINGS_DEPLOYMENT_NAME`  | Deployment name for Azure OpenAI embeddings                                    |\n",
    "| `AZURE_AI_SEARCH_INDEX_NAME`       | Name of the Azure AI Search index                                              |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Notes**\n",
    "- Ensure the `.env` file is properly configured before running the notebook.\n",
    "- The packages installed enable document analysis, image processing, vector embeddings, and search capabilities using Azure AI services.\n",
    "\n",
    "This setup is critical for enabling multimodal RAG workflows that integrate text, image, and table embeddings for advanced AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeOutputOption\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "import uuid\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Load environment variables from .env file for secure credential management\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Azure and OpenAI configuration from environment variables\n",
    "di_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "di_credential = AzureKeyCredential(os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\"))\n",
    "chat_model = os.getenv(\"AOAI_CHAT_DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "aoai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "vision_model_endpoint = os.getenv(\"AZURE_VISION_MODEL_ENDPOINT\")\n",
    "vision_model_key = os.getenv(\"AZURE_VISION_MODEL_KEY\")\n",
    "search_endpoint = os.getenv(\"AZURE_AI_SEARCH_SERVICE_NAME\")\n",
    "search_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\")\n",
    "embeddings_model = os.getenv(\"AOAI_EMBEDDINGS_DEPLOYMENT_NAME\", \"text-embedding-3-small\")\n",
    "search_index_name = os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\", \"langchain-vector-demo\")\n",
    "\n",
    "# Initialize Azure Document Intelligence client for document analysis\n",
    "document_intelligence_client = DocumentIntelligenceClient(di_endpoint, di_credential)\n",
    "\n",
    "# Initialize Azure Vision client for image analysis\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=vision_model_endpoint,\n",
    "    credential=AzureKeyCredential(vision_model_key)\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI chat client for LLM-based summarization\n",
    "llm_client = AzureChatOpenAI(\n",
    "    azure_deployment=chat_model,\n",
    "    api_version=\"2023-05-15\",\n",
    "    temperature=0.3,\n",
    "    model_name=chat_model,\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI embeddings client for vectorization\n",
    "embeddings_model_client = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    "    azure_deployment=embeddings_model,\n",
    ")\n",
    "\n",
    "# Path to the sample document (PDF) to be analyzed\n",
    "# For simplicity, this example uses a document sampled from Azure Architecture reference\n",
    "# File can be downloaded from here https://drive.google.com/file/d/1oWxuqCAnu5GEOgNA0pHQCeLgxPf-wduU/view?usp=sharing\n",
    "path_to_sample_documents = \"azure_ref_architecture.pdf\"\n",
    "\n",
    "# Analyze the document using the prebuilt-layout model, requesting figure extraction\n",
    "with open(path_to_sample_documents, \"rb\") as f:\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\",\n",
    "        body=f,\n",
    "        output=[AnalyzeOutputOption.FIGURES], # Requesting figure extraction\n",
    "    )\n",
    "result = poller.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b934fe",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "\n",
    "In this section we will extract the data and organize as Page -> Page Count [tables, images], this helps in efficient search retrieval. Imagine of search query matches the table, image summary the entire page can be located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text and Headings\n",
    "pages = []\n",
    "\n",
    "# Structure the documents as Pages -> Paragraphs -> Text\n",
    "for paragraph in result.paragraphs:\n",
    "    page = {}\n",
    "    page_number = paragraph[\"boundingRegions\"][0][\"pageNumber\"] if paragraph[\"boundingRegions\"] else None\n",
    "    role = paragraph[\"role\"] if \"role\" in paragraph else \"paragraph\"\n",
    "    content = paragraph[\"content\"] if \"content\" in paragraph else \"\"\n",
    "    \n",
    "    # check if page already exists\n",
    "    if page_number and any(p[\"page_number\"] == page_number for p in pages):\n",
    "        page = next(p for p in pages if p[\"page_number\"] == page_number)\n",
    "        page[\"content\"] = page.get(\"content\", \"\") + \" \" + content\n",
    "        if role in [\"heading\", \"sectionHeading\", \"title\"]:\n",
    "            page[\"sections\"] = page.get(\"sections\", \"\") + \",\" + content\n",
    "        continue\n",
    "    else:\n",
    "        page = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"content\": content,\n",
    "            \"sections\": content if role in [\"heading\", \"sectionHeading\", \"title\"] else \"\",\n",
    "            \"page_number\": page_number,\n",
    "            \"page_content\": [] # Initialize to fill tables, figures later\n",
    "        }\n",
    "    pages.append(page)\n",
    "print(f\"Extracted {len(pages)} documents with headings and text.\")\n",
    "\n",
    "# Extract Tables and add to pages\n",
    "table_elements = []\n",
    "if result.tables:\n",
    "    for idx, table in enumerate(result.tables):\n",
    "        page_number = table[\"boundingRegions\"][0][\"pageNumber\"] if table[\"boundingRegions\"] else None\n",
    "        page = next((p for p in pages if p[\"page_number\"] == page_number), None)\n",
    "        if not page:\n",
    "            print(f\"Page {page_number} not found for table {idx}. Skipping.\")\n",
    "            continue\n",
    "        print(f\"Table {idx} found on page {page_number}.\")\n",
    "        t_documents = []\n",
    "        for cell in table.cells:\n",
    "            t_document = {}\n",
    "            t_document[\"row\"] = cell.row_index\n",
    "            t_document[\"column\"] = cell.column_index\n",
    "            t_document[\"row_content\"] = cell.content\n",
    "            t_documents.append(t_document)\n",
    "        page[\"page_content\"].append({\n",
    "            \"type\": \"table\",\n",
    "            \"content\": str(t_documents),\n",
    "            \"page_number\": page_number,\n",
    "            \"id\": str(uuid.uuid4())\n",
    "        })\n",
    "    print(f\"Extracted {len(result.tables)} tables from the document.\")    \n",
    "\t\n",
    "# # Extract Figures and add to pages later\n",
    "if result.figures:\n",
    "    operation_id = poller.details[\"operation_id\"]\n",
    "    os.makedirs(\"extracted_images\", exist_ok=True)\n",
    "    for figure in result.figures:\n",
    "        page_number = figure[\"boundingRegions\"][0][\"pageNumber\"] if figure[\"boundingRegions\"] else None\n",
    "        page = next((p for p in pages if p[\"page_number\"] == page_number), None)\n",
    "        if not page:\n",
    "            print(f\"Page {page_number} not found for figure {figure.id}. Skipping.\")\n",
    "            continue\n",
    "        if figure.id:\n",
    "            response = document_intelligence_client.get_analyze_result_figure(\n",
    "                model_id=result.model_id, result_id=operation_id, figure_id=figure.id\n",
    "            )\n",
    "            print(f\"Figure {figure.id} extracted from page {page_number}.\")\n",
    "            page['page_content'].append({\n",
    "                \"type\": \"figure\",\n",
    "                \"content\": \"\", # this will be filled with the image summary later\n",
    "                \"image_url\": f\"extracted_images/{figure.id}.png\", # Save the public URL for the image in real world scenarios\n",
    "                \"page_number\": page_number,\n",
    "                \"id\": figure.id\n",
    "            })\n",
    "            with open(f\"extracted_images/{figure.id}.png\", \"wb\") as writer:\n",
    "                writer.writelines(response)\n",
    "else:\n",
    "    print(\"No figures found.\")\n",
    "print(f\"Extracted {len(result.figures)} figures from the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9059b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample Content with Tables\")\n",
    "print(pages[5])\n",
    "print('--'*50)\n",
    "print(\"Sample Content with Figures\")\n",
    "print(pages[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9802f0d",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "In this section we generate summaries for tables, images. I'm using GPT4o for image summarization you may use any VLM. Generating summaries for pages is optionally here, however it may help increase search score when hybrid search is used. \n",
    "\n",
    "**Note**: I initially planned to use Azure Computer Vision, however I noticed that the it only provides precise captions of the image but does not generate summaries hence I switched to VLM (infact a multi-modal LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "import tqdm\n",
    "\n",
    "# Text Summarization \n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | llm_client | StrOutputParser()\n",
    "\n",
    "# Image Summarization\n",
    "def encode_image(image_path):\n",
    "    '''Getting the base64 string'''\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def image_summarize(image_path):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    prompt_text = \"You are an assistant tasked with summarizing images. Extract text, image from the input and Give a concise summary.\"\n",
    "    img_base64 = encode_image(image_path)\n",
    "    msg = llm_client.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt_text},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_base64}\" # Use base64 encoded image\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "# Summarize content of pages\n",
    "# Note: The content vectorization is commented out to avoid increase in size of the file\n",
    "texts = [i[\"content\"] for i in pages if i[\"content\"] != \"\"]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "for i, page in tqdm.tqdm(enumerate(pages)):\n",
    "    page[\"summary\"] = text_summaries[i] if i < len(text_summaries) else \"No summary available\" \n",
    "    # page[\"content_vector\"] = embeddings_model_client.embed_documents(page[\"content\"])\n",
    "    for content in page[\"page_content\"]:\n",
    "        if content[\"type\"] == \"table\":\n",
    "            content[\"summary\"] = summarize_chain.invoke(content[\"content\"])\n",
    "            # content[\"content_vector\"] = embeddings_model_client.embed_documents(content[\"summary\"] + str(content[\"text\"])) # we remove the content vector for tables if it is too large\n",
    "        elif content[\"type\"] == \"figure\":\n",
    "            content[\"summary\"] = image_summarize(content['image_url'])\n",
    "            # content[\"content_vector\"] = embeddings_model_client.embed_documents(content[\"summary\"])\n",
    "        else:\n",
    "            content[\"summary\"] = \"No summary available\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f760a9b",
   "metadata": {},
   "source": [
    "### Multi-Vector Search\n",
    "\n",
    "**Azure Multi-Vector Field Search [Public Preview]**\n",
    "\n",
    "This notebook uses a public preview feature in Azure AI Search called [Multi-vector field](https://learn.microsoft.com/en-us/azure/search/vector-search-multi-vector-fields). It allows indexing multiple child vectors within a single document field, making it particularly effective for multimodal data or long-form documents, where a single vector representation may lose important context or detail. We can embed multiple chunks of a document and associate those embeddings with the parent document, allowing retriever hits on the chunks to return the larger document. This capability ensures finer-grained retrieval and relevance when working with diverse data types like text and images.\n",
    "\n",
    "Since this feature is in preview the only support method to create the Index is via Portal or REST API. Before you proceed make sure the index is created in Azure Search either from the portal or via REST API. \n",
    "The Index `JSON` file can be found under `/configs/multi-vector-index.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a07f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=search_index_name,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "# Uploading the documents to Azure AI Search\n",
    "# Note: Assuming the index with same name above is created in Azure Search either from the portal or via REST API.\n",
    "# The Index `JSON` file can be found under `/configs/multi-vector-index.json`\n",
    "search_client.upload_documents(\n",
    "    documents=pages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a search to find the page with the query\n",
    "result = search_client.search(\"show integration of Azure Web Application Firewall (WAF)\", top=1)\n",
    "for r in result:\n",
    "    print(f\"Page Number: {r['page_number']}, Page Summary: {r['summary']}\")\n",
    "    for content in r[\"page_content\"]:\n",
    "        print('-' * 50)\n",
    "        display(Markdown(f\"Page Content Summary: {content['summary']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c3e47c",
   "metadata": {},
   "source": [
    " As you can see the search result returns the page number and summary of the page although the query matches the image summary. This allows for efficient retrieval of relevant information based on user queries. # This is particularly useful in scenarios where users need to quickly locate specific information within large documents, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3856686",
   "metadata": {},
   "source": [
    "### RAG with multi-modal data\n",
    "\n",
    "In this section, we will build a RAG (Retrieval-Augmented Generation) pipeline. During the retrieval phase, search results will be restructured to include page content, table data, image summaries, and the images themselves. Images will be converted to Base64-encoded values and passed to a multimodal language model to enhance the quality of the generated response. The most relevant image identified for the query will also be included in the final response to deliver a richer user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca576254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import AzureAISearchRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Initialize user memory to store conversation context\n",
    "conversation_memory = {}\n",
    "\n",
    "retriever = AzureAISearchRetriever(\n",
    "    top_k=3, \n",
    "    service_name=search_endpoint,\n",
    "    api_key=search_key,\n",
    "    content_key=\"content\",  # This is the field name for the vector embeddings\n",
    "    index_name=search_index_name,\n",
    "    azure_ad_token=\"True\" # Bug in langchain-azure-ai-search, need to set this to True to use API Key\n",
    ")\n",
    "\n",
    "def splitDocs(docs, config):\n",
    "    imgs = []\n",
    "    table_summaries = []\n",
    "    image_summaries = []\n",
    "    image_urls = []\n",
    "    conversation_id = config['metadata']['conversation_id']\n",
    "    for doc in docs:\n",
    "        content = doc.page_content + \"\\n\" + \"\\n\".join([p[\"summary\"] for p in doc.metadata[\"page_content\"]])\n",
    "        for page_content in doc.metadata[\"page_content\"]:\n",
    "            if page_content[\"type\"] == \"table\":\n",
    "                table_summaries.append(page_content[\"summary\"])\n",
    "            elif page_content[\"type\"] == \"figure\":\n",
    "                image_summaries.append(page_content[\"summary\"])\n",
    "                image_urls.append(page_content[\"image_url\"])\n",
    "                base64_image = encode_image(page_content[\"image_url\"])\n",
    "                imgs.append(base64_image)\n",
    "    conversation_memory[conversation_id] = {'image_urls': image_urls}\n",
    "    return { \n",
    "        'content': content, \n",
    "        'images': imgs, \n",
    "        'table_summaries': table_summaries, \n",
    "        'image_summary': image_summaries, \n",
    "        \"image_urls\": image_urls \n",
    "    }\n",
    "\n",
    "def prompt_func(dict):\n",
    "    format_texts = \"\\n\".join(dict['context'][\"content\"])\n",
    "    table_summaries = \"\\n\".join(dict['context'][\"table_summaries\"])\n",
    "    image_summaries = \"\\n\".join(dict['context'][\"image_summary\"])\n",
    "    message = [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant! Your name is Bob. \\\n",
    "                You are given context that contains document content along with tables and images. A summary of the content along with tables and images is provided. \\\n",
    "                You are an expert in answering questions using text, tables, and images.\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \n",
    "                 \"text\": f\"\"\"\n",
    "                 Answer the question based only on the following context, which can include text, tables, and the below image: Question: {dict[\"question\"]} \\\n",
    "                    Text {format_texts}, table summaries: {table_summaries}, image summaries: {image_summaries} \"\"\"},\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    if dict['context']['images']:\n",
    "        for i in range(len(dict['context']['images'])):\n",
    "            message.append(\n",
    "                HumanMessage(\n",
    "                    content=[\n",
    "                        {\"type\": \"image_url\", \n",
    "                         \"image_url\": {\"url\": f\"data:image/jpeg;base64,{dict['context']['images'][i]}\"}}\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        print(\"No images found in the context.\")\n",
    "    return message\n",
    "\n",
    "conversation_id = str(uuid.uuid4())\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(splitDocs), \"question\": RunnablePassthrough()} \n",
    "    | RunnableLambda(prompt_func) \n",
    "    | llm_client \n",
    "    | StrOutputParser()\n",
    ")\n",
    "answer = chain.invoke(\n",
    "    {\"question\": \"As an AI Architect summarize the Azure AI Foundry chat reference architecture\"}, \n",
    "    config={\"conversation_id\": conversation_id}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(answer))\n",
    "######## Display images \n",
    "for image_url in conversation_memory[conversation_id]['image_urls']:\n",
    "    display(Markdown(f\"![Image]({image_url})\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
