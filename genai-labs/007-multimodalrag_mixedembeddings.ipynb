{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2893d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet azure-ai-documentintelligence \"python-dotenv\" \"pandas\" \"langchain-openai\" \"azure-ai-vision-imageanalysis\" \"azure-search-documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeOutputOption\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "import pandas as pd\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "import uuid\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "di_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "di_credential = AzureKeyCredential(os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\"))\n",
    "chat_model = os.getenv(\"AOAI_CHAT_DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "aoai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "vision_model_endpoint = os.getenv(\"AZURE_VISION_MODEL_ENDPOINT\")\n",
    "vision_model_key = os.getenv(\"AZURE_VISION_MODEL_KEY\")\n",
    "search_endpoint = os.getenv(\"AZURE_AI_SEARCH_SERVICE_NAME\")\n",
    "search_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\")\n",
    "embeddings_model = os.getenv(\"AOAI_EMBEDDINGS_DEPLOYMENT_NAME\", \"text-embedding-3-small\")\n",
    "search_index_name = os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\", \"langchain-vector-demo\")\n",
    "\n",
    "# Create a Document Intelligence client for synchronous operations,\n",
    "document_intelligence_client = DocumentIntelligenceClient(di_endpoint, di_credential)\n",
    "\n",
    "# Create an Image Analysis client for synchronous operations,\n",
    "# using API key authentication\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=vision_model_endpoint,\n",
    "    credential=AzureKeyCredential(vision_model_key)\n",
    ")\n",
    "\n",
    "# Summary chain\n",
    "llm_client = AzureChatOpenAI(\n",
    "    azure_deployment=chat_model,\n",
    "    api_version=\"2023-05-15\",\n",
    "    temperature=0.3,\n",
    "    model_name=chat_model,\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    ")\n",
    "\n",
    "embeddings_model_client = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    "    azure_deployment=embeddings_model\n",
    ")\n",
    "\n",
    "\n",
    "# Download link: https://arxiv.org/pdf/2304.08485\n",
    "path_to_sample_documents = \"llalava.pdf\"\n",
    "with open(path_to_sample_documents, \"rb\") as f:\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\",\n",
    "        body=f,\n",
    "        output=[AnalyzeOutputOption.FIGURES],\n",
    "    )\n",
    "result = poller.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text and Headings\n",
    "documents = []\n",
    "heading = \"\"\n",
    "for paragraph in result.paragraphs:\n",
    "    document = { \"type\": \"text\" }\n",
    "    document[\"role\"] = paragraph[\"role\"] if \"role\" in paragraph else \"paragraph\"\n",
    "    if document[\"role\"] == \"sectionHeading\":\n",
    "        # save the section heading\n",
    "        heading = paragraph[\"content\"]\n",
    "    # check if heading is already present in documents\n",
    "    is_existing_heading = [doc[\"heading\"] == heading for doc in documents]\n",
    "    if not any(is_existing_heading):\n",
    "        # if not, create a new document entry\n",
    "        document[\"text\"] = paragraph[\"content\"]\n",
    "        document[\"heading\"] = heading if heading else \"No Heading\"\n",
    "        documents.append(document)\n",
    "    else:\n",
    "        # if it exists, append the text to the existing document\n",
    "        for doc in documents:\n",
    "            if doc[\"heading\"] == heading:\n",
    "                doc[\"text\"] += \" \" + paragraph[\"content\"]\n",
    "                break \n",
    "print(f\"Extracted {len(documents)} documents with headings and text.\")\n",
    "\n",
    "\n",
    "# Extract Tables\n",
    "table_elements = []\n",
    "for idx, table in enumerate(result.tables):\n",
    "    t_documents = []\n",
    "    for cell in table.cells:\n",
    "        t_document = {}\n",
    "        t_document[\"row\"] = cell.row_index\n",
    "        t_document[\"column\"] = cell.column_index\n",
    "        t_document[\"row_content\"] = cell.content\n",
    "        t_documents.append(t_document)\n",
    "    table_elements.append({ \"text\": t_documents, \"type\": \"table\" })\n",
    "\t\n",
    "\t\n",
    "# Extract Figures\n",
    "operation_id = poller.details[\"operation_id\"]\n",
    "if result.figures:\n",
    "    os.makedirs(\"extracted_images\", exist_ok=True)\n",
    "    for figure in result.figures:\n",
    "        if figure.id:\n",
    "            response = document_intelligence_client.get_analyze_result_figure(\n",
    "                model_id=result.model_id, result_id=operation_id, figure_id=figure.id\n",
    "            )\n",
    "            with open(f\"extracted_images/{figure.id}.png\", \"wb\") as writer:\n",
    "                writer.writelines(response)\n",
    "else:\n",
    "    print(\"No figures found.\")\n",
    "print(f\"Extracted {len(result.figures)} figures from the document.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding and Vector Store Creation\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | llm_client | StrOutputParser()\n",
    "\n",
    "# Apply to text\n",
    "texts = [i[\"text\"] for i in documents if i[\"text\"] != \"\"]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "for i, doc in enumerate(documents):\n",
    "    doc[\"summary\"] = text_summaries[i]   \n",
    "\t\n",
    "\t\n",
    "\t# Apply to tables\n",
    "table_summaries = summarize_chain.batch(table_elements, {\"max_concurrency\": 5})\n",
    "for i, table in enumerate(table_elements):\n",
    "    table_elements[i][\"summary\"] = table_summaries[i]\n",
    "\t\n",
    "\t\n",
    "\t# print sample text document\n",
    "print(f\"Sample text doc: {documents[0]}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# print sample table document\n",
    "print(f\"Sample table doc: {table_elements[0]}\")\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "\n",
    "image_summaries = []\n",
    "def encode_image(image_path):\n",
    "    '''Getting the base64 string'''\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def image_summarize(prompt_text, image_path):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    img_base64 = encode_image(image_path)\n",
    "    msg = llm_client.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt_text},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "prompt_text = \"You are an assistant tasked with summarizing images. Extract text, image from the input and Give a concise summary.\"\n",
    "image_summaries = [image_summarize(prompt_text, f\"extracted_images/{figure.id}.png\") for figure in result.figures if figure.id]\n",
    "\n",
    "\n",
    "image_data = []\n",
    "for i, figure in enumerate(result.figures):\n",
    "    image_summary = {}\n",
    "    image_summary[\"id\"] = str(uuid.uuid4())  # Generate a unique ID for the image summary\n",
    "    image_summary[\"summary\"] = image_summaries[i] if i < len(image_summaries) else \"No summary available\"\n",
    "    image_summary['type'] = \"image\"\n",
    "    image_summary['image_url'] = f\"extracted_images/{figure.id}.png\" # this should be the path to the image file, SAS URL or base64 encoded string in real applications\n",
    "    image_data.append(image_summary)\n",
    "# print sample image summary\n",
    "print(f\"Sample image summary: {image_data[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a07f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure Search Index with Mixed Embeddings\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration,\n",
    ")\n",
    "\n",
    "# Create Search Index\n",
    "search_index_client = SearchIndexClient(\n",
    "    endpoint=search_endpoint,\n",
    "    credential=AzureKeyCredential(search_key),\n",
    ")\n",
    "search_index = SearchIndex(\n",
    "    name=search_index_name,\n",
    "    fields=[\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchField(name=\"content\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchField(name=\"summary\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchField(name=\"heading\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SimpleField(name=\"type\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"image_url\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"content_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1536,  vector_search_profile_name=\"my-vector-config\"),\n",
    "    ],\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[VectorSearchProfile(name=\"my-vector-config\", algorithm_configuration_name=\"my-algorithms-config\")],\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithms-config\")],\n",
    "    )\n",
    ") \n",
    "\n",
    "try:\n",
    "    search_index_client.delete_index(search_index_name)\n",
    "except Exception as e:\n",
    "    print(f\"Index deletion failed: {e}\")\n",
    " \n",
    "search_index_client.create_index(search_index)\n",
    "\n",
    "\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    credential=AzureKeyCredential(search_key),\n",
    "    index_name=search_index_name)\\\n",
    "\n",
    "search_client.upload_documents(\n",
    "    documents=[\n",
    "        {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"content\": doc[\"text\"],\n",
    "            \"summary\": doc[\"summary\"],\n",
    "            \"heading\": doc[\"heading\"],\n",
    "            \"type\": doc[\"type\"],\n",
    "            \"content_vector\": embeddings_model_client.embed_query(doc[\"text\"] if \"text\" in doc else \"\"),\n",
    "        } for doc in documents\n",
    "    ] + [\n",
    "        {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"content\": str(table[\"text\"]),\n",
    "            \"summary\": table[\"summary\"],\n",
    "            \"type\": table[\"type\"],\n",
    "            \"content_vector\": embeddings_model_client.embed_query(str(table[\"text\"])),\n",
    "        } for table in table_elements\n",
    "    ] + [\n",
    "        {\n",
    "            \"id\": image[\"id\"],\n",
    "            \"content\": image[\"summary\"],\n",
    "            \"type\": image[\"type\"],\n",
    "            \"image_url\": image[\"image_url\"],\n",
    "            \"content_vector\": embeddings_model_client.embed_query(image[\"summary\"]),\n",
    "        } for image in image_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "query = (\"What is the performance of LLaVa across across multiple image domains / subjects?\",)\n",
    "embeddings = embeddings_model_client.embed_query(query[0])  # query is a tuple, we need the first element\n",
    "vector_query = VectorizedQuery(vector=embeddings, k_nearest_neighbors=3, fields=\"content_vector\")\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    top=1,\n",
    ")\n",
    "for result in results:\n",
    "    print(f\"Document ID: {result['id'], result['type'], result['summary']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca576254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# The vectorstore to use to index the child chunks\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import uuid\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "    \n",
    "store = InMemoryStore()\n",
    "\n",
    "\n",
    "from langchain_community.retrievers import AzureAISearchRetriever\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=search_endpoint,\n",
    "    azure_search_key=search_key,\n",
    "    index_name=search_index_name,\n",
    "    search_client=search_client,\n",
    "    embedding_function=AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=aoai_endpoint,\n",
    "        api_key=aoai_key,\n",
    "        azure_deployment=embeddings_model)\n",
    ")\n",
    "\n",
    "retriever = AzureAISearchRetriever(\n",
    "    top_k=1, \n",
    "    index_name=search_index_name,\n",
    ")\n",
    "\n",
    "res = retriever.invoke(\"LLAVA\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
