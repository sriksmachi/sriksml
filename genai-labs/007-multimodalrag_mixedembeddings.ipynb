{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e6398c",
   "metadata": {},
   "source": [
    "# ðŸ¤– Multimodal RAG with Embeddings of Text, Image, and Table Summaries\n",
    "#### A cookbook for deriving more value from multi-modal data in Enterprise RAG Applications\n",
    "\n",
    "### ðŸ—‚ï¸ Introduction\n",
    "\n",
    "Why Most Enterprise RAG Applications Fall Short: The Overlooked Importance of Context Engineering ?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) applications are rapidly gaining traction across enterprises, offering powerful capabilities for knowledge retrieval and generative insights. However, the true effectiveness of these solutions hinges on a critical but often underestimated componentâ€”**data extraction and context engineering**. This notebook deals with 3 most-common problems seen in Enterprise RAG Applications.\n",
    "\n",
    "1. **Multi-Modal data extraction**: Many RAG implementations fail to deliver tangible value because they rely solely on extracting plain text from source documents. This narrow approach overlooks the rich, structured, and visual data often embedded in enterprise contentâ€”such as **images, charts, and tables**. For instance, financial reports and research papers frequently convey essential information through non-textual formats that are crucial for accurate understanding and meaningful generation. Neglecting this rich content leads to incomplete context, ultimately resulting in **subpar performance and poor-quality outputs**. To build truly effective RAG solutions, enterprises must adopt a more comprehensive content extraction strategyâ€”one that intelligently incorporates all relevant modalities, not just text.\n",
    "2. **Text only inputs to LLM in Generation Phase** : Another common pitfall in enterprise RAG applications is the exclusive reliance on text when interacting with large language models (LLMs). When only textual data is passed to the model, valuable information embedded in images, tables, and other visual elementsâ€”which are often used to clarify, quantify, or summarize insightsâ€”is lost. This significantly weakens the modelâ€™s ability to generate accurate, complete responses.\n",
    "3. **Cohesive Search Documents**: Equally important is the cohesion of extracted content. In most documents, information is presented in a tightly interlinked mannerâ€”text, visuals, and structured data are intentionally designed to work together to convey meaning. Extracting and using only fragments of that information disrupts this contextual integrity. For example, referencing a table without its corresponding explanatory textâ€”or vice versaâ€”can lead to misleading or incomplete answers. To achieve high-quality, reliable outputs, RAG applications must preserve this semantic cohesion during retrieval and context construction. This involves not only multimodal extraction but also thoughtful chunking, ranking, and packaging of information before itâ€™s passed to the LLM.In essence, effective RAG isnâ€™t just about finding relevant documentsâ€”itâ€™s about preserving the full fidelity and structure of information within them. Overlooking this principle is a key reason why many enterprise RAG deployments underperform.\n",
    "\n",
    "\n",
    "This notebook demonstrates how to leverage Azure AI services to build a robust multimodal RAG pipeline, enabling efficient data extraction, summarization, and **multi-vector** fields for downstream tasks.\n",
    "The following section defines key learning objectives in each section. \n",
    "\n",
    "### ðŸŽ¯ Key learning Objectives\n",
    "\n",
    "1. ðŸ“„ **Data Extraction**: In this notebook we will learn to extract all types of data from rich PDFs using Azure Document Intelligence. \n",
    "2. ðŸ“ **Summarization**: Implement text, table, and image summarization using Azure OpenAI models. At this stage we will also organize the data into a data-structure that supports **multi-vector** indexes.\n",
    "3. ðŸ§  **Embedding & Search**: Create vector embeddings for multimodal data (text, images) and store them in Azure AI Search for efficient retrieval. Reshape search results to optimize context for text generation using both text and image data. Finally, demonstrate how relevant retrieved images enhance the generated response.\n",
    "\n",
    "### ðŸ Conclusion\n",
    "\n",
    "By the end of this notebook, users will have a clear understanding of how to build a multimodal RAG pipeline that integrates text, image, and table data for advanced AI applications. This approach enhances the ability to generate meaningful insights from diverse data sources, paving the way for innovative solutions in enterprise and research domains. In short, robust context engineering is not optionalâ€”itâ€™s foundational. Without it, even the most advanced RAG systems are at risk of missing the mark.ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2893d6f",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "This cell outlines the installation of required Python packages and the configuration of environment variables necessary for running the notebook Multimodal RAG with embeddings of Text, Image, and Table Summaries.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Installation**\n",
    "The following Python packages are installed to enable multimodal retrieval-augmented generation (RAG) capabilities:\n",
    "```python\n",
    "%pip install --quiet azure-ai-documentintelligence \"python-dotenv\" \"langchain-openai\" \"azure-search-documents\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Environment Variables**\n",
    "The `.env` file must include the following configurations for secure credential management:\n",
    "\n",
    "| **Variable Name**                  | **Description**                                                                 |\n",
    "|------------------------------------|---------------------------------------------------------------------------------|\n",
    "| `AZURE_OPENAI_ENDPOINT`            | Azure OpenAI endpoint URL                                                      |\n",
    "| `AZURE_OPENAI_API_KEY`             | API key for Azure OpenAI                                                       |\n",
    "| `AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT` | Endpoint for Azure Document Intelligence                                       |\n",
    "| `AZURE_DOCUMENT_INTELLIGENCE_KEY`  | API key for Azure Document Intelligence                                        |\n",
    "| `AZURE_VISION_MODEL_ENDPOINT`      | Endpoint for Azure Vision Model                                                |\n",
    "| `AZURE_VISION_MODEL_KEY`           | API key for Azure Vision Model                                                 |\n",
    "| `AZURE_AI_SEARCH_SERVICE_NAME`     | Name of the Azure AI Search service                                            |\n",
    "| `AZURE_AI_SEARCH_API_KEY`          | API key for Azure AI Search service                                            |\n",
    "| `AOAI_EMBEDDINGS_DEPLOYMENT_NAME`  | Deployment name for Azure OpenAI embeddings                                    |\n",
    "| `AZURE_AI_SEARCH_INDEX_NAME`       | Name of the Azure AI Search index                                              |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Notes**\n",
    "- Ensure the `.env` file is properly configured before running the notebook.\n",
    "- The packages installed enable document analysis, image processing, vector embeddings, and search capabilities using Azure AI services.\n",
    "\n",
    "This setup is critical for enabling multimodal RAG workflows that integrate text, image, and table embeddings for advanced AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7c8416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeOutputOption\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "import uuid\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Load environment variables from .env file for secure credential management\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Azure and OpenAI configuration from environment variables\n",
    "di_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "di_credential = AzureKeyCredential(os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\"))\n",
    "chat_model = os.getenv(\"AOAI_CHAT_DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "aoai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "vision_model_endpoint = os.getenv(\"AZURE_VISION_MODEL_ENDPOINT\")\n",
    "vision_model_key = os.getenv(\"AZURE_VISION_MODEL_KEY\")\n",
    "search_endpoint = os.getenv(\"AZURE_AI_SEARCH_SERVICE_NAME\")\n",
    "search_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\")\n",
    "embeddings_model = os.getenv(\"AOAI_EMBEDDINGS_DEPLOYMENT_NAME\", \"text-embedding-3-small\")\n",
    "search_index_name = os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\", \"langchain-vector-demo\")\n",
    "\n",
    "# Initialize Azure Document Intelligence client for document analysis\n",
    "document_intelligence_client = DocumentIntelligenceClient(di_endpoint, di_credential)\n",
    "\n",
    "# Initialize Azure Vision client for image analysis\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=vision_model_endpoint,\n",
    "    credential=AzureKeyCredential(vision_model_key)\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI chat client for LLM-based summarization\n",
    "llm_client = AzureChatOpenAI(\n",
    "    azure_deployment=chat_model,\n",
    "    api_version=\"2023-05-15\",\n",
    "    temperature=0.3,\n",
    "    model_name=chat_model,\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI embeddings client for vectorization\n",
    "embeddings_model_client = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    "    azure_deployment=embeddings_model,\n",
    ")\n",
    "\n",
    "# Path to the sample document (PDF) to be analyzed\n",
    "# For simplicity, this example uses a document sampled from Azure Architecture reference\n",
    "# File can be downloaded from here https://drive.google.com/file/d/1oWxuqCAnu5GEOgNA0pHQCeLgxPf-wduU/view?usp=sharing\n",
    "path_to_sample_documents = \"azure_ref_architecture.pdf\"\n",
    "\n",
    "# Analyze the document using the prebuilt-layout model, requesting figure extraction\n",
    "with open(path_to_sample_documents, \"rb\") as f:\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\",\n",
    "        body=f,\n",
    "        output=[AnalyzeOutputOption.FIGURES], # Requesting figure extraction\n",
    "    )\n",
    "result = poller.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b934fe",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "\n",
    "In this section we will extract the data and organize as Page -> Page Count [tables, images], this helps in efficient search retrieval. Imagine of search query matches the table, image summary the entire page can be located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e726f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 31 documents with headings and text.\n",
      "Table 0 found on page 6.\n",
      "Table 1 found on page 28.\n",
      "Table 2 found on page 29.\n",
      "Extracted 3 tables from the document.\n",
      "Figure 2.1 extracted from page 2.\n",
      "Figure 12.1 extracted from page 12.\n",
      "Figure 26.1 extracted from page 26.\n",
      "Figure 27.1 extracted from page 27.\n",
      "Extracted 4 figures from the document.\n"
     ]
    }
   ],
   "source": [
    "# Extract Text and Headings\n",
    "pages = []\n",
    "\n",
    "# Structure the documents as Pages -> Paragraphs -> Text\n",
    "for paragraph in result.paragraphs:\n",
    "    page = {}\n",
    "    page_number = paragraph[\"boundingRegions\"][0][\"pageNumber\"] if paragraph[\"boundingRegions\"] else None\n",
    "    role = paragraph[\"role\"] if \"role\" in paragraph else \"paragraph\"\n",
    "    content = paragraph[\"content\"] if \"content\" in paragraph else \"\"\n",
    "    \n",
    "    # check if page already exists\n",
    "    if page_number and any(p[\"page_number\"] == page_number for p in pages):\n",
    "        page = next(p for p in pages if p[\"page_number\"] == page_number)\n",
    "        page[\"content\"] = page.get(\"content\", \"\") + \" \" + content\n",
    "        if role in [\"heading\", \"sectionHeading\", \"title\"]:\n",
    "            page[\"sections\"] = page.get(\"sections\", \"\") + \",\" + content\n",
    "        continue\n",
    "    else:\n",
    "        page = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"content\": content,\n",
    "            \"sections\": content if role in [\"heading\", \"sectionHeading\", \"title\"] else \"\",\n",
    "            \"page_number\": page_number,\n",
    "            \"page_content\": [] # Initialize to fill tables, figures later\n",
    "        }\n",
    "    pages.append(page)\n",
    "print(f\"Extracted {len(pages)} documents with headings and text.\")\n",
    "\n",
    "# Extract Tables and add to pages\n",
    "table_elements = []\n",
    "if result.tables:\n",
    "    for idx, table in enumerate(result.tables):\n",
    "        page_number = table[\"boundingRegions\"][0][\"pageNumber\"] if table[\"boundingRegions\"] else None\n",
    "        page = next((p for p in pages if p[\"page_number\"] == page_number), None)\n",
    "        if not page:\n",
    "            print(f\"Page {page_number} not found for table {idx}. Skipping.\")\n",
    "            continue\n",
    "        print(f\"Table {idx} found on page {page_number}.\")\n",
    "        t_documents = []\n",
    "        for cell in table.cells:\n",
    "            t_document = {}\n",
    "            t_document[\"row\"] = cell.row_index\n",
    "            t_document[\"column\"] = cell.column_index\n",
    "            t_document[\"row_content\"] = cell.content\n",
    "            t_documents.append(t_document)\n",
    "        page[\"page_content\"].append({\n",
    "            \"type\": \"table\",\n",
    "            \"content\": str(t_documents),\n",
    "            \"page_number\": page_number,\n",
    "            \"id\": str(uuid.uuid4())\n",
    "        })\n",
    "    print(f\"Extracted {len(result.tables)} tables from the document.\")    \n",
    "\t\n",
    "# # Extract Figures and add to pages later\n",
    "if result.figures:\n",
    "    operation_id = poller.details[\"operation_id\"]\n",
    "    os.makedirs(\"extracted_images\", exist_ok=True)\n",
    "    for figure in result.figures:\n",
    "        page_number = figure[\"boundingRegions\"][0][\"pageNumber\"] if figure[\"boundingRegions\"] else None\n",
    "        page = next((p for p in pages if p[\"page_number\"] == page_number), None)\n",
    "        if not page:\n",
    "            print(f\"Page {page_number} not found for figure {figure.id}. Skipping.\")\n",
    "            continue\n",
    "        if figure.id:\n",
    "            response = document_intelligence_client.get_analyze_result_figure(\n",
    "                model_id=result.model_id, result_id=operation_id, figure_id=figure.id\n",
    "            )\n",
    "            print(f\"Figure {figure.id} extracted from page {page_number}.\")\n",
    "            page['page_content'].append({\n",
    "                \"type\": \"figure\",\n",
    "                \"content\": \"\", # this will be filled with the image summary later\n",
    "                \"image_url\": f\"extracted_images/{figure.id}.png\", # Save the public URL for the image in real world scenarios\n",
    "                \"page_number\": page_number,\n",
    "                \"id\": figure.id\n",
    "            })\n",
    "            with open(f\"extracted_images/{figure.id}.png\", \"wb\") as writer:\n",
    "                writer.writelines(response)\n",
    "else:\n",
    "    print(\"No figures found.\")\n",
    "print(f\"Extracted {len(result.figures)} figures from the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df9059b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Content with Tables\n",
      "{'id': 'fe6db65d-574d-4fb7-8682-83e3f00f50fa', 'content': \"Identity and access management The following guidance expands on the identity and access management guidance in the App Service baseline architecture. The chat UI uses its managed identity to authenticate the chat UI API code to Foundry Agent Service by using the Azure AI Persistent Agents SDK. The Azure AI Foundry project also has a managed identity. This identity authenticates to services such as AI Search through connection definitions. The project makes those connections available to Foundry Agent Service. An Azure AI Foundry account can contain multiple Azure AI Foundry projects. Each project should use its own system-assigned managed identity. If different workload components require isolated access to connected data sources, create separate Azure AI Foundry projects within the same account and avoid sharing connections across them. If your workload doesn't require isolation, use a single project. Role-based access roles You're responsible for creating the required role assignments for the system-assigned managed identities. The following table summarizes the role assignment that you must add to App Service, the Azure AI Foundry project, and individuals who use the portal: [ Expand table Resource Role Scope App Service Azure AI User Azure AI Foundry account Azure AI Foundry project Search Index Data Reader AI Search Portal user (for each individual) Azure AI Developer Azure AI Foundry account Network security To simplify the learning experience for building an end-to-end chat solution, this architecture doesn't implement network security. It uses identity as its perimeter and uses public cloud constructs. Services such as AI Search, Azure AI Foundry, and App Service are reachable from the internet. This setup increases the attack surface of the architecture.\", 'sections': 'Identity and access management,Role-based access roles,Network security', 'page_number': 6, 'page_content': [{'type': 'table', 'content': \"[{'row': 0, 'column': 0, 'row_content': 'Resource'}, {'row': 0, 'column': 1, 'row_content': 'Role'}, {'row': 0, 'column': 2, 'row_content': 'Scope'}, {'row': 1, 'column': 0, 'row_content': 'App Service'}, {'row': 1, 'column': 1, 'row_content': 'Azure AI User'}, {'row': 1, 'column': 2, 'row_content': 'Azure AI Foundry account'}, {'row': 2, 'column': 0, 'row_content': 'Azure AI Foundry project'}, {'row': 2, 'column': 1, 'row_content': 'Search Index Data Reader'}, {'row': 2, 'column': 2, 'row_content': 'AI Search'}, {'row': 3, 'column': 0, 'row_content': 'Portal user (for each individual)'}, {'row': 3, 'column': 1, 'row_content': 'Azure AI Developer'}, {'row': 3, 'column': 2, 'row_content': 'Azure AI Foundry account'}]\", 'page_number': 6, 'id': '81caa66a-d202-429b-8305-a6da0f3b24a7'}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample Content with Figures\n",
      "{'id': '2b710501-7d8c-4716-8c1c-ac9778f02b01', 'content': 'Azure AI Foundry Azure AI Foundry account 2 A App Service built- in authentication (Easy Auth) Managed identity App Service 1 https://domainname.azurewebsites.net 3 App Service instance Foundry Agent Service Managed identities User Azure AI Foundry project 6 5 4 Application Insights Azure Monitor Azure OpenAI model Monitoring Microsoft Azure Azure AI Search + Download a Visio filet of this architecture. Workflow The following workflow corresponds to the previous diagram: 1. An application user interacts with a web application that contains chat functionality. They issue an HTTPS request to the App Service default domain on azurewebsites. net . This domain automatically points to the App Service built-in public IP address. The Transport Layer Security connection is established from the client directly to App Service. Azure fully manages the certificate. 2. The App Service feature called Easy Auth ensures that the user who accesses the website is authenticated via Microsoft Entra ID. 3. The application code deployed to App Service handles the request and renders a chat UI for the application user. The chat UI code connects to APIs that are also hosted in that same App Service instance. The API code connects to an Azure AI agent in Azure AI Foundry by using the Azure AI Persistent Agents SDK.', 'sections': ',Workflow', 'page_number': 2, 'page_content': [{'type': 'figure', 'content': '', 'image_url': 'extracted_images/2.1.png', 'page_number': 2, 'id': '2.1'}]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Content with Tables\")\n",
    "print(pages[5])\n",
    "print('--'*50)\n",
    "print(\"Sample Content with Figures\")\n",
    "print(pages[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9802f0d",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "In this section we generate summaries for tables, images. I'm using GPT4o for image summarization you may use any VLM. Generating summaries for pages is optionally here, however it may help increase search score when hybrid search is used. \n",
    "\n",
    "**Note**: I initially planned to use Azure Computer Vision, however I noticed that the it only provides precise captions of the image but does not generate summaries hence I switched to VLM (infact a multi-modal LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cfd5560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:29,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "import tqdm\n",
    "\n",
    "# Text Summarization \n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | llm_client | StrOutputParser()\n",
    "\n",
    "# Image Summarization\n",
    "def encode_image(image_path):\n",
    "    '''Getting the base64 string'''\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def image_summarize(image_path):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    prompt_text = \"You are an assistant tasked with summarizing images. Extract text, image from the input and Give a concise summary.\"\n",
    "    img_base64 = encode_image(image_path)\n",
    "    msg = llm_client.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt_text},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_base64}\" # Use base64 encoded image\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "# Summarize content of pages\n",
    "# Note: The content vectorization is commented out to avoid increase in size of the file\n",
    "texts = [i[\"content\"] for i in pages if i[\"content\"] != \"\"]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "for i, page in tqdm.tqdm(enumerate(pages)):\n",
    "    page[\"summary\"] = text_summaries[i] if i < len(text_summaries) else \"No summary available\" \n",
    "    # page[\"content_vector\"] = embeddings_model_client.embed_documents(page[\"content\"])\n",
    "    for content in page[\"page_content\"]:\n",
    "        if content[\"type\"] == \"table\":\n",
    "            content[\"summary\"] = summarize_chain.invoke(content[\"content\"])\n",
    "            # content[\"content_vector\"] = embeddings_model_client.embed_documents(content[\"summary\"] + str(content[\"text\"])) # we remove the content vector for tables if it is too large\n",
    "        elif content[\"type\"] == \"figure\":\n",
    "            content[\"summary\"] = image_summarize(content['image_url'])\n",
    "            # content[\"content_vector\"] = embeddings_model_client.embed_documents(content[\"summary\"])\n",
    "        else:\n",
    "            content[\"summary\"] = \"No summary available\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f760a9b",
   "metadata": {},
   "source": [
    "### Multi-Vector Search\n",
    "\n",
    "**Azure Multi-Vector Field Search [Public Preview]**\n",
    "\n",
    "This notebook uses a public preview feature in Azure AI Search called [Multi-vector field](https://learn.microsoft.com/en-us/azure/search/vector-search-multi-vector-fields). It allows indexing multiple child vectors within a single document field, making it particularly effective for multimodal data or long-form documents, where a single vector representation may lose important context or detail. We can embed multiple chunks of a document and associate those embeddings with the parent document, allowing retriever hits on the chunks to return the larger document. This capability ensures finer-grained retrieval and relevance when working with diverse data types like text and images.\n",
    "\n",
    "Since this feature is in preview the only support method to create the Index is via Portal or REST API. Before you proceed make sure the index is created in Azure Search either from the portal or via REST API. \n",
    "The Index `JSON` file can be found under `/configs/multi-vector-index.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99a07f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a521db8d0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a521dad90>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a521da090>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a521dbb10>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a521dacd0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a52b9a510>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a52b98610>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a52b9af10>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd0050>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd0310>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1a90>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1850>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd14d0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd0a50>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd0690>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1e90>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1f10>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd2190>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1ad0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd0cd0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1b50>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd0d10>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd0fd0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1010>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1410>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1c50>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd2610>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1150>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd2490>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd1f90>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x18a54cd2410>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=search_index_name,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "# Uploading the documents to Azure AI Search\n",
    "# Note: Assuming the index with same name above is created in Azure Search either from the portal or via REST API.\n",
    "# The Index `JSON` file can be found under `/configs/multi-vector-index.json`\n",
    "search_client.upload_documents(\n",
    "    documents=pages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3887994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Number: 12, Page Summary: This architecture leverages the Foundry Agent Service standard setup to ensure enterprise-grade security, compliance, and control for chat applications on Azure. Key features include:\n",
      "\n",
      "- **Network Isolation**: Users bring their own network and Azure resources to isolate data and store chat/agent state.\n",
      "- **Private Endpoints**: All communication between components and Azure services occurs over private endpoints, keeping data traffic within the virtual network.\n",
      "- **Outbound Traffic Control**: Outbound traffic is routed through Azure Firewall, enforcing strict egress rules.\n",
      "- **Baseline Reference**: The Foundry Agent Service reference implementation provides a foundational end-to-end chat solution for customization and scaling toward production.\n",
      "\n",
      "**Key Components**:\n",
      "- Azure Application Gateway with Web Application Firewall (WAF) for request routing and inspection.\n",
      "- Private endpoints for secure access to Azure services like Key Vault, Storage, AI Search, and Cosmos DB.\n",
      "- Managed identities for secure authentication.\n",
      "- Azure Monitor for application insights and monitoring.\n",
      "- Optional integration with an API gateway.\n",
      "\n",
      "This architecture ensures robust security, scalability, and compliance for enterprise chat applications.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Page Content Summary: ### Summary of the Image:\n",
       "The diagram illustrates the architecture of an Azure-based solution integrating multiple services for secure and efficient application deployment and AI model utilization. Key components include:\n",
       "\n",
       "1. **User Interaction**: Users access the system via a virtual network with DNS zones, Application Gateway (with Azure WAF), and DDoS protection.\n",
       "2. **Application Services**: Managed identity-enabled App Services are deployed across multiple zones, supported by private endpoints for secure connectivity to Azure Key Vault, Azure Storage, and other resources.\n",
       "3. **Azure AI Foundry Integration**: The Foundry Agent Service connects to Azure AI Foundry accounts and projects using managed identities.\n",
       "4. **Knowledge Store**: Private endpoints link to services like Azure Cosmos DB, Azure Storage, and Azure Cognitive Search for data management and retrieval.\n",
       "5. **Security Layers**: Azure Firewall and jump box subnets handle outbound traffic and secure agent builds.\n",
       "6. **Build Agents**: Dedicated subnets host build agents for deploying and managing AI models.\n",
       "7. **Monitoring**: Application Insights and Azure Monitor provide system observability.\n",
       "\n",
       "The architecture emphasizes security, scalability, and integration of AI capabilities with foundational Azure services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets do a search to find the page with the query\n",
    "result = search_client.search(\"show integration of Azure Web Application Firewall (WAF)\", top=1)\n",
    "for r in result:\n",
    "    print(f\"Page Number: {r['page_number']}, Page Summary: {r['summary']}\")\n",
    "    for content in r[\"page_content\"]:\n",
    "        print('-' * 50)\n",
    "        display(Markdown(f\"Page Content Summary: {content['summary']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c3e47c",
   "metadata": {},
   "source": [
    " As you can see the search result returns the page number and summary of the page although the query matches the image summary. This allows for efficient retrieval of relevant information based on user queries. # This is particularly useful in scenarios where users need to quickly locate specific information within large documents, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3856686",
   "metadata": {},
   "source": [
    "### RAG with multi-modal data\n",
    "\n",
    "In this section, we will build a RAG (Retrieval-Augmented Generation) pipeline. During the retrieval phase, search results will be restructured to include page content, table data, image summaries, and the images themselves. Images will be converted to Base64-encoded values and passed to a multimodal language model to enhance the quality of the generated response. The most relevant image identified for the query will also be included in the final response to deliver a richer user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca576254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import AzureAISearchRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Initialize user memory to store conversation context\n",
    "conversation_memory = {}\n",
    "\n",
    "retriever = AzureAISearchRetriever(\n",
    "    top_k=3, \n",
    "    service_name=search_endpoint,\n",
    "    api_key=search_key,\n",
    "    content_key=\"content\",  # This is the field name for the vector embeddings\n",
    "    index_name=search_index_name,\n",
    "    azure_ad_token=\"True\" # Bug in langchain-azure-ai-search, need to set this to True to use API Key\n",
    ")\n",
    "\n",
    "def splitDocs(docs, config):\n",
    "    imgs = []\n",
    "    table_summaries = []\n",
    "    image_summaries = []\n",
    "    image_urls = []\n",
    "    conversation_id = config['metadata']['conversation_id']\n",
    "    for doc in docs:\n",
    "        content = doc.page_content + \"\\n\" + \"\\n\".join([p[\"summary\"] for p in doc.metadata[\"page_content\"]])\n",
    "        for page_content in doc.metadata[\"page_content\"]:\n",
    "            if page_content[\"type\"] == \"table\":\n",
    "                table_summaries.append(page_content[\"summary\"])\n",
    "            elif page_content[\"type\"] == \"figure\":\n",
    "                image_summaries.append(page_content[\"summary\"])\n",
    "                image_urls.append(page_content[\"image_url\"])\n",
    "                base64_image = encode_image(page_content[\"image_url\"])\n",
    "                imgs.append(base64_image)\n",
    "    conversation_memory[conversation_id] = {'image_urls': image_urls}\n",
    "    return { \n",
    "        'content': content, \n",
    "        'images': imgs, \n",
    "        'table_summaries': table_summaries, \n",
    "        'image_summary': image_summaries, \n",
    "        \"image_urls\": image_urls \n",
    "    }\n",
    "\n",
    "def prompt_func(dict):\n",
    "    format_texts = \"\\n\".join(dict['context'][\"content\"])\n",
    "    table_summaries = \"\\n\".join(dict['context'][\"table_summaries\"])\n",
    "    image_summaries = \"\\n\".join(dict['context'][\"image_summary\"])\n",
    "    message = [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant! Your name is Bob. \\\n",
    "                You are given context that contains document content along with tables and images. A summary of the content along with tables and images is provided. \\\n",
    "                You are an expert in answering questions using text, tables, and images.\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \n",
    "                 \"text\": f\"\"\"\n",
    "                 Answer the question based only on the following context, which can include text, tables, and the below image: Question: {dict[\"question\"]} \\\n",
    "                    Text {format_texts}, table summaries: {table_summaries}, image summaries: {image_summaries} \"\"\"},\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    if dict['context']['images']:\n",
    "        for i in range(len(dict['context']['images'])):\n",
    "            message.append(\n",
    "                HumanMessage(\n",
    "                    content=[\n",
    "                        {\"type\": \"image_url\", \n",
    "                         \"image_url\": {\"url\": f\"data:image/jpeg;base64,{dict['context']['images'][i]}\"}}\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        print(\"No images found in the context.\")\n",
    "    return message\n",
    "\n",
    "conversation_id = str(uuid.uuid4())\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(splitDocs), \"question\": RunnablePassthrough()} \n",
    "    | RunnableLambda(prompt_func) \n",
    "    | llm_client \n",
    "    | StrOutputParser()\n",
    ")\n",
    "answer = chain.invoke(\n",
    "    {\"question\": \"As an AI Architect summarize the Azure AI Foundry chat reference architecture\"}, \n",
    "    config={\"conversation_id\": conversation_id}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2447288f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Azure AI Foundry chat reference architecture is designed to enable secure, scalable, and efficient integration of AI capabilities with foundational Azure services. It incorporates enterprise-grade security, compliance, and control, leveraging private endpoints and Azure Firewall for network isolation and secure communication. Key components include:\n",
       "\n",
       "1. **User Interaction**: Users interact with the system through a virtual network, DNS zones, and an Application Gateway with Azure Web Application Firewall (WAF) for request inspection. DDoS protection ensures resilience against attacks.\n",
       "\n",
       "2. **Application Services**: Managed identity-enabled App Services are deployed across multiple zones, connected securely to Azure resources like Key Vault, Storage, and others via private endpoints.\n",
       "\n",
       "3. **Azure AI Foundry Integration**: The Foundry Agent Service connects to Azure AI Foundry accounts and projects, utilizing managed identities for secure access.\n",
       "\n",
       "4. **Knowledge Store**: Private endpoints link to services like Azure Cosmos DB, Azure Storage, and Azure Cognitive Search for data storage, management, and retrieval.\n",
       "\n",
       "5. **Security Layers**: Azure Firewall enforces egress rules for outbound traffic, while jump box subnets provide secure access for managing agent builds.\n",
       "\n",
       "6. **Build Agents**: Dedicated subnets host build agents responsible for deploying and managing AI models.\n",
       "\n",
       "7. **Monitoring**: Application Insights and Azure Monitor provide observability into system performance and health.\n",
       "\n",
       "The architecture ensures that all communication between components and Azure services occurs over private endpoints, maintaining data traffic within the workload's virtual network. Outbound traffic is strictly routed through Azure Firewall, enforcing security policies. This setup serves as a baseline for developing custom solutions and transitioning to production environments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](extracted_images/12.1.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(answer))\n",
    "######## Display images \n",
    "for image_url in conversation_memory[conversation_id]['image_urls']:\n",
    "    display(Markdown(f\"![Image]({image_url})\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
